Timestamp,"Please check the box below to indicate that you have read this statement in its entirety; that your questions about the survey have been answered to your satisfaction; and that you voluntarily agree to participate in the survey.  
You may print a copy of this consent form if you wish.","PROJECT FUTURE DIRECTION AND PRIORITIES

Thinking about scikit-learn's future, what aspects of the library would you prioritize for improvement? 
A greater numerical value signifies a greater level of priority. [Performance]","PROJECT FUTURE DIRECTION AND PRIORITIES

Thinking about scikit-learn's future, what aspects of the library would you prioritize for improvement? 
A greater numerical value signifies a greater level of priority. [Reliability]","PROJECT FUTURE DIRECTION AND PRIORITIES

Thinking about scikit-learn's future, what aspects of the library would you prioritize for improvement? 
A greater numerical value signifies a greater level of priority. [Packaging]","PROJECT FUTURE DIRECTION AND PRIORITIES

Thinking about scikit-learn's future, what aspects of the library would you prioritize for improvement? 
A greater numerical value signifies a greater level of priority. [New features]","PROJECT FUTURE DIRECTION AND PRIORITIES

Thinking about scikit-learn's future, what aspects of the library would you prioritize for improvement? 
A greater numerical value signifies a greater level of priority. [Technical documentation]","PROJECT FUTURE DIRECTION AND PRIORITIES

Thinking about scikit-learn's future, what aspects of the library would you prioritize for improvement? 
A greater numerical value signifies a greater level of priority. [Educational materials]","PROJECT FUTURE DIRECTION AND PRIORITIES

Thinking about scikit-learn's future, what aspects of the library would you prioritize for improvement? 
A greater numerical value signifies a greater level of priority. [Website redesign]","PROJECT FUTURE DIRECTION AND PRIORITIES

Thinking about scikit-learn's future, what aspects of the library would you prioritize for improvement? 
A greater numerical value signifies a greater level of priority. [Other]",Please expand on your answer about the priorities for scikit-learn.,What single immediate change to scikit-learn would bring the most value to you as a scikit-learn user? ,"TECHNICAL QUESTIONS

PROJECT
Please order the following ML tasks in order of priority to you: (A higher number means higher priority) [Regression]","TECHNICAL QUESTIONS

PROJECT
Please order the following ML tasks in order of priority to you: (A higher number means higher priority) [Classification]","TECHNICAL QUESTIONS

PROJECT
Please order the following ML tasks in order of priority to you: (A higher number means higher priority) [Forecasting]","TECHNICAL QUESTIONS

PROJECT
Please order the following ML tasks in order of priority to you: (A higher number means higher priority) [Outlier/anomaly detection]","TECHNICAL QUESTIONS

PROJECT
Please order the following ML tasks in order of priority to you: (A higher number means higher priority) [Dimensionality reduction]","TECHNICAL QUESTIONS

PROJECT
Please order the following ML tasks in order of priority to you: (A higher number means higher priority) [Clustering]","TECHNICAL QUESTIONS

PROJECT
Please order the following ML tasks in order of priority to you: (A higher number means higher priority) [Other]",What visualizations do you use to evaluate your models? Select all that apply.,Which DataFrame libraries do you use? Select all that apply.,"MODELING
What do you like the most about scikit-learn? ",Which other Machine Learning libraries do you use? Select all that apply.,Which estimators do you regularly use? Select all that apply.,"Have you ever written your own estimator, or extended an existing scikit-learn estimator?",What ML features are important for your use case? Select all that apply.,Is there additional information you want to pass to an estimator that is not X and Y?," If so, what kind of information would that be?",How would it benefit the model training process?[,"DEPLOYMENT
Considering your current machine learning projects, how critical would GPU capabilities within scikit-learn be? (A higher number means it is very critical)","For model registry and experiment tracking, do you use any of the following tools? Select all that apply.","For scheduling, do you use any of the following tools? Select all that apply.",How long does a typical model training take in your ML projects?, [How many deployed models are you (and your team) currently maintaining?],"To what extent do you agree with the following statement?
Open source ML & AI frameworks and libraries are crucial for ensuring transparency and the reproducibility of AI research and development.","VOLUNTEER FOR INTERVIEW
Would you like to volunteer for a short conversation with the scikit-learn survey team to discuss your responses in more detail?"," If yes, please provide your email address. "
8/27/2024 9:42:56,I have read this statement in its entirety and affirm the stated conditions.,1,3,4,2,5,6,7,8,,,3,2,1,4,5,6,7,"ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,,"Keras, PyTorch","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,Uncertainty estimates for prediction,No,,,2,MLFlow,,less than a minute,,Strongly agree,No,
8/27/2024 9:49:23,I have read this statement in its entirety and affirm the stated conditions.,1,3,2,1,3,2,8,,,Forecasting or time-series predictions,2,5,1,4,3,6,7,"Reliability diagram, Feature importance, Residual plots, Learning curves","pandas, Polars",I like the simple and deep API that allows for customizability and also plug-and-play solutions.,"PyTorch, XGBoost","Pipeline, ColumnTransformer",No,"Calibration of regressors, Uncertainty estimates for prediction",No,,,1,MLFlow,,"less than 10 seconds, less than a minute, less than a day",1,Strongly agree,Yes,joseph@aimdyn.com
8/27/2024 10:01:45,I have read this statement in its entirety and affirm the stated conditions.,7,7,6,8,6,5,2,2,,gpu support,7,7,5,4,6,5,4,"Confusion matrix, Precision-Recall curve","pandas, Spark DataFrame",coherence of the interface,"Keras, LightGBM, Transformers","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Feature importances",No,,,5,Custom tool,"Airflow, Custom tool","less than an hour, less than a day",3,Strongly agree,No,
8/27/2024 10:04:14,I have read this statement in its entirety and affirm the stated conditions.,6,7,7,7,8,8,8,8,,,2,7,,6,6,6,2,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","Dask DataFrame, pandas",,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Sample weights, Non-euclidean metrics",No,,,3,MLFlow,Airflow,less than a day,1,,,
8/27/2024 10:04:24,I have read this statement in its entirety and affirm the stated conditions.,4,2,2,4,2,2,4,1,,,4,4,4,1,1,1,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance",pandas,,"LightGBM, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Feature importances, Sample weights",Yes,offset,,2,MLFlow,,more than a day,More than 5,Strongly agree,No,
8/27/2024 10:16:02,I have read this statement in its entirety and affirm the stated conditions.,6,6,3,8,8,5,2,1,,,7,6,1,1,6,6,7,"Confusion matrix, ROC curve, Precision-Recall curve","pandas, Spark DataFrame",Ease of use ,"Jax, Keras, PyTorch, Transformers","LogisticRegression, ColumnTransformer",Yes,"Calibration of regressors, Sample weights",No,,,3,"MLFlow, Weight and biases",Other,less than a day,3,Strongly agree,No,
8/27/2024 10:20:11,I have read this statement in its entirety and affirm the stated conditions.,8,1,1,6,6,1,1,1,I think the biggest issue with sklearn is its relative slow performance.,Speed,2,1,3,4,5,6,7,"Confusion matrix, Feature importance, Residual plots, Learning curves","pandas, Spark DataFrame","Easy implementation, wide range of models","CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,5,MLFlow,"Airflow, Custom tool",less than a day,3,Strongly agree,Yes,ddvillacresesz@gmail.com
8/27/2024 10:25:49,I have read this statement in its entirety and affirm the stated conditions.,4,,,2,3,1,,,"Greater support: Tweedie distributions (HistGradientBoostingClassifier is decent, but could be better), GroupedStratifiedShuffleSplit (without CV!), Hierarchical loss functions would be amazing (so that you can gather losses upwards into larger and larger groups etc), also better handling of errors. We are far behind when it comes to considering errors in data science... imagine an algo that could take in some tabular data, where each numeric value has an associated error (and this may differ per row), you could then pass that error into the model so it knows not to take those specific cells too seriously [imaginining this inside a simple decision tree... you could feed this error to the place where the Gini coeff is calculated]","A series of tutorials/newsletter of advanced concepts/libraries for senior data scientists.
Honestly, I don't care that much about 95% of the release notes, but the 5% that addresses new ways to address specific problems is amazing. There is still so much of scikit that I don't know about, but I don't want to trawl through all the docs to find it. A weekly Youtube series aimed at data scientists with 5-10+ years experience would be great.",1,2,6,5,4,3,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Polars","Documentation, interoperability","CatBoost, LightGBM, Transformers, XGBoost",HistGradientBoostingRegressor or HistGradientBoostingClassifier,Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",Yes,ERRORS,"Individual row/columns (i.e. cells) could be taken ""less seriously""",1,MLFlow,Airflow,less than an hour,1,Strongly agree,Yes,leviwaddingham1@gmail.com
8/27/2024 10:39:14,I have read this statement in its entirety and affirm the stated conditions.,1,1,2,2,2,2,3,3,In industry we have millions of data points.umap and hdbscan are very slow,Ultra fast umap ,3,2,3,3,1,1,3,"ROC curve, Precision-Recall curve, Feature importance",pandas,How easy it is to use,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,1,"MLFlow, Weight and biases",,less than an hour,More than 5,Strongly agree,Yes,Manab.chetia31@gmail.com 
8/27/2024 10:48:45,I have read this statement in its entirety and affirm the stated conditions.,8,8,6,8,8,8,1,1,This is an amazing tool for learning,More educational materials,7,7,3,7,4,4,1,"Confusion matrix, Precision-Recall curve, Feature importance",pandas,,"PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Cost-sensitive learning, Feature importances",No,,,4,MLFlow,Kubeflow,less than an hour,2,Strongly agree,No,
8/27/2024 10:57:35,I have read this statement in its entirety and affirm the stated conditions.,1,3,,2,,,,,"Performance new features and reliability 
all relate to the fact that sklearn should expand into distributed computing. Even if this means a breaking change from the core or creating a separate library entirely. Real dataset don't fit in memory on one computer. Some algorithms need a performance boost like SVM.",Possibility to work on distributed datasets.,3,2,,1,,,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","cudf, Dask DataFrame, pandas, Polars, Spark DataFrame",The simplicity and consistency of its API.,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer, Other",Yes,"Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",,,,1,MLFlow,Airflow,less than a day,1,Strongly agree,No,spa.simone@gmail.com
8/27/2024 11:00:18,I have read this statement in its entirety and affirm the stated conditions.,4,5,6,1,3,2,7,8,,trainning materials.....sorry I am a noob,6,3,5,1,2,4,7,"Confusion matrix, ROC curve, Feature importance, Residual plots","DuckDB, pandas",,"LightGBM, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",,,,3,,,less than an hour,3,Strongly agree,No,
8/27/2024 11:16:01,I have read this statement in its entirety and affirm the stated conditions.,2,,,1,,,,,,,2,1,,3,,3,,"Confusion matrix, ROC curve, Precision-Recall curve",pandas,everything kinda it is easy to use and so advanced ,"Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,,No,,,1,,,less than 10 minutes,More than 5,Agree,Yes,minaibrahim190@gmail.com
8/27/2024 11:21:59,I have read this statement in its entirety and affirm the stated conditions.,3,3,1,4,2,1,1,1,Hellinger distance split in trees for binary classification ,"Invest in trees. Variations, pipelines and new stuff ",2,1,7,5,4,3,6,"ROC curve, Feature importance, Other",pandas,All stuff integrated together ,"PyTorch, Transformers, XGBoost",HistGradientBoostingRegressor or HistGradientBoostingClassifier,Yes,"Uncertainty estimates for prediction, Feature importances",No,,,1,MLFlow,,less than a day,More than 5,Strongly agree,Yes,keith.m.edmonds@gmail.com
8/27/2024 11:30:02,I have read this statement in its entirety and affirm the stated conditions.,3,3,3,3,3,3,3,3,,,3,3,3,3,3,3,3,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Learning curves, Other","cudf, Dask DataFrame, DuckDB, Polars",,"CatBoost, Keras, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,,,4,"MLFlow, Weight and biases","Dagster, Kubeflow, Custom tool",less than 10 seconds,3,Strongly agree,Yes,
8/27/2024 11:31:43,I have read this statement in its entirety and affirm the stated conditions.,8,4,2,8,2,2,2,2,"I would love to see scikit cover key area where there is still no trusted ready-to-go solution (ex: nlp, timeseries, etc). Fasttext and Prophet were potential candidates but neither have the reliability, quality and design that only scikit can bring to the table. In fact, fasttext got dropped with no replacement in sight.","Taking over Fasttext as a model (or adaptation). It's an orphan project that will leave a gap in the ecosystem, leaving user the choice between suboptimal sparse models from the current scikit nlp stack, or much more complex options (spacy, torch).",6,7,6,5,3,3,4,"ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Spark DataFrame","The overall quality, consistency, and ""trademark"". The user guides are great and informative, the models are to-the-point, and I immensely like the HistGradientBoosting models.","CatBoost, PyTorch","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,2,MLFlow,Kubeflow,"less than 10 minutes, less than an hour",More than 5,Strongly agree,Yes,raphael.assal@gmail.com
8/27/2024 11:35:48,I have read this statement in its entirety and affirm the stated conditions.,6,5,7,7,7,6,6,2,,,6,6,4,3,2,6,1,"Confusion matrix, ROC curve, Feature importance, Learning curves","pandas, Polars",,"Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of probabilistic classifiers, Feature importances",No,,,4,MLFlow,"Airflow, Other",less than 10 minutes,More than 5,Agree,Yes,aside-syrup-shy@duck.com
8/27/2024 11:37:49,I have read this statement in its entirety and affirm the stated conditions.,3,,,3,,,,,,,2,3,,,,1,,"Confusion matrix, ROC curve, Precision-Recall curve",pandas,Everything kinda it is so easy to use and advanced ,"Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Calibration of probabilistic classifiers, Calibration of regressors",No,,,5,,,less than 10 minutes,More than 5,Agree,Yes,minaibrahim190@gmail.com
8/27/2024 12:03:53,I have read this statement in its entirety and affirm the stated conditions.,,3,2,,3,3,,,Reliability and compatibility with other libraries are the most important to me. ,,3,3,3,,,,,"Confusion matrix, ROC curve, Feature importance, Learning curves","pandas, Polars",Prevalence,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost",RandomForestClassifier or RandomForestRegressor,Yes,Feature importances,,,,4,MLFlow,,less than 10 minutes,1,Neither agree nor disagree,Yes,aysetubat22@gmail.com
8/27/2024 12:22:02,I have read this statement in its entirety and affirm the stated conditions.,6,6,3,8,3,6,3,3,,,4,7,5,5,5,4,7,"Confusion matrix, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Spark DataFrame",,"Keras, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,3,"MLFlow, Other",,"less than 10 seconds, less than a minute, less than an hour",,Strongly agree,No,
8/27/2024 12:30:58,I have read this statement in its entirety and affirm the stated conditions.,7,6,4,7,8,8,1,8,"I believe that more than a package, scikit-learn has initiated a standard that pushed innovation forward with easy to use and composable blocks that brought with them best practices for the ML and AI industry, all that with transparency on the sources and with an open mindset. I believe then it needs to double-down on its own ""alma matter"" and birth principles, and invest on thinking, building and integrating the next decade of standards and best practices for the AI and ML industry. It should provide not only constructs of materialized code, but also fundamental research literature research to consolidate modern best practices of the field into new standards. Composability is also key, and at the heart of scikit-learn, which should invest in integrating more with the community of other projects to modernize itself too (either on backend related tasks like GPU computing OR on data science practice tasks like improving evaluation pipelines). All that done with transparency and research documentation will consolidate scikit-learn's position as the open standards consolidator for the community.","It is mostly not for me, but what about an official GUI for scikit-learn, embedding a nice bot showing to you best practices and hidden uses of powerful scikit-learn features?",5,3,6,1,2,4,7,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","Dask DataFrame, DuckDB, pandas, Polars",Composability,"CatBoost, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Uncertainty estimates for prediction, Feature importances, Metadata routing, Non-euclidean metrics",Yes,Custom loss functions,Easily add and compare extra customized constraints,2,"MLFlow, Other","Airflow, Other",less than a day,1,Strongly agree,Yes,andersonpachaves@gmail.com
8/27/2024 12:31:24,I have read this statement in its entirety and affirm the stated conditions.,3,2,2,3,2,2,2,,,,3,3,1,2,2,3,,"Confusion matrix, Feature importance, Residual plots","pandas, Polars",Standardized function for each class ,"CatBoost, Keras, LightGBM, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,4,MLFlow,"Argo, Other",more than a day,2,Strongly agree,No,
8/27/2024 12:42:22,I have read this statement in its entirety and affirm the stated conditions.,8,8,,8,7,6,1,1,,,4,6,5,7,7,7,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","Dask DataFrame, Modin, pandas, Polars",,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer, Other",No,"Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",,,,5,MLFlow,Airflow,more than a day,More than 5,Strongly agree,,
8/27/2024 13:12:07,I have read this statement in its entirety and affirm the stated conditions.,6,7,7,8,8,5,1,,,Serialization and saving of models in a more robust and python version agnostic format than pickle.,7,7,2,7,4,6,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,The fantastic and consitent API. The ability to roll new custom estimators (check out our chemistry featurizer Scikit-Mol as example),"Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,Failed samples flag. Currently it can blow up everything if a single sample for some featurizer doesn't compute correctly and is invalid.,"It would benefit the production models, not the training process where I can weed out the failed samples. But uhh, what users send against your predictors is kinda wild sometimes.",3,"Weight and biases, Other",,less than 10 minutes,More than 5,Strongly agree,Yes,esbenbjerrum@gmail.com
8/27/2024 13:31:59,I have read this statement in its entirety and affirm the stated conditions.,5,8,6,5,7,7,4,,"Correctness, consistency, API design etc is important. Maybe more so than performance.","The API is what I get the most value out of. So keeping up the work with the API, interchangable functions/interfaces etc.",6,4,4,4,1,1,3,"ROC curve, Feature importance, Residual plots, Learning curves",pandas,"the consistent api and the documentation. good prioritization of algos. seems sklearn follows the 20/80 rule well, implementing the most important stuff for ML.","LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Metadata routing",No,"Rarely, but sometimes. In one project I needed to pass quantiles per data point, for a datapoint-specific quantile regression. Implementing this myself using metadata routing and a custom model. Very easy with the good sklearn API",,1,MLFlow,Kubeflow,less than a minute,2,Strongly agree,No,
8/27/2024 13:48:20,I have read this statement in its entirety and affirm the stated conditions.,7,8,5,5,8,8,5,,,,7,5,4,6,2,3,1,"Feature importance, Residual plots",pandas,The fact it has several different model types available,,"RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, ColumnTransformer",No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",,,,2,,,less than 10 minutes,3,Agree,No,
8/27/2024 13:50:31,I have read this statement in its entirety and affirm the stated conditions.,,,,3,,3,,,,,,,3,3,,,,"Precision-Recall curve, Feature importance, Residual plots",pandas,,"CatBoost, Keras, LightGBM, Transformers, XGBoost","Pipeline, ColumnTransformer",,"Calibration of probabilistic classifiers, Calibration of regressors",,,,,,,,,,,
8/27/2024 13:54:27,I have read this statement in its entirety and affirm the stated conditions.,5,,,5,5,,,,More community examples with real data to show the challenges of using ml (with sklearn) in practice ,More real-life use cases (dos and don'ts),4,3,5,1,,2,,"ROC curve, Feature importance, Residual plots, Learning curves","pandas, Polars, Spark DataFrame",The api design! The concept of pipeline,"Jax, PyTorch, Other","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning",Yes,Multiple targets ,More flexible models,4,MLFlow,"Airflow, Kubeflow",less than an hour,2,Strongly agree,Yes,juanitorduz@gmail.com
8/27/2024 14:03:22,I have read this statement in its entirety and affirm the stated conditions.,5,5,6,6,7,8,2,1,"The examples, references, and demos set this library apart from many others in a wonderful way. I can learn more about the concepts behind functions in this documentation than almost any other library. It's simply outstanding. ","Hard to think of anything. A way to natively export a visual selection (like a lasso capture) of plotted data directly into a scikit-learn class, rather than printing the lasso's capture by index, then looping and grabbing the points from the original set in multiple steps.

I guess really: more examples on how to use interactive plots to export subsets of interesting data. Interactive plots are technically outside of skikit-learn, but interactive examples help me process new features faster during exploratory processing. ",2,4,6,3,7,5,,"Confusion matrix, Feature importance",pandas,"The unified syntax, with the fit, transform style makes it easier to use a new class of functions. ",PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",No,,,2,,,less than an hour,3,Strongly agree,No,
8/27/2024 14:19:45,I have read this statement in its entirety and affirm the stated conditions.,5,7,8,5,8,7,3,2,,,7,2,5,3,4,6,1,"Confusion matrix, ROC curve, Residual plots, Learning curves","DuckDB, pandas, Polars, Spark DataFrame",,"LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, Pipeline",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Sample weights",Yes,constraints,,4,,"Airflow, Other",less than a day,More than 5,Strongly agree,Yes,rkovashikawa@gmail.com
8/27/2024 14:40:31,I have read this statement in its entirety and affirm the stated conditions.,5,6,5,7,4,7,4,4,"It would be great to see more materials about the hidden functionalities in scikit-learn. Some great examples are learning curves, decision boundary,  polynomial features.","Having xgboost sklearn process categorical labels. I understand that xgboost developers should do it, but possibly some middle ground can be found? For example, allow label encoder to work in a pipeline to ease the burden for a user. Most people just want a hassle-free experience and don’t care whose task this is.",2,6,4,3,2,2,4,"Reliability diagram, ROC curve, Learning curves","DuckDB, pandas","I like the diversity of algorithms, it’s like a one-stop shop. I also like writing my own algorithm implementations and use scikit-learn for performance benchmarks. I also love visualization tools, they are very customizable.","CatBoost, PyTorch, Transformers, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Cost-sensitive learning",No,,,3,Custom tool,Other,less than an hour,1,Agree,Yes,denis.a.burakov@gmail.com
8/27/2024 15:01:49,I have read this statement in its entirety and affirm the stated conditions.,8,8,6,6,4,4,1,1,,,4,5,,7,7,7,4,"Confusion matrix, ROC curve, Residual plots",pandas,,"Keras, PyTorch, Transformers","Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Cost-sensitive learning, Non-euclidean metrics",No,,,3,"MLFlow, Weight and biases",Argo,less than 10 minutes,3,Strongly agree,No,
8/27/2024 16:21:28,I have read this statement in its entirety and affirm the stated conditions.,4,6,1,7,8,7,1,1,,,7,7,6,3,1,1,,"Confusion matrix, Feature importance","pandas, Polars","A fairly standard API between estimators, and many of  the most important ML routines.","CatBoost, LightGBM, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,1,Other,Other,less than an hour,More than 5,Strongly agree,No,
8/27/2024 16:30:15,I have read this statement in its entirety and affirm the stated conditions.,8,4,4,8,5,8,1,4,"I get bullies saying ""you can't scale with sklearn"". I'd love to see tutorials/examples of people deploying sklearn on larger datasets.",Greater integration with polars,6,7,1,1,3,2,4,"Precision-Recall curve, Residual plots, Learning curves","cudf, Dask DataFrame, DuckDB, pandas, Polars, Spark DataFrame",Consistency of API and excellent docs.,"CatBoost, Jax, LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Uncertainty estimates for prediction, Sample weights, Metadata routing",No,NA,NA,4,"MLFlow, Weight and biases, Other","Airflow, Kubeflow, Other","less than 10 minutes, less than an hour, less than a day",More than 5,Strongly agree,Yes,garrettrmooney@gmail.com
8/27/2024 17:39:05,I have read this statement in its entirety and affirm the stated conditions.,8,7,6,7,6,8,3,2,,,7,5,7,5,6,4,,"Confusion matrix, Feature importance, Learning curves",pandas,,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",,,,4,,,"less than 10 minutes, less than an hour, less than a day",,Strongly agree,,
8/27/2024 19:55:08,I have read this statement in its entirety and affirm the stated conditions.,8,5,3,8,4,7,1,1,New features and performance boosts are always welcome!,A more turn key way to import models from statsmodels would be nice!,7,7,5,5,6,7,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,Lots of pre-processing options.,"Transformers, XGBoost",RandomForestClassifier or RandomForestRegressor,Yes,"Uncertainty estimates for prediction, Feature importances",Yes,Offset for logistic regression.,Wouldn’t have to do anything manual or define my own estimator. Though more straightforward integration with statsmodels would also achieve this.,4,,Other,less than 10 minutes,2,Strongly agree,Yes,jacobperius@gmail.com
8/27/2024 20:27:21,I have read this statement in its entirety and affirm the stated conditions.,5,6,2,4,8,7,3,1,,,6,7,3,2,4,5,1,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots, Learning curves",pandas,,"Keras, PyTorch","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,4,,,less than 10 minutes,1,Strongly agree,No,
8/27/2024 20:34:56,I have read this statement in its entirety and affirm the stated conditions.,6,5,4,7,3,8,1,2,"Furthering education materials would be fantastic. The MOOC sklearn team developed is pretty old. Developing better resources for learning the package and ML in general would be fantastic. 

Going the formal route of textbook & lecturer resources could also work to empower college lecturers that heavily use sklearn to teach applied ML. ",I have to bounce between Python and pyspark fairly often. It would be great if the package could better support pyspark implantation if models (like xgboost). ,6,7,3,5,5,4,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","pandas, Polars, Spark DataFrame","So easy, so reliable. Sklearn pipelines is such a good feature. ","CatBoost, LightGBM, Transformers, XGBoost","LogisticRegression, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Feature importances, Sample weights",No,,,3,MLFlow,"Airflow, Custom tool",less than an hour,More than 5,Strongly agree,Yes,Nickbrecht@gmail.com
8/27/2024 22:57:57,I have read this statement in its entirety and affirm the stated conditions.,3,2,2,3,2,2,1,1,,,3,3,3,1,1,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","Dask DataFrame, pandas",,"Keras, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,2,,Airflow,"less than 10 minutes, less than an hour",2,Strongly agree,,
8/28/2024 1:45:09,I have read this statement in its entirety and affirm the stated conditions.,3,3,2,3,2,1,1,2,,,1,2,2,3,1,2,,"Confusion matrix, ROC curve, Precision-Recall curve",pandas,,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Calibration of probabilistic classifiers, Feature importances",,,,5,Weight and biases,,less than a day,2,Strongly agree,Yes,prratadiya@gmail.com
8/28/2024 2:48:55,I have read this statement in its entirety and affirm the stated conditions.,7,3,,8,6,5,4,,"I would like to see good supports from even better integrations that allow use of complex ML models, integrate with pytorch, and working well performance wise with models such as transformers. Easier ways to debug and trace and faster computation in ML packages would be helpful. ",,4,7,3,2,6,5,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Spark DataFrame","Good documentation, easy use of libraries","CatBoost, Jax, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",,,,4,"DVC, MLFlow, Custom tool, Other","Airflow, Argo, Kubeflow, Custom tool","less than a day, more than a day",More than 5,Strongly agree,,
8/28/2024 3:29:28,I have read this statement in its entirety and affirm the stated conditions.,6,8,6,7,8,5,1,1,"I would appreciate more Deep learning capabilities with the usual convenient API, which is one of the best APIs in the ML Sektor in my opinion. ","See above, deep learning capabilities ",4,6,6,7,5,7,,"Confusion matrix, Precision-Recall curve, Feature importance","Modin, pandas",The API Design! Simple and logical,"PyTorch, Other","RandomForestClassifier or RandomForestRegressor, Other",No,Feature importances,No,,,4,Custom tool,Other,"less than an hour, less than a day",3,Strongly agree,No,
8/28/2024 3:37:30,I have read this statement in its entirety and affirm the stated conditions.,8,2,1,6,2,1,1,1,,,6,7,3,2,5,4,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","Dask DataFrame, pandas",,"CatBoost, PyTorch, Transformers","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,1,"DVC, MLFlow",,less than a day,3,Strongly agree,No,
8/28/2024 4:18:45,I have read this statement in its entirety and affirm the stated conditions.,7,7,7,5,5,5,3,,"Training on larger datasets and deploying in real scenarios require performance and packaging. I think the website is great already, as are the docs. ",More methods to organise and run ml experiments,7,5,6,2,2,2,,"Confusion matrix, Feature importance, Residual plots",pandas,"Wide range of algorithms, lots of documentation and examples","Keras, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction",Yes,Weight,Forcing model to learn more from important datapoints ,2,,,less than a day,5,Strongly agree,No,
8/28/2024 4:56:18,I have read this statement in its entirety and affirm the stated conditions.,8,1,1,8,8,8,1,1,Sci-Kit learn is too slow. Make everything use multiple cores. ,"Integrate better with Polars, etc. ",1,7,1,7,5,5,1,"ROC curve, Feature importance, Learning curves","pandas, Polars, Spark DataFrame",,LightGBM,Other,Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing",Yes,Fitting estimators from data that will not be provided at prediction time. ,,1,,"Airflow, Kubeflow",less than a day,More than 5,Strongly agree,No,
8/28/2024 5:26:38,I have read this statement in its entirety and affirm the stated conditions.,6,6,,,,,,,,speeding up training with 'absolute_error' criterion for decision trees,6,6,4,5,3,4,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,,"Keras, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Feature importances",,,,3,"MLFlow, Weight and biases",Airflow,less than an hour,More than 5,Strongly agree,No,
8/28/2024 5:56:01,I have read this statement in its entirety and affirm the stated conditions.,7,7,7,7,1,1,2,,"The documentation lacks basic, simple examples and tutorials","Improve the documentation by including more basic, entry-level tutorials",7,7,2,6,6,7,4,Confusion matrix,pandas,,"PyTorch, Transformers",Other,No,Calibration of regressors,No,,,2,DVC,,less than an hour,1,Strongly agree,No,
8/28/2024 5:57:22,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,1,1,2,4,,,5,6,2,2,3,2,2,"Confusion matrix, ROC curve","pandas, Polars",,PyTorch,LogisticRegression,,"Calibration of probabilistic classifiers, Cost-sensitive learning, Sample weights",No,,,2,"DVC, MLFlow","Dagster, Custom tool",more than a day,1,Strongly agree,No,
8/28/2024 6:45:01,I have read this statement in its entirety and affirm the stated conditions.,4,6,3,5,7,8,2,1,"There is already a breadth of tools and models in scikit-learn. In the new age of ""AI"", I hope that scikit-learn will stick to its roots and focus on traditional machine learning tasks. The new series with Vincent is incredible. The deep dive into boosted trees helped solidify that concept for me. I go back to it as a reference often. How many other algorithms does scikit-learn have where a video tutorial or deep dive would be useful? A nice side effect is that the videos are a gold mine of tips and tricks for scikit-learn. Even when documentation is awesome, which it is, it's no substitute for listening to an expert talk through their thought process while implementing an algorithm.",More human resources. My hope with this request is that new features and bugfixes will be incorporated quicker than they are today. Hopefully it would free up other resources to help with educational content.,6,7,5,3,2,4,1,"ROC curve, Precision-Recall curve, Feature importance, Learning curves, Other",pandas,The pipeline paradigm.,"LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,1,"DVC, MLFlow, Weight and biases",,less than 10 minutes,2,Strongly agree,Yes,arvkevi@gmail.com
8/28/2024 6:49:30,I have read this statement in its entirety and affirm the stated conditions.,7,7,7,7,8,8,7,7,"as users, i need more educational materials/example or use cases.  ",none so far. ,6,7,7,7,6,7,6,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Polars",very easy to use. ,XGBoost,LogisticRegression,No,Uncertainty estimates for prediction,No,,,4,Other,Airflow,less than an hour,1,Strongly agree,No,
8/28/2024 7:24:27,I have read this statement in its entirety and affirm the stated conditions.,2,3,2,2,3,3,1,,,,3,2,3,2,1,1,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Polars",,"LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Metadata routing",,,,2,"MLFlow, Weight and biases, Custom tool","Airflow, Metaflow (outerbounds)","less than 10 minutes, less than an hour",More than 5,Strongly agree,,
8/28/2024 7:25:26,I have read this statement in its entirety and affirm the stated conditions.,5,5,5,8,5,5,5,,,dataframe agnostic support,7,7,6,3,4,2,,"Confusion matrix, ROC curve, Feature importance","pandas, Polars",Unified API and pipelines,LightGBM,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Non-euclidean metrics",No,,,1,"DVC, MLFlow",Custom tool,less than a day,More than 5,Strongly agree,Yes,cedric.wilting+scikit-learn@gmail.com
8/28/2024 7:53:38,I have read this statement in its entirety and affirm the stated conditions.,7,4,4,6,5,4,2,,sklearn is already quite reliable and phenomenally documented. The reliance on pandas/joblib makes performance one of the most critical issues to me,less headaches when parallelizing work (e.g. ColumnTransformer performance regressions),7,7,4,2,1,2,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","DuckDB, pandas, Polars",The API and great docs,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Feature importances, Metadata routing",No,,,2,MLFlow,,less than a day,More than 5,Strongly agree,No,
8/28/2024 8:13:00,I have read this statement in its entirety and affirm the stated conditions.,7,8,7,7,5,6,3,,,"General support for forecasting tasks. Specifically what is missing the most are index-conscious operations such as moving/expanding window transformers, index feature extraction etc. That being said I understand that not supporting structured data like that has been an early design decision for scikit-learn (and probably for the best)",4,4,7,5,4,2,4,"Confusion matrix, Feature importance, Residual plots, Learning curves, Other","pandas, Polars, Spark DataFrame","the beauty and simplicity of the API, how easy it is to create custom/complicated model pipelines that still behave like simple Estimators API-wise and the documentation","LightGBM, Other","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction",No,,,2,"MLFlow, Custom tool",Other,less than 10 minutes,More than 5,Strongly agree,Yes,leonidas.tsap@gmail.com
8/28/2024 9:12:10,I have read this statement in its entirety and affirm the stated conditions.,6,8,8,3,2,1,1,1,,,5,5,4,7,7,3,1,"Confusion matrix, ROC curve, Feature importance","pandas, Spark DataFrame",,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances, Sample weights",No,,,2,MLFlow,Airflow,less than a day,3,Strongly agree,No,
8/28/2024 11:54:19,I have read this statement in its entirety and affirm the stated conditions.,4,2,5,1,3,6,7,,Sklearn should prioritise integrating innovation,Allows infinite values in tree estimators ,2,1,3,5,4,6,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Polars",Ease of use,"CatBoost, Keras, LightGBM, PyTorch, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing",Yes,Costs ,Allows better business oriented decisions,5,"MLFlow, Custom tool",Custom tool,less than a day,3,Agree,Yes,Lucas.cr.morin@gmail.com
8/28/2024 12:27:22,I have read this statement in its entirety and affirm the stated conditions.,,,,,,,,,"I like sklearn as it is, and would prioritize nothing in particular. Everything is good",/,4,7,3,2,6,5,1,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves",pandas,Everything <3,"CatBoost, Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning",No,,,2,Custom tool,,less than a minute,,Strongly agree,No,
8/28/2024 14:56:00,I have read this statement in its entirety and affirm the stated conditions.,3,2,3,1,2,2,1,1,,,3,1,2,3,3,2,1,"Confusion matrix, Precision-Recall curve, Learning curves","Dask DataFrame, pandas",,"Keras, PyTorch",RandomForestClassifier or RandomForestRegressor,Yes,"Calibration of regressors, Feature importances, Non-euclidean metrics",No,,,4,MLFlow,Custom tool,less than a day,More than 5,Strongly agree,No,
8/28/2024 17:30:51,I have read this statement in its entirety and affirm the stated conditions.,5,5,5,7,2,1,1,1,"Selection above is a bit confusing as it can be already good and no need for improvement (e.g., reliability) or it doesn't matter as much in my opinion (e.g., website redesign)",,5,5,5,6,7,5,,Other,pandas,,PyTorch,,No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Sample weights, Non-euclidean metrics",No,,,5,Weight and biases,,less than a day,3,Agree,No,
8/28/2024 19:50:20,I have read this statement in its entirety and affirm the stated conditions.,2,3,4,8,5,6,5,,Great product!  It would be great to see more learners that could accept categorical data; more support for neural networks; if pandas/polars is passed in the return pandas/polars; some better support for time series; classical ANOVA and regression support , If pandas/polars is passed in the return pandas/polars; ,5,7,6,3,4,2,1,"Confusion matrix, Feature importance, Learning curves","pandas, Polars",The APIs are very consistent; nice broad spectrum of products; lots of options on the APIs,"CatBoost, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Other",No,"Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",No,N/A,N/A,2,Weight and biases,,"less than 10 seconds, less than a minute, less than 10 minutes, less than an hour",2,Strongly agree,Yes,allen.v.wass@gmail.com
8/29/2024 6:21:20,I have read this statement in its entirety and affirm the stated conditions.,4,5,6,8,7,3,2,,,,7,6,7,7,4,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,Ease of use. Documentation ,PyTorch,"RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Sample weights",No,,,4,MLFlow,Custom tool,less than a day,4,Strongly agree,No,
8/29/2024 15:29:34,I have read this statement in its entirety and affirm the stated conditions.,8,4,4,8,8,4,1,1,"These are things I value, and can alway be better",More advanced neural network,7,7,1,1,1,1,4,"Confusion matrix, ROC curve, Feature importance, Other","cudf, pandas, Spark DataFrame",Easy cookie cutter usage,PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,,No,,,5,Weight and biases,,"less than 10 seconds, less than a minute, less than 10 minutes",1,Strongly agree,Yes,dfraser206@gmail.com
8/29/2024 18:01:49,I have read this statement in its entirety and affirm the stated conditions.,5,5,3,8,6,8,1,1,"My career was partly built off the back of reading the sklearn docs and trying out all the different estimators and classifiers as a student. Most of it didn't work, but that's part of the experience. I also learned a lot of plotting and numpy along the way from the great docs. Hence: new features, and more documentation pls!","Custom loss functions for random forest classifier. No idea if this is feasible, but I think xgboost has something like this. Ranking loss would be my first use case (e.g. rank this item within this group higher than another item in the same group, but ignore other groups). I use the RAX library in jax for this. For logreg this can be achieved by adding dummy encoding - a new column indicating which group an item comes from, but that wouldn't work for RFC because of feature bagging. ",5,7,1,1,3,6,1,"ROC curve, Precision-Recall curve","DuckDB, pandas",LogReg is fast. Library installs and Just Works everywhere Ive tried it. Handles sparse inputs. Often some good hyperparam settings. ,"Jax, PyTorch, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Uncertainty estimates for prediction, Sample weights",Yes,"Ranks and groups. Say I have ten groups with unknown baselines. I know the baseline is constant within a group, so I want to rank within this group, but I don't know the absolute magnitudes of the baselines so I don't want to compare across groups. ","This would let me use Random Forest Classifier for my task, for which (at the moment) I can only use logreg with dummy encoding. ",2,,,less than 10 minutes,3,Strongly agree,Yes,lewis.martin44@gmail.com
8/30/2024 2:48:13,I have read this statement in its entirety and affirm the stated conditions.,3,3,1,3,3,2,1,1,,,3,3,2,2,1,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,Consistency and variety ,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,2,Custom tool,,"less than 10 seconds, less than a minute, less than 10 minutes, less than an hour",1,Strongly agree,,
8/30/2024 3:42:00,I have read this statement in its entirety and affirm the stated conditions.,4,5,4,7,4,5,4,1,I think it should try to cover the largest set of ML algorithms.,Maybe the possibility to automate some pipelines.,7,7,6,3,,5,1,"ROC curve, Feature importance",pandas,fit predict,"PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Uncertainty estimates for prediction, Feature importances",No,,Don't know ,2,Custom tool,,less than an hour,1,Strongly agree,Yes,r.guidotti@unich.it
8/30/2024 8:35:06,I have read this statement in its entirety and affirm the stated conditions.,,8,,8,8,,,,"Robustness, traceability, certification of code, methods",p-values :D,6,6,6,7,7,7,,"Confusion matrix, ROC curve, Feature importance","Dask DataFrame, pandas",API simplicity,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning",No,,,5,,,less than a day,2,Strongly agree,No,mario.lovric@inantro.hr
8/30/2024 8:36:34,I have read this statement in its entirety and affirm the stated conditions.,3,3,2,4,4,4,1,1,"What’s really making the difference, in my opinion, is the high quality of sklearn documentation.",Probably a better DataFrame support,5,4,7,6,4,3,1,"Confusion matrix, ROC curve, Feature importance, Residual plots, Learning curves","pandas, Polars",Its consistency ,"Keras, LightGBM, Transformers, XGBoost","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Metadata routing",No,,,3,MLFlow,Other,less than a day,More than 5,Strongly agree,No,
8/31/2024 19:06:29,I have read this statement in its entirety and affirm the stated conditions.,8,6,6,7,6,4,4,8,,GPU-Optimized shallow learning algorithms would be amazing,5,7,6,4,6,4,,"Confusion matrix, ROC curve, Feature importance, Residual plots","pandas, Spark DataFrame",The variety of ML tools,"LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",Yes,,,5,MLFlow,"Airflow, Other",less than a day,1,Strongly agree,No,
9/1/2024 13:36:47,I have read this statement in its entirety and affirm the stated conditions.,8,5,2,7,6,4,3,1,Essential is being able to do fast prototyping and fast testing. It works pretty well today and new features sound like a very good thing.,Bayesian modeling ,7,5,6,3,4,2,1,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves","pandas, Other",Compatibility with external libraries and fast prototyping,"Keras, PyTorch, XGBoost, Other","LogisticRegression, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",Yes,"Weights (generally), distributions for bayesian models",Better accuracy for output in real life,2,"Weight and biases, Custom tool","Custom tool, Other","less than 10 seconds, less than 10 minutes",More than 5,Strongly agree,Yes,lsgiorello@yahoo.fr
9/2/2024 11:27:59,I have read this statement in its entirety and affirm the stated conditions.,8,5,5,8,6,3,3,2,,,7,3,5,6,6,5,1,"Feature importance, Residual plots, Learning curves, Other","cudf, pandas, Polars",,"Keras, LightGBM, PyTorch, XGBoost","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,"Time series groupings, i.e. disconected segments of a time series",,2,MLFlow,,less than an hour,1,Strongly agree,,
9/2/2024 11:33:41,I have read this statement in its entirety and affirm the stated conditions.,5,8,1,4,1,5,1,1,"Scikit-learn's main focus should be to provide useful tools for the data scientist. My first concern with tools will always be reliability while performance needs to follow for the tool to remain usable. As new standard techniques are created, new features need to be implemented. Scikit-learn is also commonly used by new data scientists to learn the basics of the science. Hence, a secondary focus on educational materials helps the industry.",A set of default pipelines (both preprocessing only and classifiers) that reliably work on any tabular dataset to quickly observe the performance of different classifiers on the given task. ,3,7,2,4,5,6,1,"Confusion matrix, ROC curve, Feature importance, Residual plots, Other",pandas,Its documentation and its ease of use.,"PyTorch, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",Yes,The allowed complexity of the model (currently done through hyperparameters).,It helps with avoiding overfitting and provides a model which I understand better.,2,Custom tool,Custom tool,"less than 10 seconds, less than a minute, less than 10 minutes, less than an hour",More than 5,Strongly agree,No,
9/2/2024 12:37:05,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,8,excellent,excellent,7,7,7,7,7,7,7,"Confusion matrix, Reliability diagram, ROC curve",cudf,excellent,CatBoost,"Pipeline, ColumnTransformer",Yes,"Sample weights, Non-euclidean metrics",Yes,excellent,excellent,5,"DVC, MLFlow, Custom tool","Airflow, Argo",less than an hour,More than 5,Strongly Disagree,Yes,excellent
9/2/2024 13:05:30,I have read this statement in its entirety and affirm the stated conditions.,5,7,8,,8,7,5,,,,5,7,,,,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Spark DataFrame",,"CatBoost, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",,Feature importances,,,,4,MLFlow,"Airflow, Kubeflow",less than an hour,2,Agree,No,
9/2/2024 13:28:10,I have read this statement in its entirety and affirm the stated conditions.,5,2,2,3,5,7,2,1,,,3,4,4,6,6,5,3,"Confusion matrix, ROC curve, Feature importance, Residual plots","pandas, Spark DataFrame",,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",No,,,4,,,"less than 10 minutes, less than an hour",1,Strongly agree,No,
9/2/2024 15:36:25,I have read this statement in its entirety and affirm the stated conditions.,8,4,4,7,4,5,4,4,,,6,7,6,6,5,6,1,"Confusion matrix, Feature importance","pandas, Polars",,XGBoost,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Cost-sensitive learning, Feature importances, Sample weights",No,,,5,,,less than a day,3,Strongly agree,No,
9/2/2024 15:53:11,I have read this statement in its entirety and affirm the stated conditions.,1,1,1,8,1,8,1,1,Nothing to wish except for more those reliable useful functions and complex examples,"1) make sparse vector available in train_test_split for stratify parameter, 2) make activation function a list of constants for MLP() function. RN we can only choose one activation functions for all the layers. 3) make support for keras NN in your Pipeline (not sure if it is your field or keras team have to work on it)",2,6,5,4,4,3,1,"Confusion matrix, Feature importance, Learning curves",pandas,Pipeline(),"Keras, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Feature importances, Sample weights",No,,,5,,,less than a day,1,Strongly agree,Yes,levakin.alex@gmail.com
9/3/2024 3:17:15,I have read this statement in its entirety and affirm the stated conditions.,7,8,6,7,8,6,1,,"As a research engineer in the industry, I am really interested in a well-documented reliable package",,6,6,4,5,5,2,,"Confusion matrix, Feature importance, Residual plots, Learning curves",pandas,Ease of use and flexibility,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",No,"Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",,,,4,MLFlow,,"less than an hour, less than a day",More than 5,Strongly agree,No,
9/3/2024 3:54:13,I have read this statement in its entirety and affirm the stated conditions.,8,,,8,,,,,"I think that if scikit-learn could handle bigger than memory datasets and utilize all the cores on the computer this would be a huge benefit for the package.

The other big improvement would be to add new Transformers to the main package.",Handle bigger than memory datasets.,5,7,4,3,5,7,,"ROC curve, Feature importance","pandas, Polars",It's simplicity.,"CatBoost, LightGBM, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,2,MLFlow,"Airflow, Kubeflow",less than a day,More than 5,Strongly agree,Yes,nicolaepopescul@gmail.com
9/3/2024 4:26:32,I have read this statement in its entirety and affirm the stated conditions.,3,3,2,3,3,2,3,3,Faster code execution ,Na,3,3,3,2,3,2,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Learning curves","Polars, Spark DataFrame",Docs,"CatBoost, Keras, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,NA,Na,5,"MLFlow, Weight and biases","Airflow, Kubeflow, Custom tool",less than an hour,2,Neither agree nor disagree,No,avishek84@yahoo.co.in
9/3/2024 4:45:04,I have read this statement in its entirety and affirm the stated conditions.,2,7,3,8,6,3,1,,,"More variants of cross-validation, including some that allow to specify pattern like stratification.",7,7,3,2,2,5,,Confusion matrix,pandas,preprocessing features,"LightGBM, PyTorch, XGBoost","Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Metadata routing",Yes,"sample weights, eval sets",allow early stopping,2,,,less than 10 minutes,1,Strongly agree,Yes,david.steyrl@univie.ac.at
9/3/2024 4:48:52,I have read this statement in its entirety and affirm the stated conditions.,5,8,7,8,7,6,5,4,"There are still some important issues related to pre-processing optimization within Pipeline such as https://github.com/scikit-learn-contrib/imbalanced-learn/issues/787 that should really be addressed. Additional ""canonical"" features/models, in scope with scikit-learn, could also be added. GPU compatibility, while probably not in scope, would be great to have. Finally, more feature importance/interpretability methods (like SHAP), should probably be added","Interpretability methods for model interpretation (ideally with a ""direction"" of influence over the result, for each feature in the model)",7,7,7,7,3,4,4,"ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,"""Modular"" structure and clean, general interface, very easy to use. Almost all the best tools for ML are available","CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Uncertainty estimates for prediction, Feature importances",,,,4,,,"less than 10 minutes, less than an hour",3,Strongly agree,No,
9/3/2024 7:21:47,I have read this statement in its entirety and affirm the stated conditions.,8,8,4,6,4,1,1,1,"To me, the most important improvement would be performance, which would allow me to use scikit-learn in a professional setting.",Performance (speed of ML estimators),7,2,5,1,6,2,,Other,pandas,The completeness of the package and the many many options for modeling with Pipeline objects.,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,"If so, it would be a GroupByEstimator to fit a model for each group. scikit-lego has a version of this.",,3,Custom tool,Custom tool,less than a day,More than 5,Strongly agree,No,
9/3/2024 7:29:16,I have read this statement in its entirety and affirm the stated conditions.,5,3,3,4,3,2,2,1,,,5,6,4,4,4,5,2,"Confusion matrix, Feature importance, Residual plots, Learning curves","pandas, Polars",,PyTorch,"RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,Feature importances,No,,,3,,,less than 10 minutes,3,Strongly agree,No,
9/3/2024 9:46:49,I have read this statement in its entirety and affirm the stated conditions.,5,8,6,5,7,6,3,2,,,3,7,1,,6,5,2,"Confusion matrix, Feature importance, Learning curves",pandas,,Keras,RandomForestClassifier or RandomForestRegressor,,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",,,,3,,,less than an hour,,Strongly agree,No,
9/3/2024 9:50:32,I have read this statement in its entirety and affirm the stated conditions.,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"Flags for synchronicity of samples in simultaneous multi-estimator operations. Error costs to encourage narrower fit for more problematic classes and wider fit for less costly (""failsafe"") classes.",,,,,,,,No,
9/3/2024 11:18:11,I have read this statement in its entirety and affirm the stated conditions.,5,6,3,4,2,6,3,1,"I feel scikit-learn's technical documentation and website are already quite strong. My one quibble is the landing page for the website is organized different (and better, IMO) than the pages you are taken to once you click on something. I'd maybe address that. But after higher-impact activities like new features, reliability, etc.",,5,7,4,4,6,6,3,"Confusion matrix, ROC curve, Feature importance, Residual plots","DuckDB, pandas, Spark DataFrame","Its API is consistent from algorithm to algorithm, and it is simple.","CatBoost, Keras, LightGBM, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,4,MLFlow,Airflow,"less than a minute, less than 10 minutes, less than an hour",2,Strongly agree,No,
9/3/2024 19:59:24,I have read this statement in its entirety and affirm the stated conditions.,7,4,8,6,5,3,2,1,"It would be great for common performance optimizations to be packaged in even easier to use ways, such as meta-models which come packaged with common transformers, scalers, and approximators so that the most common pipelines can all be accomplished in one step.","Eliminating the need to transform Pandas dataframes into Numpy arrays and back--I am always concerned about mislabeling my data when I convert it back, and sometimes that does happen.",5,2,7,4,6,3,1,"Confusion matrix, Feature importance",pandas,"The packaging is really convenient, which gives me peace of mind.",,HistGradientBoostingRegressor or HistGradientBoostingClassifier,No,"Uncertainty estimates for prediction, Feature importances",No,,,1,,,"less than 10 seconds, less than 10 minutes, less than a day, more than a day",1,Strongly agree,Yes,h.mustafa.mail@gmail.com
9/3/2024 23:00:30,I have read this statement in its entirety and affirm the stated conditions.,2,2,2,2,2,1,1,2,Train the youth for the future.,Make there on cloud base database to use.,1,1,2,2,2,2,2,"Confusion matrix, Reliability diagram, ROC curve, Learning curves",pandas,Solving the probelms,"Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,Map of Sk Learn,Yes,1,Weight and biases,Kubeflow,less than 10 seconds,More than 5,Strongly agree,Yes,ghulammohyuddinzafar@gmail.com
9/4/2024 2:46:54,I have read this statement in its entirety and affirm the stated conditions.,8,8,6,8,6,6,7,5,"Im care more about the speed performance and reliability of sklearn. The current packaging of sklearn is good. If possible, I would like sklearn to contain some popular packages, e.g., umap. Docs are fine to me. Make the website cooler if possible.",Include umap into sklearn,7,6,5,5,7,6,4,"Confusion matrix, ROC curve, Feature importance, Residual plots, Learning curves","pandas, Polars",BaseEstimator,"Keras, LightGBM, XGBoost","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Uncertainty estimates for prediction, Feature importances",No,None,None,4,Custom tool,Custom tool,less than 10 minutes,1,Strongly agree,Yes,matthew.szhang91@gmail.com
9/4/2024 3:02:41,I have read this statement in its entirety and affirm the stated conditions.,7,7,4,6,5,7,2,2,,,7,5,2,2,4,4,3,Learning curves,,,,Other,Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",,,,3,,,less than 10 minutes,,Strongly agree,,
9/4/2024 4:27:08,I have read this statement in its entirety and affirm the stated conditions.,7,6,,8,8,7,,,I would like to have some capabilities to develop deep learning models as well...,,6,7,6,5,5,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,Documentation and examples,"Keras, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,Feature importances,No,,,2,,,less than an hour,2,Strongly agree,No,merve@mersis.com.tr
9/4/2024 4:45:35,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,8,,,,,,,,,,,,,,,,,,,,,,,,More than 5,,,
9/4/2024 4:52:25,I have read this statement in its entirety and affirm the stated conditions.,4,4,4,4,4,4,4,4,no,deleting it would be funny,6,7,5,4,3,2,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","Dask DataFrame, pandas",it's cool as,"Keras, LightGBM, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Feature importances, Metadata routing",No,"I said no dude, why doesn't this only pop up if I choose Yes as the previous answer? And if the answer to that is ""Google Forms don't have that functionality"", then maybe you should choose a different form for your survey.",bro,1,Other,Other,less than an hour,1,Strongly agree,No,
9/4/2024 8:06:07,I have read this statement in its entirety and affirm the stated conditions.,1,2,4,3,5,6,7,8,Performance has been something I have had to look outside of Scikit for,GPU based ML models,3,1,4,5,6,2,7,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","cudf, pandas, Spark DataFrame",Commonality between models and extensive library,"Keras, LightGBM, PyTorch, XGBoost",RandomForestClassifier or RandomForestRegressor,No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,1,,,"less than an hour, less than a day",More than 5,Strongly agree,No,
9/4/2024 8:13:21,I have read this statement in its entirety and affirm the stated conditions.,5,1,3,8,6,8,5,1,,Add Extended Isolation Forest model,7,7,7,7,2,4,1,"Confusion matrix, Feature importance, Learning curves",pandas,A lot of models,"CatBoost, LightGBM, PyTorch, XGBoost",Pipeline,No,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances",No,,,3,,Airflow,less than an hour,3,Strongly agree,No,
9/4/2024 9:13:25,I have read this statement in its entirety and affirm the stated conditions.,8,6,7,5,4,3,1,2,,,6,7,4,5,3,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","cudf, DuckDB, pandas, Polars, Spark DataFrame",,"CatBoost, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",Yes,,,3,"MLFlow, Weight and biases",Airflow,"less than 10 minutes, less than an hour, less than a day",4,Strongly agree,,
9/4/2024 9:24:14,I have read this statement in its entirety and affirm the stated conditions.,8,5,5,7,8,8,1,1,,,,7,,,,,,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves",pandas,,"PyTorch, Transformers",,,,,,,4,Other,Other,more than a day,2,Strongly agree,Yes,srirakshith@fiddler.ai
9/4/2024 9:34:02,I have read this statement in its entirety and affirm the stated conditions.,5,2,2,8,8,6,1,1,New features like categorical feature support for all ML-methods that theoretically support them would be a high priority. Also other algorithms/ training methods for existing ML-methods would be great.,Implementation of GUIDE for Decision Trees and categorical feature support for DTs,7,6,1,1,1,3,1,"Residual plots, Learning curves",pandas,"The interface (like fit, predict, score, ...) , in which all methods fit in.",PyTorch,"RandomForestClassifier or RandomForestRegressor, Other",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning",No,,,3,,,less than a minute,2,Strongly agree,No,
9/4/2024 14:36:27,I have read this statement in its entirety and affirm the stated conditions.,8,6,2,4,2,2,1,1,"Performance is always the greatest bottleneck for my projects. You have already addressed that in many ways, with many excellent algorithms but it's the kind of thing that always needs more... An obvious topic would be GPU support, focusing on NON-Nvidia GPUs which are available.","GPU support, especially if it can be wide rather than ""deep"": not just Nvidia GPUs but enable low-cost GPUs available on most laptops using the graphics-GPUs that are common... Of course, that would mean you don't max out the possibilities (which is why I say wide not deep). But it would offer a major boost at low cost.",5,7,2,3,1,3,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,"It is both wide and deep. I simply use it as my go-to tool, its where I begin looking.","Keras, PyTorch, Transformers, XGBoost, Other","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",,,,5,,,"less than 10 minutes, less than an hour, less than a day",More than 5,Strongly agree,No,
9/4/2024 16:26:33,I have read this statement in its entirety and affirm the stated conditions.,7,6,3,4,8,5,2,1,"Documentation and variable/function names are still far from adequate to learning. We need intuitive names and clear documentation, with simple examples for every configuration. For reference, one very adequate documentation and variables names are in Mathematica and Wolfram Alpha from Wolfram.","More clear and specific error mensagens, with possible solutions.",3,6,5,2,4,7,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,"It is very simples and ready to use, has a great number of methods and has manu compatibility of other libraries.","Keras, LightGBM, XGBoost","Pipeline, Other",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,3,,,less than a day,More than 5,Neither agree nor disagree,Yes,danilo.capovilla@hotmail.com
9/4/2024 18:04:15,I have read this statement in its entirety and affirm the stated conditions.,3,4,5,6,8,7,1,,"As the go-to pkg for most DS/ML tasks, I personally think sklearn's technical documentation and educational materials are lacking.  Would like to see more docs and examples to that purpose.  Better packaging is along the same thought process about making skl even easier to use.  Performance and reliability on the other hand are not typical concerns for dev env projects.  ",More sample codes and use cases,5,7,7,6,6,6,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,modules for full model dev cycle,"CatBoost, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",No,"Calibration of probabilistic classifiers, Feature importances",No,,,1,,Airflow,less than a day,More than 5,Strongly agree,No,
9/4/2024 21:00:28,I have read this statement in its entirety and affirm the stated conditions.,6,6,6,6,5,5,1,4,"I think the Website has no need for redesign. It's one of the best documentation available out there, making accessible a very easy to understand. I think new features is always good, especially if they are turned into automation task. Ways to build more complex stacking systems ",Bayesian Optimization and data-cleaning/manipulation. ,6,7,7,1,2,2,4,"ROC curve, Feature importance",pandas,the API is intuitive. I can't think that I'd ever had learned Machine Learning if it were not by sklearn. ,"CatBoost, Keras, LightGBM, XGBoost","Pipeline, ColumnTransformer, Other",No,"Calibration of regressors, Feature importances",No,,,4,Other,Other,less than an hour,,Strongly agree,No,
9/5/2024 4:23:51,I have read this statement in its entirety and affirm the stated conditions.,8,3,1,3,3,1,1,1,would GPU acceleration help performance?,,5,7,1,4,7,6,1,Confusion matrix,pandas,"simple consistent api, good documentation, good coverage","PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Sample weights",No,,,5,Other,,less than a day,,Strongly agree,No,
9/5/2024 6:26:25,I have read this statement in its entirety and affirm the stated conditions.,7,7,7,4,7,7,5,6,,,7,7,7,7,7,7,7,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","pandas, Spark DataFrame",,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,3,"MLFlow, Weight and biases",Airflow,less than 10 minutes,More than 5,Agree,No,
9/5/2024 7:42:31,I have read this statement in its entirety and affirm the stated conditions.,8,5,3,8,6,4,2,1,.,"Ability to also filter by samples in a pipeline. For example you can tranform, construct and filter features in multiple ways, but sklearn has no transformers / estimators that would operate on training samples.",7,7,1,2,5,5,1,"Confusion matrix, ROC curve, Feature importance",pandas,"The abstractions for common operations (GridSearch, cross validation), all implemented metrics and preprocessing methods...","PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances, Sample weights",Yes,Sample metadata,It would allow to filter also by samples,3,"DVC, Weight and biases",,"less than 10 seconds, less than an hour",More than 5,Strongly agree,No,.
9/5/2024 9:25:24,I have read this statement in its entirety and affirm the stated conditions.,3,4,4,2,3,4,3,,,,4,3,3,3,3,4,,"Confusion matrix, ROC curve",pandas,,PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,4,,,"less than 10 seconds, less than a minute",2,Strongly agree,No,
9/5/2024 14:42:08,I have read this statement in its entirety and affirm the stated conditions.,7,3,2,8,4,6,1,3,"Inclusion of algorithms in the learning to rank family, gpu utilization on discrete gpus and  arm chipsets",,3,4,7,6,5,4,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","cudf, DuckDB, pandas, Polars",Completeness of documentation and accessible language,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost, Other","RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",No,,,4,"MLFlow, Weight and biases","Airflow, Argo","less than a minute, less than an hour",More than 5,Strongly agree,,
9/5/2024 21:14:57,I have read this statement in its entirety and affirm the stated conditions.,7,5,1,6,8,8,5,,"Even though generally better than most documentation, the subject of sklearn can be quite arid, specially for starters. Besides, the library is now so vast, a broad, friendly and appealling material would be most welcome.",Connection of the implementation and theoretical background for it.,5,5,2,6,6,6,,"Confusion matrix, Precision-Recall curve, Residual plots, Learning curves","Dask DataFrame, pandas",It's vastness. scikit-learn has been my one-stop library for machine learning needs.,"LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,5,,,less than 10 minutes,,Strongly agree,Yes,5968qgl68@mozmail.com
9/6/2024 3:35:19,I have read this statement in its entirety and affirm the stated conditions.,7,7,3,8,8,8,1,3,,,6,7,5,5,6,6,4,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","DuckDB, pandas, Polars",,"LightGBM, PyTorch, Transformers, XGBoost",RandomForestClassifier or RandomForestRegressor,No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",,,,3,"DVC, Weight and biases",Airflow,"less than 10 minutes, less than an hour, less than a day",More than 5,Strongly agree,No,
9/6/2024 7:34:20,I have read this statement in its entirety and affirm the stated conditions.,8,8,7,8,6,8,7,7,Visualization tools,More focus on deep learning (neural networks),2,3,2,3,2,2,2,"Confusion matrix, ROC curve, Feature importance, Residual plots, Learning curves",pandas,"Documentation, website examples","CatBoost, Keras, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Feature importances",No,,,5,MLFlow,Airflow,"less than 10 minutes, less than a day",1,Strongly agree,No,
9/6/2024 7:55:19,I have read this statement in its entirety and affirm the stated conditions.,6,6,3,6,6,5,3,,"I think the most important properties of sklearn are performance, reliability, and a steady flow of new features, while good documentation is very important for developers.",,5,6,4,2,6,4,,"Confusion matrix, Residual plots",pandas,I like the fact that different training systems use same method names (i.e. fit_transform),"Transformers, XGBoost",RandomForestClassifier or RandomForestRegressor,No,Uncertainty estimates for prediction,No,,,3,,,less than a minute,,Strongly agree,No,
9/6/2024 9:48:11,I have read this statement in its entirety and affirm the stated conditions.,4,4,4,8,3,8,3,,,,4,5,7,7,7,7,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,API universality,"CatBoost, Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Non-euclidean metrics",No,,,3,Custom tool,,less than 10 minutes,,Strongly agree,Yes,romius2001@gmail.com
9/6/2024 9:58:33,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,1,5,4,4,,Adding new models like transformers,Transformers.,6,7,5,,,,,"ROC curve, Precision-Recall curve, Feature importance",pandas,the supporting tools to understand the model performance and decision making,"Keras, Transformers, XGBoost",LogisticRegression,No,"Uncertainty estimates for prediction, Feature importances, Sample weights",,,,5,,,less than a day,More than 5,Strongly agree,Yes,kareememadfarid@gmail.com
9/6/2024 10:38:47,I have read this statement in its entirety and affirm the stated conditions.,8,5,3,7,4,8,8,4,,educational materials about all algorithms with math behind it,6,7,3,5,5,5,3,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","pandas, Polars, Spark DataFrame",number of written modules,"CatBoost, Jax, LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,5,Weight and biases,Airflow,more than a day,3,Strongly agree,No,
9/6/2024 10:43:31,I have read this statement in its entirety and affirm the stated conditions.,5,8,6,4,8,8,2,4,,,5,7,5,6,5,7,4,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Polars",Its ease of use at first glance but the possibility to go very deep as well,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Uncertainty estimates for prediction, Feature importances, Metadata routing",No,,,3,MLFlow,Airflow,"less than a minute, less than 10 minutes",2,Strongly agree,No,
9/6/2024 11:38:37,I have read this statement in its entirety and affirm the stated conditions.,8,7,6,5,4,3,2,1,The guides and educational resources are good enough already.,Inclusion of imbalanced classification learning undersampling and oversampling capabilities that are easy to use.,3,7,2,6,5,4,1,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots, Learning curves, Other","pandas, Polars","It is easy to use, very complete and quite fast and intuitive.","LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Other",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Sample weights",No,,,4,"Custom tool, Other","Airflow, Metaflow (outerbounds)","less than an hour, less than a day",More than 5,Strongly agree,No,
9/6/2024 12:33:02,I have read this statement in its entirety and affirm the stated conditions.,,,,,4,4,4,,"More comprehensive and easy to understand docs and tutorials like that of Android Studio, Firebase etc.",Improved documentation.,4,4,1,,,4,,"Feature importance, Residual plots",pandas,Good friend.,,RandomForestClassifier or RandomForestRegressor,No,Feature importances,No,,,,,,less than an hour,More than 5,Strongly agree,,
9/6/2024 13:49:52,I have read this statement in its entirety and affirm the stated conditions.,8,4,4,5,7,3,4,8,Memory management and parallel processing for large (30GB) datasets locally.,Memory management,5,7,4,3,7,3,3,"Confusion matrix, Feature importance","Dask DataFrame, pandas",Comprehensive features,Keras,HistGradientBoostingRegressor or HistGradientBoostingClassifier,No,"Feature importances, Sample weights",Yes,Class imbalance,Automatically tune training to account for class imbalance,1,,,less than a day,1,Neither agree nor disagree,No,
9/6/2024 13:51:50,I have read this statement in its entirety and affirm the stated conditions.,6,7,5,4,7,5,2,1,,,6,6,6,1,7,7,4,"ROC curve, Residual plots, Learning curves",pandas,"The clean focus on floating point matrices (numpy ndarrays) makes the logic of how I use sklearn super transparent. Compared to other data science libraries, sklearn feels ""clean"" in its APIs, and I find this very appealing, even if I am forced to do more of my own data prep work outside of sklearn (e.g. dealing with string-valued categorical variables).","LightGBM, PyTorch, Transformers","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,Uncertainty estimates for prediction,,,,1,"Weight and biases, Custom tool",Custom tool,"less than 10 minutes, more than a day",3,Strongly agree,Yes,lukemerrick@gmail.com
9/6/2024 14:07:22,I have read this statement in its entirety and affirm the stated conditions.,6,7,2,2,6,4,1,1,"Performance and reliability of the estimators provided in scikit learn are already at good place, however, these are things that could always get better and better over time. The documentation too could be improved too. The rest are not really very important",rust bindings,4,4,2,2,4,4,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Polars",,"PyTorch, Transformers","RandomForestClassifier or RandomForestRegressor, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,2,,,less than 10 minutes,,Strongly agree,Yes,omokennanna832@gmail.com
9/6/2024 17:53:30,I have read this statement in its entirety and affirm the stated conditions.,6,6,7,8,6,4,3,,It is currently in a good state. There are already plenty of educational materials. I don't think that needs as much attention. New features are always welcome,Better optimizers are really needed. GPU access would be great.,5,6,7,4,,3,2,"Confusion matrix, Feature importance, Residual plots, Learning curves, Other",pandas,Range of features and stability,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",Yes,"Forecast horizons, lags",Useful for time series data,4,"DVC, Other","Custom tool, Other","less than 10 seconds, less than a minute, less than 10 minutes, less than an hour, less than a day, more than a day",More than 5,Strongly agree,Yes,christian.oleary@mtu.ie
9/7/2024 11:34:16,I have read this statement in its entirety and affirm the stated conditions.,6,7,4,8,4,3,1,1,,Additional Anomaly/Outlier Detection Models and more consistency in their implementation,4,4,4,7,4,4,4,"ROC curve, Precision-Recall curve, Feature importance",pandas,"Wealth of models, reliability, performance, documentation","Keras, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,,No,,,4,MLFlow,,"less than 10 seconds, less than a minute, less than 10 minutes, less than an hour",4,Strongly agree,Yes,chamxs@gmail.com
9/7/2024 17:31:46,I have read this statement in its entirety and affirm the stated conditions.,6,4,4,5,3,6,1,,,,6,6,4,5,5,4,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",,,,4,"MLFlow, Weight and biases, Custom tool",Other,"less than 10 seconds, less than a minute, less than 10 minutes, less than an hour",1,Agree,No,
9/7/2024 18:38:05,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,5,6,6,1,5,The docs could be better,better docs,6,7,7,2,5,1,4,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Polars",Simplicity,"Keras, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,5,"MLFlow, Weight and biases","Argo, Kubeflow","less than a day, more than a day",3,Agree,No,
9/8/2024 1:06:55,I have read this statement in its entirety and affirm the stated conditions.,8,2,2,7,6,6,1,1,,include bayesian optimization for hyperparameter search,6,5,3,3,5,4,1,"Confusion matrix, Feature importance","pandas, Polars",documentation,"PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,2,,,less than an hour,More than 5,Strongly agree,No,
9/8/2024 3:08:38,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,,Performance along with ease of use + good documentation is a killer combination.,"Ease of use of the package, with clear examples illustrating the use of the feature.",7,7,7,7,7,7,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Spark DataFrame","Ease of use, with methods/functions for evaluation.","Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,"May be a unique key, to identify the row uniquely, which might help in later stages for evaluation. It helps a lot, when we shuffle the data, so a unique key helps to identify the record uniquely.",A lot !!,3,"MLFlow, Weight and biases, Custom tool","Airflow, Kubeflow, Custom tool",less than a day,More than 5,Strongly agree,No,
9/8/2024 4:46:38,I have read this statement in its entirety and affirm the stated conditions.,7,8,7,5,6,8,7,8,It is very useful and reliable I always visit when I feel trouble in my project or model. ,Yaa! You should provide a document in which should mathematical intuition of Algorithm and other functions. ,6,7,6,5,6,7,7,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots",pandas,API,"Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",No,,,4,"MLFlow, Weight and biases","Airflow, Argo, Dagster, Kubeflow, Metaflow (outerbounds)",less than a minute,2,Strongly agree,Yes,arjuns921192@gmail.com
9/8/2024 10:21:53,I have read this statement in its entirety and affirm the stated conditions.,6,8,8,6,6,5,7,5,I mostly use it because it is popular,,7,7,7,4,6,4,2,"Confusion matrix, ROC curve, Feature importance, Residual plots, Learning curves, Other","pandas, Polars",Flexibility,"CatBoost, Keras, LightGBM, XGBoost, Other","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances, Metadata routing, Non-euclidean metrics",No,,,4,,,"less than an hour, less than a day",,Strongly agree,No,
9/8/2024 10:22:42,I have read this statement in its entirety and affirm the stated conditions.,6,1,8,3,1,2,1,1,Flexibility in model packaging would be highly beneficial. Better integration with ONNX or something similar to PyTorch's JIT scripting + export would make portability much more robust.,Integrated ONNX export/load utilities with documentation.,1,7,1,3,6,6,1,"Confusion matrix, ROC curve, Precision-Recall curve",pandas,The uniform API and the very detailed and comprehensive documentation and examples.,"PyTorch, Transformers",Other,No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Non-euclidean metrics",Yes,"Allow X to have non-uniform shape, and the ability to pass in partitioning data. Sort of like ragged tensors.",Improve memory efficiency and allow for custom feature pooling options.,3,Weight and biases,,less than a day,More than 5,Strongly agree,Yes,dave.f.hollander@gmail.com
9/8/2024 13:54:36,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,1,1,1,1,"I would appreciate , if you could optimize exciting models for GPUs or make it faster and efficient in general.
So, it could be applied to wider range of applications.
Also, would love to see new features for customization in each modules.","Optmization of exciting models, scikit learn are good to start with , but are really rusty and really slow on real world applications.
",7,7,7,7,7,7,1,"Confusion matrix, ROC curve, Feature importance, Learning curves","cudf, pandas, Polars",Ease of use and clean documentation.,"CatBoost, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,5,"MLFlow, Weight and biases","Kubeflow, Metaflow (outerbounds), Custom tool","less than an hour, less than a day",1,Agree,No,dhanush13589@gmail.com
9/8/2024 17:13:18,I have read this statement in its entirety and affirm the stated conditions.,6,8,4,8,7,4,1,1,,,5,6,5,4,4,3,7,"Confusion matrix, ROC curve, Precision-Recall curve","pandas, Polars",Preprocessing and model selection modules,"LightGBM, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning",No,,,3,,,less than a day,,Strongly agree,Yes,cannone.dario@gmail.com
9/8/2024 18:01:21,I have read this statement in its entirety and affirm the stated conditions.,7,8,8,6,8,7,5,5,,Covariance structures in Linear models.,7,6,6,5,5,3,2,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Polars",Consistent API,"CatBoost, Keras, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",No,,,2,,Airflow,less than 10 minutes,4,Strongly agree,Yes,p.z.hadjipantelis@gmail.com
9/8/2024 18:39:37,I have read this statement in its entirety and affirm the stated conditions.,4,1,5,2,3,6,8,7,"With all the ""AI hype"" present nowadays, I am sure that 80% of the current ML libraries won't exist 3 years from now. Scikit-learn is a safe bet where I can find what I need (other than GPU acceleration)","Believe it or not, including Bayesian Additive Regression Trees as a new estimator. It is arguably one of the most practical and relevant models for many real-life scenarios. Good for causal inference, and a solid estimator overall.",1,4,2,3,5,6,7,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","cudf, Dask DataFrame, pandas","Single, unified API for dozens of different problems. It just makes experimentation simple.","Jax, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,2,"MLFlow, Custom tool","Coiled, Dagster",less than a day,More than 5,Strongly agree,Yes,julio_antonio.soto_vicente@sandoz.net
9/8/2024 20:03:00,I have read this statement in its entirety and affirm the stated conditions.,8,8,7,4,7,8,3,,,"A pseudocode of how things are implemented under the hood will be useful. Previously Id just go look at the source code, but now it looks like most have beeen migrated to low level code for optimization which is understandable. But I occasionaly want to look at how some internals would look like in python.",7,6,7,4,3,3,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,,PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,group informations,,5,MLFlow,Airflow,less than 10 minutes,More than 5,Strongly agree,Yes,bipin.lekhak@outlook.com
9/8/2024 20:52:02,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,8,,,7,7,7,7,7,7,7,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,simple and easy to use,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",Yes,text,,4,"DVC, Weight and biases","Airflow, Dagster",less than an hour,2,Strongly agree,No,
9/8/2024 23:12:24,I have read this statement in its entirety and affirm the stated conditions.,7,8,3,5,4,6,2,1,"Performance, Reliability & Ease of using are most important in my view",It eases life if dataframe can be used directly into ML algorithms (APIs) instead of arrary conversions. Training videos of scikit learn to use with the best practices,5,7,6,3,2,4,1,"Confusion matrix, Feature importance",pandas,pandas multipurpose & intuitive use,Other,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Cost-sensitive learning, Feature importances",No,,,5,Other,Other,less than 10 minutes,1,Strongly agree,Yes,srinivasadireddy1@gmail.com
9/9/2024 2:14:37,I have read this statement in its entirety and affirm the stated conditions.,7,6,5,8,4,3,1,,,Not used very much to answer this question,7,6,1,4,3,5,,"Precision-Recall curve, Feature importance, Learning curves",pandas,Ease of Use,Keras,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Cost-sensitive learning, Sample weights",No,,,3,,,less than a minute,,Strongly agree,No,
9/9/2024 3:34:20,I have read this statement in its entirety and affirm the stated conditions.,8,4,4,7,2,2,1,1,,,6,7,7,5,6,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","Polars, Spark DataFrame",,"CatBoost, LightGBM, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,3,MLFlow,"Airflow, Argo",less than 10 minutes,4,Strongly agree,Yes,walter@sperat.com.ar
9/9/2024 4:46:07,I have read this statement in its entirety and affirm the stated conditions.,5,4,6,8,8,8,5,4,,,6,7,4,2,6,3,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Polars",,"LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, Pipeline, ColumnTransformer",No,"Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,4,MLFlow,"Airflow, Metaflow (outerbounds)","less than an hour, less than a day",4,Strongly agree,Yes,tobias.schulzeheinrichs@gmail.com
9/9/2024 7:45:47,I have read this statement in its entirety and affirm the stated conditions.,7,8,4,7,6,5,2,,,,7,7,6,5,4,7,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,,"Keras, Other","LogisticRegression, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Metadata routing, Non-euclidean metrics",No,,,3,,,less than an hour,1,Strongly agree,No,
9/9/2024 9:14:30,I have read this statement in its entirety and affirm the stated conditions.,4,4,4,7,6,7,4,4,love it,,7,7,3,3,7,7,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Spark DataFrame",ease of use and performance,"PyTorch, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Other",No,Feature importances,Yes,,,5,"MLFlow, Custom tool",,"less than 10 minutes, more than a day",1,Agree,No,
9/9/2024 20:46:45,I have read this statement in its entirety and affirm the stated conditions.,6,5,4,3,1,2,7,,,,1,4,6,5,2,3,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,,"PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing",No,,,3,,,less than 10 minutes,2,Strongly agree,No,
9/10/2024 0:31:04,I have read this statement in its entirety and affirm the stated conditions.,3,5,5,2,4,3,2,1,,,2,7,5,6,3,4,1,"Confusion matrix, ROC curve, Precision-Recall curve","Dask DataFrame, pandas",,XGBoost,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,Feature importances,No,,,1,,,less than a minute,1,Strongly agree,No,
9/10/2024 1:05:00,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,5,8,5,5,1,It will be better to add some better explanation and easy use for special use cases ,The examples ,7,7,7,7,7,7,7,"Confusion matrix, ROC curve, Precision-Recall curve","pandas, Spark DataFrame",,"CatBoost, Jax, Keras, LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",No,,,3,,,,2,Strongly agree,Yes,shivam574488@gmail.com
9/10/2024 4:03:04,I have read this statement in its entirety and affirm the stated conditions.,6,3,3,7,8,8,8,2,,,4,7,7,7,5,7,3,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","pandas, Spark DataFrame",,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, ColumnTransformer",No,,,,,5,Weight and biases,,less than an hour,More than 5,Strongly agree,,
9/10/2024 4:55:29,I have read this statement in its entirety and affirm the stated conditions.,5,5,8,4,7,5,2,1,,,5,7,4,3,2,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","DuckDB, pandas, Polars",,"Jax, PyTorch, Transformers, XGBoost","LogisticRegression, Pipeline",Yes,"Calibration of probabilistic classifiers, Sample weights",No,,,3,,,less than a day,More than 5,Agree,No,
9/10/2024 5:27:44,I have read this statement in its entirety and affirm the stated conditions.,5,3,3,4,2,1,2,1,,,2,7,1,3,7,6,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,,"CatBoost, Keras, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,4,Other,Other,"less than an hour, less than a day",2,Neither agree nor disagree,Yes,giuseppe.minardi95@gmail.com
9/10/2024 7:15:49,I have read this statement in its entirety and affirm the stated conditions.,8,8,6,6,7,8,2,1,,,5,7,6,4,6,6,1,"Confusion matrix, Precision-Recall curve","pandas, Polars",,"PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Feature importances",No,,,5,MLFlow,Kubeflow,"less than 10 minutes, less than an hour",,Strongly agree,No,
9/10/2024 9:38:23,I have read this statement in its entirety and affirm the stated conditions.,6,7,4,6,5,6,3,4,,,6,7,3,2,4,5,1,"Confusion matrix, ROC curve, Feature importance, Residual plots, Learning curves","Dask DataFrame, pandas, Polars, Spark DataFrame",Uniform api allowing for easy composition with meta estimators,"CatBoost, LightGBM, PyTorch, XGBoost, Other","Pipeline, Other",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Sample weights, Metadata routing",Yes,,,4,"Custom tool, Other",Kubeflow,more than a day,More than 5,Strongly agree,,
9/10/2024 9:48:04,I have read this statement in its entirety and affirm the stated conditions.,6,8,5,7,8,3,1,1,,Add generalized additive models with interaction ,5,7,2,2,3,4,1,"Confusion matrix, ROC curve, Feature importance",pandas,,"CatBoost, Keras, LightGBM, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",No,"Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,Uncertainty ,provides a better description on the trustworthiness of a datapoint,3,,,less than a day,,Strongly agree,Yes,roberto.francischello@med.unipi.it
9/10/2024 10:30:56,I have read this statement in its entirety and affirm the stated conditions.,8,5,4,5,7,3,2,1,,More performance/optimisation,7,7,6,3,5,4,1,"Confusion matrix, ROC curve, Residual plots",pandas,,"Jax, Keras, PyTorch, Transformers","LogisticRegression, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Non-euclidean metrics",No,,,5,Custom tool,Custom tool,less than a day,1,Strongly agree,No,
9/10/2024 11:18:15,I have read this statement in its entirety and affirm the stated conditions.,1,1,1,7,8,6,1,1,The documentation and educational materials often assume a relatively high level of background. I'd use this more for teaching undergrads if the documentation was more clear. Recommend testing on undergrads who are taking their first software course.,"Honestly? More choice in distance metrics, especially something akin to angular distance. This is probably an unpopular answer though.",5,7,5,7,5,7,,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves",pandas,"Well tested, has what I need most of the time.",PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Other",No,"Cost-sensitive learning, Feature importances, Non-euclidean metrics",No,,,3,,,less than 10 minutes,2,Strongly agree,No,
9/10/2024 11:49:37,I have read this statement in its entirety and affirm the stated conditions.,7,6,1,8,1,1,1,,,,6,4,2,6,7,4,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,,"CatBoost, LightGBM, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,5,,,less than 10 minutes,2,Strongly agree,No,
9/10/2024 11:55:20,I have read this statement in its entirety and affirm the stated conditions.,6,6,7,7,7,5,3,4,,,4,7,6,4,4,5,4,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","DuckDB, Modin, pandas, Polars",,"PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning",,,,2,"MLFlow, Weight and biases",Airflow,less than an hour,More than 5,Strongly agree,,
9/10/2024 15:59:38,I have read this statement in its entirety and affirm the stated conditions.,3,2,1,8,6,6,1,1,"New features are most important, and second to that is making improvements to the (already excellent) documentation by providing even more usage advice and examples.",,6,7,4,5,2,3,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,"Sensible model defaults, excellent documentation, thoughtful API design, great community",,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,,,,less than 10 minutes,,Strongly agree,Yes,kevin@dataschool.io
9/10/2024 16:30:15,I have read this statement in its entirety and affirm the stated conditions.,4,3,2,7,2,2,2,1,,,7,4,7,7,7,4,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,easy-to-understand api calls and documentation,"Jax, Keras, PyTorch, Transformers",Pipeline,Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",No,,,5,"MLFlow, Weight and biases","Argo, Kubeflow",less than an hour,3,Strongly agree,No,
9/11/2024 2:13:05,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,5,3,1,1,"Include new methods like conformal prediction, GenAI, timeseries, DeepLearning algorithms,  compatibility with polars, improve graphical diagnostics, explainability, MLOPS, FMOPS, LLMOPs","GenAI features, like preprocessing, embeddings, training, inference and evaluation",5,5,7,7,5,5,7,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Polars, Spark DataFrame","Robustness, API consistency, documentation","CatBoost, LightGBM, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning",No,,,5,"MLFlow, Weight and biases, Other",Other,"less than 10 seconds, less than a minute, less than 10 minutes, less than an hour, less than a day",More than 5,Strongly agree,Yes,dancruiz@amazon.com
9/11/2024 2:37:19,I have read this statement in its entirety and affirm the stated conditions.,5,7,7,8,8,7,4,,"1. New Features (8/10)
Adding new features is critical to ensuring scikit-learn remains relevant in a fast-evolving machine learning ecosystem. Innovations in algorithms, support for new model architectures, and integration with other data science tools would significantly enhance its capabilities. The addition of cutting-edge techniques can give users more flexibility and keep the library competitive with other frameworks like PyTorch or TensorFlow.

2. Technical Documentation (8/10)
Comprehensive and clear documentation is essential, especially when the library is being widely used by both beginners and experts. Scikit-learn already has good documentation, but it can always be improved, particularly in offering more in-depth explanations, examples for advanced use cases, and better clarification of complex topics like hyperparameter tuning and model evaluation.

3. Reliability (7/10)
Ensuring that scikit-learn remains a reliable tool in production environments is vital. The library is often used in real-world applications where robustness is key. More rigorous testing, enhanced backward compatibility, and better error handling would make it easier for users to trust it in mission-critical systems.

4. Educational Materials (7/10)
Scikit-learn is a go-to library for those learning machine learning, and providing more educational resources like tutorials, video walkthroughs, and interactive notebooks would help foster a deeper understanding of ML concepts. With machine learning becoming mainstream, creating more beginner-friendly materials is important to engage a wider audience.

5. Packaging (7/10)
Simplifying the installation process and ensuring compatibility across various platforms is another priority. Many users, especially beginners, face difficulties with dependencies, version conflicts, and setup issues. Streamlined packaging and deployment would make scikit-learn more accessible.

6. Performance (5/10)
While performance is important, scikit-learn focuses more on ease of use and interpretability rather than being a high-performance library like PyTorch. It performs well for moderate-sized datasets, and though it could benefit from optimization, its primary audience (students, researchers) isn’t necessarily looking for cutting-edge speed. Enhancing performance for large datasets can be addressed, but it's not the highest priority.

7. Website Redesign (4/10)
The scikit-learn website is functional and serves its purpose well. Although some visual and structural improvements could enhance usability, it’s a lower priority compared to the other aspects. The main focus should remain on improving the library’s functionality and educational resources.",Technical Documentation and Educational Materials ,6,6,5,5,4,5,5,"Confusion matrix, ROC curve, Precision-Recall curve",pandas,Its simplicity and ease of use. ,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Non-euclidean metrics",No,,,3,Weight and biases,Kubeflow,less than 10 minutes,2,Strongly agree,No,
9/11/2024 6:01:04,I have read this statement in its entirety and affirm the stated conditions.,7,7,6,8,8,6,8,,,,6,7,2,6,4,6,,"Confusion matrix, ROC curve, Precision-Recall curve","Dask DataFrame, pandas",,"CatBoost, Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Non-euclidean metrics",Yes,,,2,,,"less than an hour, less than a day",More than 5,Strongly agree,No,
9/11/2024 7:33:20,I have read this statement in its entirety and affirm the stated conditions.,7,7,7,2,5,5,2,1,"It can be a little bit slow, when using kfold validation in nested loops, so finding a way to speed that up would be nice 

It would be nice to be more easily able to see what goes on inside the algorithm. As someone who likes to look inside the black box it would be nice to see a simple example worked by hand (written out in full) so that one can fully see how the computation works. ",Speed. I am using scikit to evaluate model performance for a variety of algorithms and it is taking a long time for me to run all of them enough times to gather sufficient data. ,3,7,2,6,5,4,1,Confusion matrix,pandas,"It is simple, easy to learn, fairly comprehensive (covers a large variety of topics), gives fairly good model performance. I really like it. ",PyTorch,LogisticRegression,No,"Calibration of probabilistic classifiers, Cost-sensitive learning, Feature importances",No,,,5,Other,Other,less than a minute,2,Strongly agree,No,
9/11/2024 10:27:42,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,5,4,4,4,1,"Performance and reliability and packaging are some of the most important characteristics when applying a model in production. Models have had to be recalled due to low speed, and much time has been lost trying to decipher models saved in pickle files.","Config based creation/loading of models. Makes saved models human readable, less platform/library version dependable (though that depends on the implementation) if relevant info can be saved in a json or yaml. Also makes training and automl easier and more transparent if models can be trained using configs.",1,7,1,5,5,5,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Spark DataFrame",Simple UI,"Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Metadata routing, Non-euclidean metrics",Yes,metadata about features,"Not training, but code reusability",5,MLFlow,Airflow,more than a day,More than 5,Strongly agree,Yes,marcogarranzoasensio@gmail.com
9/11/2024 10:41:01,I have read this statement in its entirety and affirm the stated conditions.,3,7,8,4,6,5,2,1,"Stick to what you're good at, and make that more robust. For bits that have to be especially performant or when I need a specialized model, I use other libraries and wrap those in a sklearn pipeline.","Fully integrated support for pandas dataframes with column names as output, robust saving of models.

A bit of a hot take: removal of the .predict method so I don't have to tell people to use .predict_proba instead.",4,7,2,3,5,6,1,"Reliability diagram, ROC curve, Residual plots, Learning curves",pandas,The documentation,"CatBoost, PyTorch, Transformers, XGBoost",RandomForestClassifier or RandomForestRegressor,Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Sample weights, Metadata routing, Non-euclidean metrics",Yes,"(hierarchical) groups, feature/target names.
possibly uncertainty/noise for perturbation of certain features/samples","Groups: Quite often some samples are related to eachother in one or several ways (sometimes even hierarchical). For example, you might be dealing with multiple visits by the same customer to different stores. If you're not very careful, this typically leads to information leakages. Having multiple levels of groups allows you to deal with multiple grouping levels in multiple levels of cross validation.

Feature/target names: I almost always use the clf.predict_proba method, and it would be awesome if that would return a dataframe with the classes as column names.

Uncertainty/noise: Some measurements for some samples might be noisy or uncertain. Right now, in order to fully utilize varying levels of certainty, I just copy the train/test data a bunch of times with some perturbations. It would be cool to have some support for that (though I expect it not to be feasible)",2,"DVC, Custom tool",Other,"less than 10 minutes, more than a day",More than 5,Strongly agree,Yes,sklearn@swier.eu
9/11/2024 10:58:57,I have read this statement in its entirety and affirm the stated conditions.,,,,,,,,,,,3,7,1,7,7,7,6,"Confusion matrix, Feature importance, Learning curves",pandas,,"CatBoost, Keras, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Non-euclidean metrics",,,,5,Other,,less than 10 minutes,1,Strongly agree,No,
9/11/2024 18:18:13,I have read this statement in its entirety and affirm the stated conditions.,,,,,,,,,I do not have specific feedback on this,There were a couple of features I had to custom implement for Pipeline but I cannot remember them off of the top of my head. So maybe adding more features to Pipeline.,4,4,1,7,7,7,1,"Confusion matrix, Precision-Recall curve, Feature importance, Residual plots",pandas,The documentation is amazing.,,"RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances, Sample weights",No,,,1,,Other,less than 10 minutes,More than 5,Strongly agree,No,
9/12/2024 2:46:04,I have read this statement in its entirety and affirm the stated conditions.,8,3,3,4,3,4,2,,"Most things about scikit-learn are absolutely great as they are. However, ti would be great if scikit-learn would have an option for using GPUs. ","There is a lot of great value already which I am perfectly happy with. Implementing use of GPU would be great. Another very welcome set of tools would be availability of algorithms for survival analysis. I know of the hazardous package, but I use mainly scikit-survival (very nice package). If scikit-survival could get implemented in scikit-learn or get the amount/type of support of scikit-learn has, would be phenomenal. ",7,7,1,4,7,4,,"Confusion matrix, ROC curve, Feature importance, Residual plots","pandas, Polars",Ease of use. Documentation. Number of algorithms available. (Cross) validation framework. ,"Keras, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,4,Weight and biases,,less than 10 minutes,1,Strongly agree,Yes,oliver.tomic@nmbu.no
9/12/2024 4:36:42,I have read this statement in its entirety and affirm the stated conditions.,3,3,2,8,2,5,2,2,,,6,7,3,2,5,4,1,"Confusion matrix, Feature importance",pandas,Ease of use,"PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances, Sample weights",No,,,4,Other,Other,less than an hour,2,Strongly agree,No,
9/12/2024 5:01:08,I have read this statement in its entirety and affirm the stated conditions.,7,4,8,4,6,5,4,,"Deploying sklearn models in non-python environments would be a game changer. Onnx is already a step forward, but not everything is supported, and converting pipelines is always challenging and error prone. Also, it is almost impossible to convert a custom estimator or transformer, although I understand that this is a limitation of python itself rather than sklearn.",Performance improvements are always welcome,6,6,7,6,7,5,,"Confusion matrix, Residual plots",Polars,Ease of use,"CatBoost, LightGBM, PyTorch, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,2,MLFlow,Airflow,less than a minute,More than 5,Strongly agree,No,
9/12/2024 10:21:04,I have read this statement in its entirety and affirm the stated conditions.,5,3,1,8,8,5,1,1,"The documentation almost always provides enough information to use a tool of interest. However, it can be difficult to understand what is happening.  I would suggest providing more technical information and use cases to better explain the algorithms, parameters, and assumptions behind each method.",Better tools for working with time-series data. Especially bayesian approaches.,6,5,7,5,4,4,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Spark DataFrame",,"CatBoost, Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning",Yes,Weights or masks for working with missing values.,"Many methods can be used with missing data, but Sklearn rarely provides support for that.",3,MLFlow,,less than a minute,More than 5,Strongly agree,No,
9/12/2024 12:11:55,I have read this statement in its entirety and affirm the stated conditions.,1,,,,,,,,Its really slow for large datasets,GPU Support,4,3,2,1,,5,6,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots",cudf,Ease of usage,"CatBoost, Keras, LightGBM, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, ColumnTransformer",No,"Uncertainty estimates for prediction, Sample weights",Yes,Gpu engine,Speed it up,1,"DVC, MLFlow, Weight and biases","Airflow, Coiled",less than 10 minutes,5,Agree,Yes,Manas.singh@berkeley.edu
9/12/2024 12:19:41,I have read this statement in its entirety and affirm the stated conditions.,4,8,5,3,7,6,,,I like scikit-learn as it is; none of my priorities indicate something wrong with scikit-learn.,,,,,,,,,"Confusion matrix, Learning curves",pandas,API,PyTorch,Pipeline,Yes,,,,,1,,,,,Strongly agree,,
9/12/2024 17:32:56,I have read this statement in its entirety and affirm the stated conditions.,3,3,6,7,7,7,1,1,"New features including Deep Learning, & Transforms. Increase example code and the layman and mathematical explanations. Everything else is good!",Convolutional layers.,1,7,6,3,3,3,1,"Confusion matrix, Precision-Recall curve, Feature importance",pandas,The algorithms follow the explanations in 'Elements of Statistical Learning'.,"Keras, PyTorch, XGBoost, Other","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Calibration of probabilistic classifiers, Feature importances",No,,,5,Custom tool,Custom tool,less than a day,4,Strongly agree,Yes,randall@numbersandtech.com
9/12/2024 22:42:28,I have read this statement in its entirety and affirm the stated conditions.,8,5,3,7,3,3,1,1,,,7,7,4,7,7,7,3,"Confusion matrix, ROC curve, Residual plots, Learning curves","DuckDB, pandas, Spark DataFrame",,"CatBoost, Keras, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Non-euclidean metrics",No,,,5,"MLFlow, Weight and biases",,less than an hour,3,Strongly agree,No,
9/13/2024 4:04:40,I have read this statement in its entirety and affirm the stated conditions.,8,8,7,5,8,8,4,,,,6,6,,7,7,6,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","cudf, pandas",,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Metadata routing",Yes,,,5,MLFlow,Airflow,more than a day,More than 5,Strongly agree,Yes,parth@vijai.info
9/13/2024 4:04:57,I have read this statement in its entirety and affirm the stated conditions.,4,8,2,6,5,5,1,1,"Ensuring existing functionality is *complete* and easy to teach would be my highest priority, i.e., that most important scientific problems can be solved with them. In this sense e.g. KernelRidge is not complete, since it can't be used to reproduce smoothing splines. Some important convenience functions, such as identity transforms (instead of mouthful FunctionTransformer(lambda x: x)) are missing, as is convenience options for the .score method (e.g., use MSE instead of R^2). ",Penalization constant of 0 for some columns in ridge regression,7,7,1,1,3,1,1,Residual plots,pandas,,"CatBoost, Keras","LogisticRegression, Pipeline, ColumnTransformer",Yes,,Yes,More flexible penalization in ridge regression and kernel ridge.,"Would allow to reproduce, e.g., smoothing splines using (kernel) ridge.",3,,,less than a minute,,Strongly agree,,
9/13/2024 4:57:48,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,3,,"Scikit-learn is doing great, but I think there's also need for further improvement especially in educational materials to help beginners and students better understand how machine learning algorithms work using the library. ",For now scikit-learn is satisfactory. Though won't turn away from embracing new positive changes if introduced. ,5,7,1,4,6,6,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,The enormous library it has which contains almost everything I need for modeling.,"Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Feature importances, Sample weights, Non-euclidean metrics",No,,,4,MLFlow,Airflow,less than a minute,4,Strongly agree,No,
9/13/2024 5:07:54,I have read this statement in its entirety and affirm the stated conditions.,7,8,7,7,5,4,3,,,Full pandas support throughout the library,7,5,7,5,4,4,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Polars, Spark DataFrame",Well thought interface that is consistently applied throughout the project,"LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Metadata routing",No,,,1,MLFlow,Other,less than an hour,More than 5,Strongly agree,No,
9/13/2024 11:41:43,I have read this statement in its entirety and affirm the stated conditions.,6,8,8,8,5,7,1,1,,An option to compute confidence intervals for all scorers,6,7,5,3,7,1,,"ROC curve, Precision-Recall curve, Feature importance",pandas,,"CatBoost, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",,,,,,,"less than a minute, less than 10 minutes, less than an hour, less than a day",2,Strongly agree,,
9/13/2024 15:49:01,I have read this statement in its entirety and affirm the stated conditions.,5,8,8,5,8,8,2,1,scikit-learn provides a strong educational foundation to data science. The interoperability of scikit-learn and clearly established programming interface are what I view as the strengths of the library. ,,7,6,4,5,2,3,1,Confusion matrix,"DuckDB, pandas","The clearly documented programming interface and interoperability with other tools. The ability to roll-your-own estimator, while still being able to use the scikit-learn MLflow Flavor is a huge asset. I really like how the html representation of a pipeline is automatically logged with mlflow.sklearn.autolog. ","CatBoost, LightGBM, PyTorch, XGBoost","LogisticRegression, Pipeline, ColumnTransformer",Yes,"Feature importances, Sample weights",No,,,3,MLFlow,"Airflow, Other",less than a day,More than 5,Strongly agree,Yes,programming@enlawson.com
9/13/2024 16:00:37,I have read this statement in its entirety and affirm the stated conditions.,6,3,5,8,4,6,2,,,,3,5,2,4,6,7,1,Feature importance,"pandas, Spark DataFrame",,Other,Other,No,"Feature importances, Sample weights, Non-euclidean metrics",,,,3,,,less than an hour,,Strongly agree,,
9/13/2024 20:16:07,I have read this statement in its entirety and affirm the stated conditions.,2,1,3,6,5,4,7,8,,,5,1,4,3,7,6,2,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Polars","Mostly-well-thought-out API, great User Guide","LightGBM, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Cost-sensitive learning, Feature importances",Yes,"weights, groups (CV), input to cost function, base predictions (for boosting from), etc.",,4,,"Airflow, Other",less than 10 minutes,More than 5,Agree,No,
9/14/2024 5:38:06,I have read this statement in its entirety and affirm the stated conditions.,7,8,7,6,6,6,6,5,,,6,6,6,6,6,6,5,Feature importance,pandas,,"Keras, PyTorch",Pipeline,,Calibration of probabilistic classifiers,,,,4,"DVC, MLFlow","Airflow, Kubeflow",more than a day,,Agree,,
9/14/2024 18:29:58,I have read this statement in its entirety and affirm the stated conditions.,6,6,1,4,8,8,7,4,"the way scikit-learn is taught on the website is EXTREMELY confusing, it should be made more simple to understand",being able to learn it and reference the documentation without getting confused and feeling overwhelmed,7,7,5,1,1,7,1,Confusion matrix,"pandas, Spark DataFrame",the simple api,"PyTorch, Transformers, Other","LogisticRegression, Other",No,,No,,,1,MLFlow,"Airflow, Custom tool","less than 10 seconds, less than a minute, less than 10 minutes",,Strongly agree,,
9/15/2024 9:05:56,I have read this statement in its entirety and affirm the stated conditions.,6,8,4,4,7,1,3,2,Good documentation and stable calculations,More examples to practically use functions,7,3,7,2,3,5,1,"Feature importance, Residual plots, Other","Dask DataFrame, DuckDB, pandas, Polars",The amount of fuctions,"Keras, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,/,/,3,Other,Other,less than a minute,2,Agree,No,/
9/15/2024 11:30:08,I have read this statement in its entirety and affirm the stated conditions.,3,3,5,8,8,5,2,,"I am writing a research paper and it would be useful to me and other researchers to have more detailed documentation on the background workings of scikit learn functions, especially clustering algorithms and how they are constructed.",More detailed documentation,2,6,5,4,3,7,,Residual plots,Other,easy to use functions that do a lot of different clustering algorithms,,,,"Uncertainty estimates for prediction, Cost-sensitive learning, Non-euclidean metrics",,,,4,,,,,,,
9/15/2024 23:28:12,I have read this statement in its entirety and affirm the stated conditions.,6,2,2,6,2,2,1,4,Improve deployability of scikit-learn models,"Make a .pandas. submodule of transformers that provide first class support for pandas dataframes. I write all my own transformers and always include a `columns_to_use` attribute in the __init__ method. Then, all my transformers can can be a single sequential series of transformers. No need to use wrappers or special .set_output calls. The code is easy to read, easy to reason about, and easy to maintain.",7,5,3,2,1,1,1,"Feature importance, Learning curves",pandas,The API! Best ML API ever constructed. ,"CatBoost, Keras, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Metadata routing",Yes,"Here was when I ran into this issue over 10 years ago (wow, I got old): https://stackoverflow.com/questions/23174964/how-to-gridsearch-over-transform-arguments-within-a-pipeline-in-scikit-learn",Optimize everything!,2,"MLFlow, Custom tool","Airflow, Custom tool",less than an hour,More than 5,Strongly agree,Yes,2jasonsanchez@gmail.com
9/16/2024 1:25:51,I have read this statement in its entirety and affirm the stated conditions.,6,7,1,8,8,8,1,1,"The technical documentation and tutorials are amazing and should be expanded. I think new features are important, like more clustering algorithms, and being able for loss matrix for the classification process like in MATLAB, Direct Learning Cost Sensitive Classifier.","Support Cost Sensitive Classifier (For Multiclass as well, direct at the loss function level).",5,7,6,5,7,4,2,"Confusion matrix, Precision-Recall curve",Polars,"Tutorials, documentation, consistent API","Jax, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Calibration of probabilistic classifiers, Cost-sensitive learning, Feature importances",No,,,2,"Weight and biases, Other",Other,less than 10 minutes,5,Strongly agree,Yes,RoyiAvital@yahoo.com
9/16/2024 2:07:54,I have read this statement in its entirety and affirm the stated conditions.,5,2,3,8,6,6,2,2,,Utilize GPU ,2,6,2,2,6,5,1,"ROC curve, Precision-Recall curve, Feature importance",Other,,PyTorch,LogisticRegression,No,Feature importances,No,,,5,Other,Other,more than a day,1,Strongly agree,No,
9/16/2024 2:17:15,I have read this statement in its entirety and affirm the stated conditions.,6,6,5,7,6,8,3,1,"One of the challenge I have with scikit learn is that the educational materials are minimal. It gives only the basic options. When trying to use complex solutions, there isnt much. I have to always rely on stackoverflow for more details. 

I would like for newer models like adaboost and other ensamble models to have stonger educational materials and stronger package. 

I would also like to see if there is a way for the package to improve or create new features as i need to sometimes use other processing steps to reduce things. ","Ability to search for a model and find its application. There is a lot of documentation but when I have to get to an implementation, it is a nightmare. I have to comb through a lot of materials or use stackoverflow to get to the answer. ",7,5,6,6,7,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,"The examples that i can port from the library to learn more about the model

I like the classification models. When it comes to regressors, i am still struggling. it may be because i am still a learner. forecasting is not there yet. i would like to learn more about feature importance and how pca can help me with feature importance.","Keras, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",Yes,"what if i could pass the % of probability that it can use to determine the output. or what if i can say i want a model to be tuned only to 80% accuracy and not above that. i dont want the model to overfit. is there a way to reduce the way the model learns so it doesnt learn too much?

I would also like to have a parameter that tells the model to use GPU vs. CPU if possible. Is there a way for the model to parallel process without using Spark DataFrame. Maybe I am still learning and this is an area I havent yet explored. Making it available as an option withink the model for some of the complex ones would be benefitial as these complex models tend to take a lot of time to process (ex: SVC vs. LogisticRegression).",it would limit the model from learning too much and use other thresholds to determine if the learning is correct. ,5,,,less than a day,1,Strongly agree,Yes,ferndzjoe@gmail.com
9/16/2024 6:10:16,I have read this statement in its entirety and affirm the stated conditions.,8,8,5,7,6,5,3,3,,Performance,5,7,5,7,7,7,5,"Confusion matrix, ROC curve, Precision-Recall curve","pandas, Spark DataFrame",,"CatBoost, Keras, LightGBM, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Calibration of probabilistic classifiers, Calibration of regressors, Cost-sensitive learning, Sample weights",Yes,Loss function ,Versatile ,4,"Weight and biases, Custom tool",Airflow,less than an hour,2,Strongly agree,No,
9/16/2024 8:36:30,I have read this statement in its entirety and affirm the stated conditions.,,,,8,,8,,,,,6,6,,6,5,2,,"Confusion matrix, Feature importance, Learning curves",pandas,,Keras,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",,,,4,,,less than a minute,,Agree,No,
9/16/2024 13:28:26,I have read this statement in its entirety and affirm the stated conditions.,7,8,8,7,8,,,,,,6,7,5,6,7,7,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Polars, Spark DataFrame",,"CatBoost, Jax, LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",No,,,1,MLFlow,"Airflow, Kubeflow","less than 10 minutes, less than an hour, less than a day",More than 5,Strongly agree,No,
9/16/2024 15:12:46,I have read this statement in its entirety and affirm the stated conditions.,4,4,4,4,8,8,6,8,"More documentation about custom transformers: - coding conventions, - methods implementation like get_feature_names_out specifics of writing custom object when 'set_config(transform_output='pandas')'",,1,5,5,3,6,5,,"Confusion matrix, ROC curve",pandas,pipelines are de facto also transformers and estimators,"Keras, XGBoost","Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",Yes,return the whole X where y_hat is just another column,explainability,4,,Kubeflow,more than a day,More than 5,Strongly agree,Yes,jaroslav.jaros@gmail.com
9/17/2024 1:17:27,I have read this statement in its entirety and affirm the stated conditions.,8,8,7,7,8,8,8,8,,,7,7,7,7,7,7,7,"Confusion matrix, ROC curve",pandas,,"PyTorch, Transformers, XGBoost","LogisticRegression, Pipeline",,"Calibration of regressors, Feature importances",,,,2,,,,,Strongly agree,Yes,btatmaja@gmail.com
9/17/2024 5:17:06,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,,can you please provide a platform for free or very minimum charges for model deployment,explanation in simple english with some pictures or examples,7,7,7,7,7,7,7,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots, Learning curves",pandas,Documentation and frame work,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Feature importances, Sample weights",No,,,1,"DVC, MLFlow, Weight and biases",,less than 10 seconds,1,Agree,Yes,gourabsingh09@gmail.com 
9/17/2024 6:55:08,I have read this statement in its entirety and affirm the stated conditions.,8,6,1,6,8,8,1,1,,Expand on the educational aspect of the website,6,7,4,4,6,4,1,Confusion matrix,pandas,Extensive documentation,Keras,"Pipeline, ColumnTransformer",No,,No,,,5,,,"less than an hour, less than a day",,Strongly agree,No,
9/17/2024 7:04:42,I have read this statement in its entirety and affirm the stated conditions.,8,8,6,8,8,8,8,1,Technical documnetation should be user friendly with good examples to help understand the purpose of the function in detail for beginners,"Add Educational materials based on categories like data evaluation,model hyperparameter tuning and add complete code documnetation implementing datasets from sites like kaggle",7,7,4,2,4,4,1,"Confusion matrix, Feature importance, Learning curves",pandas,Easy to use,"Keras, Transformers","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Feature importances",No,,,,Other,Other,less than an hour,1,Strongly agree,Yes,santhoshroyal8177@gmail.com
9/17/2024 7:06:27,I have read this statement in its entirety and affirm the stated conditions.,8,6,2,5,4,2,1,1,,,7,5,2,2,3,2,1,"Confusion matrix, Feature importance, Other","Dask DataFrame, pandas, Polars",,"LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",,,,1,Custom tool,Other,less than an hour,More than 5,Strongly agree,No,
9/17/2024 8:29:55,I have read this statement in its entirety and affirm the stated conditions.,7,4,2,5,6,6,2,,,Add XGBoost Classifier,2,6,2,2,2,2,,"Confusion matrix, ROC curve",pandas,Out of box Ease of use,"PyTorch, XGBoost",RandomForestClassifier or RandomForestRegressor,No,"Calibration of probabilistic classifiers, Cost-sensitive learning",No,,,5,,,less than 10 minutes,1,Strongly agree,No,
9/17/2024 10:02:14,I have read this statement in its entirety and affirm the stated conditions.,6,7,5,6,4,3,1,,,,7,7,7,5,5,5,3,Other,"pandas, Polars",,"CatBoost, Jax, Keras, LightGBM, Other","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Sample weights",No,,,3,MLFlow,"Custom tool, Other",less than 10 minutes,3,Agree,No,
9/17/2024 11:25:25,I have read this statement in its entirety and affirm the stated conditions.,4,6,1,8,8,1,1,1,,,3,6,2,7,5,5,4,"Confusion matrix, ROC curve",pandas,The documentation,"PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,1,"MLFlow, Other",,less than a minute,1,Strongly agree,Yes,jmifsud@ricres.org
9/17/2024 14:15:01,I have read this statement in its entirety and affirm the stated conditions.,6,5,7,8,3,4,2,1,,,6,7,4,5,3,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves, Other",pandas,,"Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,,,,,5,,,"less than 10 minutes, less than an hour",,Strongly agree,Yes,dubeyadarshmain@gmail.com
9/18/2024 1:40:44,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,8,,Require More Open Source LLM Models,7,7,7,7,7,7,7,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots","pandas, Polars",Performance,"PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,,,,,3,,,less than a minute,3,Strongly agree,,
9/18/2024 1:48:21,I have read this statement in its entirety and affirm the stated conditions.,3,4,7,5,6,8,2,,"I think the website is good. I like the user guide. For me as a student I like the examples and other educational stuff in the website. For me it is important that basic machine learning models are included in the library, like naive bayes and linear models. So for me new features are not so important.",,7,6,2,4,5,3,,"Confusion matrix, Precision-Recall curve, Residual plots, Learning curves",pandas,I like principal component analysis and clustering.,,LogisticRegression,No,,No,,,1,,,less than 10 seconds,,Agree,No,
9/18/2024 5:01:05,I have read this statement in its entirety and affirm the stated conditions.,7,6,5,7,7,7,2,,,,5,6,5,5,5,5,,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves","cudf, pandas",Standardized API accros models,"PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,4,MLFlow,"Airflow, Kubeflow",less than an hour,More than 5,Strongly agree,No,
9/18/2024 5:43:50,I have read this statement in its entirety and affirm the stated conditions.,7,3,6,8,4,8,1,1,"Performance: faster tools are always nice to have
Reliability: I never had big problems with it
New features: explainability would be a big step forward
Packaging: deployment of models could be expanded upon
Technical docs: I think they are in a good state
Educational materials: a centralized learning center for machine learning could be a big improvement. A lot can already be found but making it more organic and educational would be a great addition.
",Explainability built in,5,7,3,6,4,2,1,"Confusion matrix, Precision-Recall curve, Feature importance","pandas, Polars",The low idea to working example time,"PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,4,MLFlow,Other,less than an hour,3,Strongly agree,Yes,castellino96@gmail.com
9/18/2024 13:47:27,I have read this statement in its entirety and affirm the stated conditions.,7,8,6,4,8,5,5,4,"All priorities listed above are important. Some of them, such as reliability, are more important.",documentations and doc-strings,7,7,7,6,6,6,,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots","pandas, Spark DataFrame",Its cause. The devs are cool too.,"Keras, PyTorch, Transformers, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",,,,4,,,more than a day,1,Strongly agree,Yes,virchan.math+scikitlearn@gmail.com
9/18/2024 15:42:30,I have read this statement in its entirety and affirm the stated conditions.,8,8,7,6,5,4,3,2,,,7,4,5,2,5,5,3,"ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Polars","The standardisation, modularity, and robustness. Other ML libraries (mainly julia)  require so much more boilerplate ","Jax, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,2,,,less than a minute,1,Strongly agree,Yes,
9/18/2024 16:05:21,I have read this statement in its entirety and affirm the stated conditions.,5,4,3,8,6,7,2,1,"If possible, I'd use sklearn for every ML problem I have — therefore, in my opinion, the more new features, the better!",,5,6,7,4,3,2,1,"ROC curve, Precision-Recall curve, Feature importance",pandas,"Simplicity, ease-of-use, well-designed default params.",LightGBM,HistGradientBoostingRegressor or HistGradientBoostingClassifier,No,"Uncertainty estimates for prediction, Feature importances, Sample weights",,,,2,,,less than a minute,4,Strongly agree,Yes,maximlevet@pocketgems.com
9/18/2024 16:59:37,I have read this statement in its entirety and affirm the stated conditions.,8,2,1,8,2,2,1,1,Implement more unsupervised metrics for clustering like a gap statistic,unsupervised metrics for clustering other than silhouette score,1,7,7,7,7,7,1,"ROC curve, Learning curves",pandas,ease of use,"PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Uncertainty estimates for prediction, Non-euclidean metrics",No,,,5,Weight and biases,,"less than 10 minutes, less than an hour, less than a day",More than 5,Strongly agree,No,
9/18/2024 17:26:05,I have read this statement in its entirety and affirm the stated conditions.,,,,,,,,,,,7,7,5,6,6,6,,"Confusion matrix, ROC curve, Precision-Recall curve",pandas,It's so easy to use and the documents are generally good,"LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances",,,,2,,,less than an hour,,,,
9/19/2024 4:44:03,I have read this statement in its entirety and affirm the stated conditions.,,8,1,8,7,8,1,,"In my opinion the most important feature for scikit-learn would be improved support for DataFrames (not only Pandas and Polars, but every DataFrame compliant with the __dataframe__ API).

Another important feature would be the possibility to use HDBSCAN to perform (approximate) prediction on samples after fitting, similar to what the original HDBSCAN package did: https://hdbscan.readthedocs.io/en/latest/faq.html#q-i-want-to-predict-the-cluster-of-a-new-unseen-point-how-do-i-do-this

In general, the technical documentation and user guide are great, but I think they can always be improved. It's easy to understand how to use advanced features of scikit-learn, such as metadata routing. It would be nice if more examples and use cases were provided. Also, more examples on how to define custom Transformers or Classifiers and Regressors, or customize existing ones.

Another area of improvement for the documentation would be the use of complex splitters for cross-validation. For instance: how to use LeaveOneGroupOut as the cv argument for the cross_validate function or for GridSearchCV? It's possible, but it's not easy to find out how! And the documentation and examples for some splitters is a bit obscure. I am still not sure what the PredefinedSplit does! In my opinion, more focus could be given to splitters in the documentation. In my experience, a lot of people are unaware of them. I have seen people write custom horrible code because they were unaware that a nice splitter class existed!","I think adding the possibility to perform approximate predictions of HDBSCAN clustering after fitting is a low hanging fruit, it should be relatively easy to implement, and I would love it since I love HDBSCAN.",5,7,5,5,6,6,,"ROC curve, Precision-Recall curve, Feature importance, Residual plots","Dask DataFrame, pandas, Polars","The API is very elegant and powerful. A lot of different methodologies are available with a consistent interface. Not only for the models, but for other important aspects of ML: preprocessing, feature selection and engineering, cross-validation... Also I think the possibility to define complex pipelines in a secure (from overfitting) way using Pipeline is awesome!","CatBoost, PyTorch, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,2,MLFlow,Airflow,less than an hour,More than 5,Strongly agree,Yes,enricogandini93@gmail.com
9/19/2024 5:26:58,I have read this statement in its entirety and affirm the stated conditions.,5,7,,8,6,4,1,,"New features will drive new users and allow scikit-learn to remain up-to-date with the newest techniques or allow for more advanced usage. Reliability, performance and technical documentation will also aid this. ",,5,6,3,6,5,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","DuckDB, pandas, Polars, Spark DataFrame",The consistent API allowing for a rich ecosystem inside but also outside scikit-learn ,"Keras, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning, Metadata routing, Non-euclidean metrics",,,,3,MLFlow,,less than an hour,2,Strongly agree,No,
9/19/2024 7:23:47,I have read this statement in its entirety and affirm the stated conditions.,4,6,1,8,5,5,1,,,,7,7,5,5,3,2,,"Confusion matrix, Reliability diagram, ROC curve, Feature importance, Residual plots, Learning curves","pandas, Polars",,"CatBoost, Keras, LightGBM, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,1,,,less than an hour,2,Strongly agree,,
9/19/2024 7:30:22,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,4,8,8,7,,,Explain in more details how each model works (in the models docs),7,7,7,6,6,7,7,"Confusion matrix, ROC curve, Feature importance",pandas,Easy to use,"CatBoost, Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,Feature importances,No,,,5,,,less than a day,1,Strongly agree,Yes,max.petitpain@gmail.com
9/19/2024 9:15:00,I have read this statement in its entirety and affirm the stated conditions.,7,6,1,7,3,4,1,1,More customizable regressors/classifiers,,7,2,5,5,2,1,1,"Learning curves, Other",pandas,,"Keras, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Metadata routing",No,,,2,,,less than an hour,More than 5,Agree,No,
9/19/2024 9:59:19,I have read this statement in its entirety and affirm the stated conditions.,6,6,6,7,6,5,3,4,,,3,6,2,3,1,3,,"Reliability diagram, ROC curve, Precision-Recall curve, Learning curves","pandas, Polars",,"CatBoost, XGBoost, Other",,Yes,Uncertainty estimates for prediction,,,,2,,,less than a minute,2,Strongly agree,,
9/19/2024 10:26:00,I have read this statement in its entirety and affirm the stated conditions.,5,8,,7,6,6,1,,"I think that sklearn fits very well into the ""small and simple"" ML, it already has great documentation and it easy to install, so it should be a priority, but there isn't too much room for improvement. Performance is always nice, but for large MLPs, there I don't see a point trying to beat PyTorch.","It's already perfect! If I had to choose, I'd go for some more ML methods, such as dimensionality reduction",1,5,1,5,6,7,1,"Confusion matrix, Precision-Recall curve",Polars,"Simple to use, support many methods, so it's trivial to ""test them all""",PyTorch,"LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,,No,,,1,Custom tool,Custom tool,"less than a minute, less than a day",,Strongly agree,No,
9/19/2024 13:05:07,I have read this statement in its entirety and affirm the stated conditions.,8,7,6,5,6,8,5,,,,7,7,7,7,7,7,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","cudf, Dask DataFrame, DuckDB, Modin, pandas, Polars, Spark DataFrame",A I,"CatBoost, Jax, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing",No,,,3,"DVC, Neptune, MLFlow, Weight and biases, Custom tool","Airflow, Argo, Coiled, Dagster, Kubeflow, Metaflow (outerbounds), Custom tool",more than a day,1,Strongly agree,No,
9/19/2024 22:11:11,I have read this statement in its entirety and affirm the stated conditions.,4,5,3,8,7,6,2,1,"I and my team use scikit-learn primarily because it is so well-documented, performant, and reliable: It’s essentially our reference machine learning framework. However, that only matters (in the long term) insofar as it continues to add implementations of techniques as they’re developed by the broader community. (For instance, until HDBSCAN was added, that algorithm was one of my main reasons to go outside scikit-learn.) So though I understand new features come with a maintenance burden, they’re also what’s likely to keep scikit-learn central to any machine learning project.",A time series split that also allows splitting by group: If comes up surprisingly often.,5,2,6,4,7,3,1,"Confusion matrix, Feature importance","pandas, Spark DataFrame","Pipelining, composition, and integrated hyperparameter tuning.","LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, Pipeline, ColumnTransformer, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Non-euclidean metrics",,,,1,MLFlow,,less than 10 minutes,More than 5,Strongly agree,Yes,geoffrey.a.reed@gmail.com
9/20/2024 0:18:35,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,8,,,7,7,7,7,7,7,7,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","cudf, Dask DataFrame, DuckDB, Modin, pandas, Polars, Spark DataFrame",,"CatBoost, Jax, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Cost-sensitive learning, Metadata routing",No,,,4,Other,Other,more than a day,1,Agree,No,
9/20/2024 3:09:42,I have read this statement in its entirety and affirm the stated conditions.,6,5,6,6,6,4,7,4,,,6,7,2,4,3,6,1,"Confusion matrix, ROC curve, Feature importance, Learning curves","DuckDB, pandas, Polars",,"CatBoost, Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances",No,,,4,MLFlow,"Airflow, Other","less than 10 minutes, less than an hour",3,Strongly agree,,
9/20/2024 4:18:19,I have read this statement in its entirety and affirm the stated conditions.,5,2,4,6,6,8,1,,,,,7,,,5,,,"Confusion matrix, ROC curve, Feature importance","pandas, Polars",Simplicity and at the same time exhaustivity,XGBoost,"RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Feature importances, Sample weights",No,,,1,MLFlow,,less than a minute,More than 5,Strongly agree,,
9/20/2024 6:20:41,I have read this statement in its entirety and affirm the stated conditions.,8,8,6,7,6,2,5,2,Hoping for features which are available in LightGBM or XGBoost but with the ease of scikit-learn,parallelity in HistGradientBoostingClassifier,3,2,,5,6,4,1,"Precision-Recall curve, Feature importance",pandas,The consistency!,"LightGBM, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Feature importances",No,,,4,MLFlow,,less than a day,1,Strongly agree,No,
9/20/2024 6:44:54,I have read this statement in its entirety and affirm the stated conditions.,8,2,2,8,2,3,1,1,Implement new features like Transformers for Time Series and improve overall performance with large datasets,"Improve perfomance by allowing using overs backend configuration easily like pyarrow, RAPIDS, JIT, etc.",3,3,6,5,4,5,1,"Confusion matrix, ROC curve, Feature importance","cudf, pandas, Polars, Spark DataFrame","Implementation of well know algorithms like CatBoost, Deep Learning models, etc.","CatBoost, Keras, LightGBM, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",Yes,Some callbacks!,"Stop iterations, logging, ect.",2,"MLFlow, Custom tool","Airflow, Kubeflow",less than an hour,More than 5,Strongly agree,No,
9/20/2024 6:54:50,I have read this statement in its entirety and affirm the stated conditions.,6,5,5,8,7,7,4,1,"Statistical capabilities are limited, glm estimators don’t have lasso or robust errors and do not warn for badly specified models (eg high multicolinearity). Pipelines can become more flexible and would like to be able to chain multiple estimators together. For instance have a pipeline A with an estimator be the input for a estimator or pipeline B. Add an option to make the splinetransformer orthogonal would be nice. I have the feeling there may be performance optimizations possible in grid search.",Splinetransformer that handles multicolinearity of splines and scales well also when using log-link estimators.,7,3,1,1,1,5,5,"Feature importance, Residual plots, Other","pandas, Polars",Pipelines,"LightGBM, Transformers","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Sample weights",Yes,"Offset, exposure (sample weights)",Otherwise you have a lot of manual data preprocessing and that defeats the purpose of having nice pipelines.,1,MLFlow,,less than 10 minutes,More than 5,Strongly agree,Yes,kjartan.majdandzic@gmail.com
9/20/2024 7:00:35,I have read this statement in its entirety and affirm the stated conditions.,5,5,3,8,7,7,5,4,,,7,7,3,6,5,4,,"Confusion matrix, ROC curve, Feature importance, Residual plots, Learning curves","cudf, pandas, Polars",,"CatBoost, Jax, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, Pipeline, ColumnTransformer",Yes,"Calibration of regressors, Feature importances, Sample weights",Yes,weights,,3,"MLFlow, Weight and biases, Other",,"less than a minute, less than a day",,Strongly agree,No,
9/20/2024 7:01:44,I have read this statement in its entirety and affirm the stated conditions.,5,5,5,8,2,2,2,5,,,3,7,3,4,7,6,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,easy to code ,"LightGBM, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Feature importances, Sample weights",No,,,4,,,less than a day,1,Agree,No,
9/20/2024 7:09:22,I have read this statement in its entirety and affirm the stated conditions.,5,8,5,8,5,5,1,7,Having competitive GBMs and neural network computations,performance on GBM,7,7,6,6,4,4,6,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Polars",,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost",Other,No,"Calibration of probabilistic classifiers, Calibration of regressors, Cost-sensitive learning",Yes,Group for example,,5,MLFlow,Airflow,less than a day,More than 5,Strongly agree,No,
9/20/2024 7:12:18,I have read this statement in its entirety and affirm the stated conditions.,5,7,7,7,7,7,6,,,,5,7,2,7,7,7,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","Dask DataFrame, DuckDB, pandas, Spark DataFrame",,"Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Non-euclidean metrics",,,,5,MLFlow,"Airflow, Kubeflow",less than an hour,More than 5,Strongly agree,Yes,debajyoti94@gmail.com
9/20/2024 7:27:39,I have read this statement in its entirety and affirm the stated conditions.,7,7,3,4,6,6,3,4,,,6,3,7,4,5,2,1,"Confusion matrix, Residual plots, Learning curves, Other","Dask DataFrame, Polars",,"Keras, LightGBM, PyTorch","Pipeline, Other",Yes,"Calibration of regressors, Sample weights, Metadata routing",Yes,"Target weights, metadata identifying the point",Easier to understand and keep track of input / output,4,Custom tool,Airflow,"less than a minute, less than a day",More than 5,Strongly agree,,
9/20/2024 7:41:42,I have read this statement in its entirety and affirm the stated conditions.,7,7,5,6,8,7,5,7,,,6,7,5,4,3,2,1,"Confusion matrix, ROC curve, Precision-Recall curve","pandas, Polars, Spark DataFrame",,"Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,5,MLFlow,Airflow,"less than 10 minutes, less than an hour",3,Strongly agree,No,
9/20/2024 8:11:22,I have read this statement in its entirety and affirm the stated conditions.,4,2,4,2,2,2,2,1,"I would split the whole scikit-learn package into a collection of sub-packages. This could benefit especially library maintainers, that now need to include the whole scikit-learn package when they often need just a few features.","Packaging, as explained before",3,3,3,2,2,3,1,"Confusion matrix, Precision-Recall curve, Feature importance",pandas,I like the consistency of the API and documentation,"LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",Yes,Sample weights are usually espressive enough to customize behavior. A useful addition could be weights for features,Changing feature weights could be useful for estimating feature importance a posteriori,1,DVC,,less than a minute,1,Strongly agree,Yes,lucasimi90@gmail.com
9/20/2024 8:34:23,I have read this statement in its entirety and affirm the stated conditions.,2,4,4,3,4,4,2,1,,,7,4,4,3,3,2,1,"ROC curve, Feature importance, Residual plots",pandas,,Other,"HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,"Exposure, weights",,1,MLFlow,Other,less than 10 seconds,More than 5,Agree,No,
9/20/2024 9:42:40,I have read this statement in its entirety and affirm the stated conditions.,8,8,7,8,7,7,1,,,,4,7,1,7,4,7,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","cudf, Dask DataFrame, pandas, Polars",,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,5,"MLFlow, Weight and biases, Other",Metaflow (outerbounds),less than a day,More than 5,Strongly agree,No,
9/20/2024 9:59:06,I have read this statement in its entirety and affirm the stated conditions.,,,,,,8,,,"Linking to other scikit learn API packages focused on survival analysis (scikit-survival) and time series (nixtla). Just linking them in a python ecosystem type page would grow visibility a lot. 

Learning to rank not well supported in python. Far as I know, only xboost and lighgbm do it well. ",Nothing. You guys are great. ,7,7,7,5,5,6,,"ROC curve, Precision-Recall curve, Feature importance","pandas, Polars, Spark DataFrame",Highly consistent user interface. ,"Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,4,MLFlow,,more than a day,More than 5,Strongly agree,Yes,gmcmacran@gmail.com
9/20/2024 10:57:22,I have read this statement in its entirety and affirm the stated conditions.,7,8,5,8,5,8,1,,Website don't seems too important. But New features are very nice. ,Improvements in column transformer and pipelines,7,7,7,7,5,7,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Spark DataFrame",The structure or pipelines and the fit predict,"CatBoost, LightGBM, PyTorch, XGBoost","LogisticRegression, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,"sample weight, and pre-score",More flexibility,5,"DVC, MLFlow","Airflow, Kubeflow","less than a minute, less than 10 minutes",5,Strongly agree,Yes,isaac.tsousa@gmail.com
9/20/2024 11:46:20,I have read this statement in its entirety and affirm the stated conditions.,1,1,4,8,7,8,1,1,,"Many objects are private (with an leading underscore). It would be cool to make more of them public, like the _MultiScorer object",5,5,7,4,3,2,,"Confusion matrix, Precision-Recall curve, Residual plots, Learning curves",pandas,,"LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",,,,3,Custom tool,Other,less than a minute,1,Strongly agree,Yes,marco.bresson@gmail.com
9/20/2024 12:14:39,I have read this statement in its entirety and affirm the stated conditions.,6,8,5,7,4,1,1,,,,6,7,4,2,3,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,,"LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,3,"MLFlow, Custom tool",Custom tool,less than 10 minutes,More than 5,Strongly agree,No,
9/20/2024 12:31:22,I have read this statement in its entirety and affirm the stated conditions.,2,4,4,4,4,4,4,2,"Support gpu acceleration,  like gridsearchcv","Gpu acceleration,  it quite pointless use scikit learn beside learning ",5,5,5,5,5,5,5,Other,pandas,Easy to use,"PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,Feature importances,No,,,5,Other,Other,more than a day,More than 5,Agree,No,
9/20/2024 12:32:38,I have read this statement in its entirety and affirm the stated conditions.,7,7,6,8,6,6,6,6,,,3,4,3,3,4,4,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",No,,,3,"MLFlow, Weight and biases",,"less than a minute, less than 10 minutes, less than an hour",3,Agree,No,
9/20/2024 14:34:31,I have read this statement in its entirety and affirm the stated conditions.,8,6,5,7,3,4,2,1,Scikit-Learn is a great library and it would great to add more features and uniformize output from all algorithms,Probability estimates for all algorithms ,2,3,5,7,6,4,1,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","Dask DataFrame, Modin, pandas, Spark DataFrame, Other",Great documentation and ease of usability ,"Keras, PyTorch, XGBoost",Other,No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,5,Other,Other,less than a day,1,Strongly agree,No,
9/20/2024 15:34:22,I have read this statement in its entirety and affirm the stated conditions.,5,,6,8,3,8,1,1,,"Going to specific sides of ML (Reinforcement learning, graph analysis, some things that I don't even know that it exist)",5,,1,2,7,7,1,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves","cudf, pandas, Other",,,"RandomForestClassifier or RandomForestRegressor, Pipeline",No,Feature importances,Yes,,,2,,"Airflow, Other",less than a day,,Strongly agree,Yes,mohamedbelditn@gmail.com
9/20/2024 16:59:50,I have read this statement in its entirety and affirm the stated conditions.,8,3,3,6,5,2,2,2,"Increase performance, multithreading Is still not for free and only on certain task",Multithreading,6,5,7,3,2,3,1,"Feature importance, Residual plots","pandas, Polars, Spark DataFrame",Standardization,"CatBoost, XGBoost","Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Feature importances",,,,4,MLFlow,Airflow,"less than an hour, less than a day",More than 5,Strongly agree,Yes,a.cale96@gmail.com
9/20/2024 18:05:02,I have read this statement in its entirety and affirm the stated conditions.,3,3,2,2,3,2,1,,"Skelarn already has a lot of features. I even can't know all about it. Some times I swith to other libs due to performance issues like ram usage. Also, tech documentation is always nice to work on, even if it's already good. ",Polars integration,2,2,3,,,,,"Confusion matrix, ROC curve, Learning curves","pandas, Polars, Spark DataFrame",Pipelines! ,"Keras, LightGBM, XGBoost","Pipeline, ColumnTransformer",Yes,Metadata routing,No,,,4,MLFlow,"Airflow, Other","less than an hour, less than a day",More than 5,Strongly agree,Yes,luis.meazzini@genesis-dataculture.com
9/20/2024 20:57:12,I have read this statement in its entirety and affirm the stated conditions.,7,7,7,6,8,7,6,6,Include more math in the documentation ,Change the return type of OHE to pandas dataframe ,6,7,4,4,6,6,4,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves",pandas,,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",No,,,2,,,less than an hour,,Strongly agree,No,
9/20/2024 23:05:05,I have read this statement in its entirety and affirm the stated conditions.,8,6,6,8,8,8,6,6,,,7,7,6,5,5,5,5,"Feature importance, Residual plots, Learning curves","pandas, Polars",,LightGBM,Pipeline,Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,,,1,Other,Other,less than a day,5,Agree,No,
9/20/2024 23:43:32,I have read this statement in its entirety and affirm the stated conditions.,8,8,5,5,8,8,5,5,,,7,7,5,5,5,5,5,"Confusion matrix, ROC curve, Feature importance, Residual plots, Learning curves",pandas,simple and easy to implement,PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,3,,,less than a minute,,Strongly agree,Yes,aanand-phy@spuvvn.edu
9/20/2024 23:56:11,I have read this statement in its entirety and affirm the stated conditions.,7,4,4,6,5,3,2,,,"Speed. Hot, nasty, speed. ",7,3,2,5,6,6,,"Confusion matrix, Feature importance","Dask DataFrame, pandas, Polars",,"LightGBM, PyTorch, XGBoost",LogisticRegression,No,"Uncertainty estimates for prediction, Feature importances",,,,4,,Argo,less than a day,More than 5,Agree,Yes,vito@interlinkedtx.com
9/21/2024 0:48:52,I have read this statement in its entirety and affirm the stated conditions.,4,4,2,2,3,,1,1,,,4,,4,2,2,2,4,"Confusion matrix, Reliability diagram, Precision-Recall curve, Learning curves","Dask DataFrame, DuckDB, pandas, Polars",Easy of use,"Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances",No,,,2,"DVC, MLFlow","Airflow, Argo","less than a day, more than a day",2,Strongly agree,Yes,Fernando.liaison@gmail.com
9/21/2024 0:52:41,I have read this statement in its entirety and affirm the stated conditions.,8,8,7,8,8,8,5,5,,,7,7,7,7,6,7,6,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Polars, Spark DataFrame",,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing",Yes,,,4,"Neptune, MLFlow, Weight and biases","Airflow, Kubeflow, Custom tool","less than an hour, less than a day, more than a day",5,Agree,Yes,
9/21/2024 2:44:20,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,7,8,7,5,4,,,1,1,2,1,2,1,4,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Spark DataFrame",pipelines,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Metadata routing, Non-euclidean metrics",No,,,5,MLFlow,Airflow,more than a day,More than 5,Strongly agree,Yes,ganeshbhat.3@gmail.com
9/21/2024 7:19:26,I have read this statement in its entirety and affirm the stated conditions.,2,3,3,2,3,3,1,1,,"Auto ml like mfeurers autosklearn. And not necessary to run on linux. 
Also, extend documentation with more theory and learning on concepts ",3,3,3,3,2,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Polars",Comprehensive. Well recognized by devs. Good documentation. ,"Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Non-euclidean metrics",Yes,Optimization choices,Not sure it always will,5,"MLFlow, Custom tool",Custom tool,less than an hour,More than 5,Strongly agree,No,
9/21/2024 9:06:26,I have read this statement in its entirety and affirm the stated conditions.,3,3,2,2,3,1,1,1,,,2,3,2,3,2,2,1,"Residual plots, Other",Polars,,,RandomForestClassifier or RandomForestRegressor,No,Non-euclidean metrics,Yes,,,2,,Other,less than 10 minutes,1,Strongly agree,No,
9/21/2024 14:31:12,I have read this statement in its entirety and affirm the stated conditions.,6,6,1,7,7,7,1,,"scikit-learn has the best documentation and education materials out there when it comes to a scientific library and it would be great if it stayed that way! As for new features, I am particularly looking forward to the metadata routing API and the callback API release.

I do not see much need of a website redesign or of improved packaging.","Transformer whose transform/inverse transform signature would be updated to be (X, y) -> (X, y) with all metaestimators updated accordingly.",6,7,7,7,5,5,3,"Confusion matrix, Precision-Recall curve, Feature importance, Residual plots, Learning curves","Dask DataFrame, pandas, Polars, Spark DataFrame, Other",The simplicity of its API and how extensible it is! Documentation is great too!,"CatBoost, Keras, LightGBM, PyTorch","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",No,,,3,"DVC, MLFlow","Kubeflow, Other","less than 10 seconds, less than an hour, less than a day",2,Strongly agree,Yes,gtauzin@protonmail.com
9/21/2024 15:02:46,I have read this statement in its entirety and affirm the stated conditions.,5,5,5,5,5,5,2,2,,,4,4,4,4,4,4,1,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves",pandas,,"CatBoost, Jax, Keras, LightGBM, PyTorch, XGBoost",HistGradientBoostingRegressor or HistGradientBoostingClassifier,Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,5,,,,,,,
9/21/2024 16:43:24,I have read this statement in its entirety and affirm the stated conditions.,3,2,2,3,2,1,1,3,,Performance on bigger datasets,1,3,1,1,1,2,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Spark DataFrame",Api,"Keras, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction",No,,,2,MLFlow,Other,less than a day,More than 5,Neither agree nor disagree,No,
9/21/2024 20:45:53,I have read this statement in its entirety and affirm the stated conditions.,4,1,1,7,7,8,1,1,I really liked yours technical and educational materials.,GPU compatibility.,7,7,5,6,4,6,7,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","DuckDB, pandas, Polars, Spark DataFrame",The amount of models and processing techniques that have.,"CatBoost, Jax, Keras, LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Uncertainty estimates for prediction, Feature importances, Sample weights",No,None,None,5,"DVC, MLFlow","Airflow, Dagster, Other","less than 10 minutes, less than an hour, less than a day",More than 5,Strongly agree,Yes,mg.21191077@gmail.com
9/22/2024 5:18:05,I have read this statement in its entirety and affirm the stated conditions.,4,1,1,8,4,1,8,6,"make cleaner website redesign similar to Hugging Face
provide example use cases for  each and every varaiations of ml algorthms",website redesign,1,1,7,5,1,1,7,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","cudf, pandas, Spark DataFrame",its fit and predict functions,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost, Other","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of regressors, Feature importances",No,,,5,"DVC, MLFlow",Airflow,"less than an hour, less than a day, more than a day",More than 5,Strongly agree,Yes,madhanlibonce94@gmail.com
9/22/2024 7:43:03,I have read this statement in its entirety and affirm the stated conditions.,4,3,3,3,2,2,1,2,,Integration ti XGBoost and LightGBM,6,6,6,4,4,4,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Polars",Easy API. Pipeline.,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost, Other",RandomForestClassifier or RandomForestRegressor,Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,Alpha values,,2,MLFlow,Other,less than an hour,More than 5,Strongly agree,No,
9/22/2024 7:58:55,I have read this statement in its entirety and affirm the stated conditions.,3,2,2,3,2,3,1,1,Scikit is the glue of production systems,Performance,2,2,2,2,2,2,1,"Precision-Recall curve, Feature importance, Learning curves, Other","Dask DataFrame, pandas, Polars, Spark DataFrame",Fit/prédit interface.,"CatBoost, Keras, LightGBM, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of regressors, Sample weights, Non-euclidean metrics",Yes,Runtime configs.,,2,"DVC, MLFlow",Airflow,less than an hour,4,Agree,No,
9/22/2024 8:43:36,I have read this statement in its entirety and affirm the stated conditions.,,,,8,,,,,The one implemented here : https://mvolkanyurtseven.medium.com/new-algorithm-for-outlier-detection-in-categorical-data-5cafd4c8429f,,,,,7,,,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","Dask DataFrame, DuckDB, Modin, pandas, Polars, Spark DataFrame",,"Keras, LightGBM, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,4,,Other,less than a day,4,Strongly agree,Yes,volkan.yurtseven@hotmail.com
9/22/2024 15:16:13,I have read this statement in its entirety and affirm the stated conditions.,7,6,5,7,6,4,1,4,"I love the library as is and believe it is growing at the right pace. Keeping algorithms/infrastructure simple, performant and reliable has always been scikit-learns bread and butter so that should be a priority. New features are always welcome as long as they dont break the reliability of the package, I always read the new package updates and see how they affect me. The documentation is top-notch given the amount of information, though it would be nice to have more informative and novel notebooks with application of said object. Education wise scikit-learn is easy to use and learn, fine as is. I think the website is great as-is as well. Its clean, neat and well put together. No need to go minimalist.","On the spot, more features/methods for feature selection/elimination/engineering would be interesting.",7,7,5,3,6,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,Ease of use and power. A true holy grail of ML.,"Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",Yes,,,3,"MLFlow, Weight and biases",,"less than 10 minutes, less than an hour, less than a day",2,Strongly agree,Yes,nichmitrea22@gmail.com
9/22/2024 15:43:43,I have read this statement in its entirety and affirm the stated conditions.,3,3,2,3,1,2,1,1,,,3,3,3,1,1,1,1,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots",pandas,,"Keras, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",No,Feature importances,No,,,2,,,less than 10 minutes,,Strongly agree,No,
9/22/2024 18:48:04,I have read this statement in its entirety and affirm the stated conditions.,4,6,6,7,8,7,3,4,My understanding of the place of scikit-learn is to be somewhat of an umbrella library for machine learning. It has implementations of as many model architectures as possible as is makes them accessible to as many people as possible. Performance of specific architectures is and should be handled by other libraries. Scikit-learn is great foot in the door for so many people to get into ML and using it for tasks. ,,5,6,6,7,5,5,4,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots","pandas, Spark DataFrame","That its reliable, has easy to use APIs and is well integrated with itself","CatBoost, Keras, LightGBM, PyTorch, XGBoost, Other","RandomForestClassifier or RandomForestRegressor, Pipeline, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning",No,,,3,"MLFlow, Other",Other,less than a day,1,Strongly agree,No,
9/22/2024 22:07:13,I have read this statement in its entirety and affirm the stated conditions.,7,6,4,5,4,4,4,,"Better performance (time & memory) for methods run on large datasets. Algorithmic tricks, and GPU and batching options (when appropriate) may help here. 

Expanding and standardizing options for supervised methods. E.g. n_jobs available in LinearRegression and LogisitcRegression, but not Ridge. 

Expanding neural network capabilities especially for MLP. n_jobs option and class_weight would be very useful.

More advanced Boosted Tree methods. ",Better performance. ,5,6,6,4,3,3,,"Confusion matrix, Precision-Recall curve, Feature importance, Residual plots",pandas,Easy to use. Common interface for most methods. Widely used. Well documented. Open source.,"CatBoost, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",No,,,4,"Weight and biases, Custom tool",Other,"less than 10 minutes, less than an hour, less than a day, more than a day",More than 5,Strongly agree,Yes,comacke@gmail.com
9/23/2024 1:44:11,I have read this statement in its entirety and affirm the stated conditions.,6,8,3,8,8,8,1,1,Add new algorithms and great tutorials and technical documentation.,Cost sensitive classification with user defined cost matrix.,4,7,7,4,7,7,,"Confusion matrix, ROC curve, Precision-Recall curve","pandas, Polars",The coherent and consistent API and documentation,"LightGBM, PyTorch","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of probabilistic classifiers, Cost-sensitive learning, Feature importances, Non-euclidean metrics",,,,2,Other,Custom tool,less than 10 minutes,5,Strongly agree,No,
9/23/2024 2:56:33,I have read this statement in its entirety and affirm the stated conditions.,6,,,7,,,,,"New features: more control on sequential feature selection with cross-validation, like cv results; more control over MLP configurations at training; separate class for hyperparameter space generation (like KerasTuner)",more control over MLP configurations at training,7,,,,,,,"Confusion matrix, Feature importance, Residual plots, Other",pandas,Convenient API,Keras,"Pipeline, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,4,Custom tool,Custom tool,less than an hour,,Agree,,
9/23/2024 6:05:46,I have read this statement in its entirety and affirm the stated conditions.,4,7,7,7,8,8,2,1,"I've learned about ml and how to do proper ml with scikit learn a lot. I have a lot of problems of packaging, versioning and running scikit learn models across different platforms and versions.",documentation and examples,5,6,4,6,7,7,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","Dask DataFrame, pandas, Spark DataFrame",simplicity and api being stable,"CatBoost, PyTorch, Transformers, XGBoost","LogisticRegression, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction",Yes,"i would like to keep some metadata: versioning, dataset used, classes names, etc.","keeping it all together, like a ""session"" or package that makes things easily reproducible. ",3,MLFlow,Kubeflow,less than an hour,More than 5,Strongly agree,Yes,luminoso@gmail.com
9/23/2024 6:37:57,I have read this statement in its entirety and affirm the stated conditions.,8,7,3,8,5,1,5,8,Be able to leverage GPU easily,,3,4,2,5,6,7,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,Easy to use,"CatBoost, Jax, Keras, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,Feature importances,No,,,5,"Neptune, MLFlow","Airflow, Kubeflow",less than a day,2,Strongly agree,No,
9/23/2024 8:00:51,I have read this statement in its entirety and affirm the stated conditions.,6,3,3,5,3,4,2,2,Perfomance is what lacks the most for my use case,,4,5,3,5,6,6,2,"Confusion matrix, ROC curve, Feature importance, Learning curves",pandas,,"Keras, PyTorch, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,2,Weight and biases,"Kubeflow, Custom tool",less than an hour,2,Agree,No,
9/23/2024 9:03:52,I have read this statement in its entirety and affirm the stated conditions.,2,3,2,3,3,2,1,1,,,3,1,2,2,1,1,1,Feature importance,pandas,,"Keras, XGBoost",LogisticRegression,No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,2,Other,Other,"less than a minute, less than 10 minutes",4,Strongly agree,,
9/23/2024 9:31:43,I have read this statement in its entirety and affirm the stated conditions.,6,7,6,6,8,7,4,1,Clearer explanation and examples of features uses would be very helpful.,"Clearer explanation of function arguments, uses and impacts.",7,7,7,4,4,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,That it supports (almost) end-to-end data science pipelines,"CatBoost, LightGBM, PyTorch, XGBoost","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",Yes,Modified loss function,More flexible training,3,Other,Airflow,less than 10 minutes,More than 5,Strongly agree,No,
9/23/2024 10:45:23,I have read this statement in its entirety and affirm the stated conditions.,6,4,2,7,4,4,2,,"Documentation and reliability are already great!  Improvement areas I would see:
1. Extendability.  I use scikit for research so I need to implement my own models.  It would be nice if some internal methods were put in utils.  Current trend seems opposite: e.g. the dictionary of named scorers became private.  Some sanity checks could be bypassed: e.g. cross entropy loss is in fact implemented but hard check on class variable makes e.g. soft labels impossible.  Extending decision trees with modified splitting criteria is also very hard 
2. Improvements to pipeline/transformers such that undersampling or adding sample weights becomes possible",Improvements to pipeline/transformers such that undersampling or adding sample weights becomes possible,3,6,2,1,4,2,,"ROC curve, Feature importance, Residual plots, Learning curves","pandas, Polars",Solid implementations.  Great documentation,"Jax, Keras","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Sample weights, Metadata routing",Yes,sample weights (already available but cannot be added in a pipeline),Handling data bias / distribution shifts would become easier,2,Custom tool,,"less than a minute, less than a day",,Strongly agree,Yes,jszymon@gmail.com
9/23/2024 10:53:09,I have read this statement in its entirety and affirm the stated conditions.,6,7,5,7,4,3,1,,,,5,5,1,1,6,1,1,Feature importance,pandas,,"PyTorch, XGBoost",Other,No,"Feature importances, Non-euclidean metrics",No,,,4,,,less than 10 minutes,1,Agree,No,
9/23/2024 13:10:06,I have read this statement in its entirety and affirm the stated conditions.,7,8,7,6,7,4,3,,,,7,6,1,4,3,5,2,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","cudf, pandas, Spark DataFrame",,"Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,5,Weight and biases,,"less than 10 seconds, less than a minute, less than 10 minutes, less than an hour, less than a day",4,Strongly agree,Yes,connectwithparam.30@gmail.com
9/23/2024 14:19:20,I have read this statement in its entirety and affirm the stated conditions.,4,5,6,6,4,3,2,2,,,4,6,5,4,3,4,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,,XGBoost,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,2,,Airflow,less than an hour,2,Strongly agree,Yes,christian.berthelsen@vegvesen.no
9/23/2024 16:01:14,I have read this statement in its entirety and affirm the stated conditions.,8,7,5,8,3,7,1,,,,6,6,7,7,5,5,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Learning curves","cudf, pandas, Polars",,"CatBoost, PyTorch, Transformers","LogisticRegression, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances",Yes,,,,"Neptune, MLFlow",Airflow,less than an hour,4,Strongly agree,No,
9/23/2024 16:41:39,I have read this statement in its entirety and affirm the stated conditions.,8,3,3,8,8,8,3,1,,,7,7,7,2,7,7,,"Confusion matrix, ROC curve, Precision-Recall curve",pandas,,"LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction",No,,,5,"Weight and biases, Custom tool",,less than a day,3,Strongly agree,,
9/23/2024 16:49:52,I have read this statement in its entirety and affirm the stated conditions.,7,2,2,7,6,1,1,1,main issues are performance and lack of certain features,add sample weight to RFECV,7,6,5,2,4,3,1,"Confusion matrix, Feature importance, Residual plots, Learning curves, Other",pandas,pipelines!,"CatBoost, LightGBM, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",Yes,sample weight (redundant I know),reduce impact of low sample outliers,2,,Airflow,less than an hour,More than 5,Strongly agree,Yes,nwalker@warriors.com
9/23/2024 17:20:54,I have read this statement in its entirety and affirm the stated conditions.,6,6,6,6,8,8,6,,I think more simple or ELI5 examples would be great. I often find that some of the documentation immediately skips to the most complex possible option. ,Adding a nice summary function for a linear regression. ,7,7,7,5,5,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","DuckDB, pandas, Polars",Everything being compatible with everything else is absolutely the best part. Being able to write custom sklearn compliant tooling as well. ,"CatBoost, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,sample weights,just makes it easier to do it all in one,1,"MLFlow, Weight and biases, Custom tool","Airflow, Other","less than an hour, less than a day",4,Strongly agree,Yes,andrew.patton@nfl.com
9/23/2024 17:32:52,I have read this statement in its entirety and affirm the stated conditions.,7,7,3,8,8,8,2,1,"I feel that we cannot learn datascience with the scikit learn documentation, but if we know datascience, we learn to use the scikit-learn",Having more time series related models,3,3,7,5,6,4,2,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",Polars,the coherence in the API between the different class,"PyTorch, Transformers, XGBoost",RandomForestClassifier or RandomForestRegressor,No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing",No,,,5,"DVC, Other",,"less than 10 seconds, less than an hour",1,Strongly agree,Yes,zrrzahymn@mozmail.com
9/24/2024 2:03:01,I have read this statement in its entirety and affirm the stated conditions.,3,2,8,4,7,8,5,,,Easy to implement guides on deploying sklearn models to an API on the cloud,6,7,2,3,6,7,,"Confusion matrix, ROC curve, Feature importance",pandas,Same syntax for different models,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Feature importances",No,,,4,,,less than 10 minutes,More than 5,Strongly agree,Yes,Hasdeep.sethi@strat7.com
9/24/2024 3:21:53,I have read this statement in its entirety and affirm the stated conditions.,5,5,5,8,8,8,5,5,I need to find good material to1 learn how to use the package ,Better in depth learning tutorial ,7,5,2,7,3,3,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,The unified API design and ease of use ,"CatBoost, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,3,,,"less than 10 minutes, less than an hour",2,Strongly agree,Yes,ahmed.ismail2013@feps.edu.eg
9/24/2024 3:48:39,I have read this statement in its entirety and affirm the stated conditions.,2,2,4,8,7,5,2,2,"I would really like scikit-learn to incorporate statistical and econometric models (and tests) a lot more, especially time series and panel models. Those are extremely useful in the industry for any prediction or causal question involving sales, activity level, finances, etc...",Add more statistical models,7,5,7,5,7,2,2,"Confusion matrix, Precision-Recall curve, Feature importance, Residual plots, Other","pandas, Polars, Spark DataFrame",The syntax is very simple,Other,Other,Yes,"Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,"A matrix or an array which specify the data and DGP structure, for example a list of indexes specifying that we have time series data so that learning is only done on the past, for example arrays specifying hierarchical relationships within the data one element of the array corresponds to a group of individuals then for each individual in each element of the array there is a list of indexes specifying the time so that learning can only be done on the past and groups effects can be introduced","It would not benefit the training but the prediction part, it would extend regression and classification model for cross-sectional data other types of data",2,MLFlow,"Custom tool, Other","less than a minute, less than 10 minutes",More than 5,Strongly agree,No,
9/24/2024 6:30:02,I have read this statement in its entirety and affirm the stated conditions.,7,8,7,7,8,8,5,,"Large and better documentation are important to keep sklearn as the go for ML lib.
Support ",Support to KS classification metric support and output probability binning ,3,7,7,5,5,6,7,"ROC curve, Feature importance, Residual plots","cudf, pandas, Polars, Spark DataFrame",Documentation,"LightGBM, Transformers, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Sample weights",Yes,"the counts.  In many cases, many combinations rows of X aren't unique and could be summarized into a count column. During training each count could contribute as 1 iteration. SAS software has this feature.",speed up!,4,"MLFlow, Custom tool","Airflow, Kubeflow, Custom tool",less than an hour,More than 5,Strongly agree,Yes,jeleandro@gmail.com
9/24/2024 7:04:07,I have read this statement in its entirety and affirm the stated conditions.,2,3,2,4,3,4,2,1,,,2,4,2,4,2,4,1,"Confusion matrix, Feature importance","Dask DataFrame, DuckDB, pandas",,"CatBoost, LightGBM, PyTorch, XGBoost, Other",Other,No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",No,,,4,MLFlow,,more than a day,More than 5,Strongly agree,No,
9/24/2024 8:01:47,I have read this statement in its entirety and affirm the stated conditions.,1,1,1,1,1,1,1,1,Awesome,"needed SMOTE , AdjustedR2score . and other data imbalance solving techniques",,,,,,,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Other","Documentation and ease to use, conatains almost everything","CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Non-euclidean metrics",No,,,3,"MLFlow, Weight and biases",,"less than an hour, less than a day",More than 5,Strongly agree,No,
9/24/2024 9:54:19,I have read this statement in its entirety and affirm the stated conditions.,5,6,6,8,7,8,1,1,,,5,7,5,6,6,6,,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves","Dask DataFrame, pandas, Spark DataFrame",,"Keras, LightGBM, PyTorch, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, Other",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Non-euclidean metrics",,,,5,"DVC, MLFlow",,"less than an hour, more than a day",More than 5,Strongly agree,,
9/24/2024 16:51:38,I have read this statement in its entirety and affirm the stated conditions.,7,7,1,8,6,1,1,1,,,7,7,5,4,6,7,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","Dask DataFrame, pandas",,"PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Non-euclidean metrics",,,,3,,,"less than 10 minutes, less than an hour",More than 5,Strongly agree,,
9/24/2024 16:55:08,I have read this statement in its entirety and affirm the stated conditions.,8,7,,8,8,8,7,,Documentation for (absolute) beginners please!,Integration with ChatGPT,6,5,7,4,2,3,1,"Confusion matrix, Precision-Recall curve, Feature importance",pandas,Easy API,XGBoost,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances",No,,,2,,Custom tool,less than 10 seconds,1,Strongly agree,No,
9/24/2024 18:26:27,I have read this statement in its entirety and affirm the stated conditions.,4,,,,6,3,,,,"I'm most interested in having clearly-documented, consistent, and intuitive control over the use of multiprocessing / multithreading in SKL.  I'd like to make smart, on-the-fly decisions about use of parallelism, but the status quo is very confusing / opaque / imprecise in the level of control available.",7,7,1,1,4,1,1,Confusion matrix,"Dask DataFrame, pandas","Clarity of documentation, consistency of API's",,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Metadata routing",Yes,Feature names,,3,,,less than an hour,2,Strongly agree,No,
9/25/2024 2:11:45,I have read this statement in its entirety and affirm the stated conditions.,6,4,2,8,5,2,1,5,"I teach ML via. scikit-learn and below are my thoughts:

- Performance: For teaching purposes, the performance is good. For real world usage on large datasets it becomes a challenge without GPU support. I know that GPU support may not be under the horizon for scikit-learn, but if this could be added it would be a big help.

- Reliability: No issues for me

- Packaging: Pretty good, with .fit() and .predict() methods. Everything is standardized and same steps

- New features: Yes yes yes! The more recent ML algorithms that have  found general acceptance in real world applications may be implemented.

- Technical Documentation: This is detailed and very well explained and with appropriate references for further reading. Could there be more explanation on the ""why?"" for each symbol in an equation. What does it do? What it is needed? This will help beginners.

Educational materials: There is already a comprehensive repository with over 300 notebooks. If possible, could there be more reasoning on why a method was used? why not other? etc. 

Website redesign: No issues for me.

- Other (just a few thoughts!)
1. Gradient Boosting Variants (like LightGBM, CatBoost)
2. Deep Learning Algorithms
3. Reinforcement Learning (RL) Algorithms
4. Variational Inference Methods
5. Graph-Based Neural Networks (e.g., GCN, GAT)
6. Online Learning Algorithms (like Vowpal Wabbit)
7. Hierarchical Bayesian Models
8. Large-Scale Linear Models (like FTRL, PEGASOS)
9. Bayesian Non-Parametrics (e.g., Gaussian Processes with Complex Kernels)
10. Factorization Machines
11. Bayesian Optimization
12. Neural Architecture Search (NAS)

Thank you scikit-learn team!!! You are doing an amazing job. Keep it up. Your efforts make a huge difference in the real world.","(1) Explanation in documentation that gives the reasoning behind: what a symbol in an equation does, why it is needed, what happens if it is increased or decreased or omitted, (2) guidelines on when to use which method and why. I understand scikit-learn user must know the ML basics beforehand, (3) help to learn how to read the ""source code"", it has so many classes that students get confused to find the exact line or block of code where the actual calculation takes place. However, in my teaching, I have come across several students who are eager to jump into using ML in scikit-learn to develop interest. Then when they see how well it works, it motivates them to do a deep dive into the algorithm.",7,7,7,2,3,5,5,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other",pandas,"Standardized implemetation methods, comprehensive documentation with examples and references","Keras, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,Feature importances,Yes,"sample weights, class weights, categorical indicators, groups, validation data, feature names, initial parameters, precomputed kernels, pentalty",It could help customize the training and thus help navigate the training process.,5,,,less than a day,1,Strongly agree,Yes,learndataa@gmail.com
9/25/2024 4:46:27,I have read this statement in its entirety and affirm the stated conditions.,5,5,3,3,6,8,1,1,"Especially for occasional users, documentation and examples ""from the source"" are key. I do not need better performance or new features for the work I do with it.","Education material, examples, use cases.",6,6,5,3,3,5,1,"Confusion matrix, Residual plots","pandas, Spark DataFrame",That it exists. :),,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Feature importances, Sample weights",No,,,4,,,less than 10 minutes,1,Strongly agree,No,
9/25/2024 7:26:24,I have read this statement in its entirety and affirm the stated conditions.,5,6,3,4,1,7,8,2,"The webstie design looks kynd of old to me, for this reason this is the number 1 priority. The documentation is wonderfull, well written and with a lot of tutorials and examples, it sometimes server as a technical book on how models and algorithms works, because of that i put it in last place and educational materials on second place, the great quality of educational materials makes more of them needed.",,4,5,3,2,6,7,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,,"CatBoost, Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Calibration of probabilistic classifiers, Feature importances, Sample weights",No,,,3,MLFlow,,less than 10 minutes,2,Strongly agree,No,
9/25/2024 12:23:58,I have read this statement in its entirety and affirm the stated conditions.,4,,4,8,,,,,"scikit-learn is a reliable, helpful library for many fundamental ML tasks. It's API is easy to learn and follow and there is a large ecosystem of libraries that interoperate with scikit-learn, making it a key part of many ML applications.

I left most of my responses blank because scikit-learn is already fantastic as a fundamental ML library. The documentation is both functional and educational -- I've learned a lot about machine learning by reading through the user guide and the many examples on scikit-learn's documentation. I frequently use scikit-learn's pipelines, preprocessing, and plotting utilities in my ML applications. Its integration with joblib enables parallelism to use all available CPU resources when calling a model's .fit() method, so its performance is quite good, even though it is surpassed by more specialized libraries like LightGBM.

scikit-learn's latest website iteration is great. The docs look modern, clean, and well-designed. I really like the new format based on the PyData Sphinx theme.

I believe priorities for future development should center on new features. Much has changed in the ML landscape in recent years, with special emphasis on generative AI models. I know it might be out-of-scope for scikit-learn, but it could be nice to consider including features related to modern ML and deep learning. For example, vector similarity search, text embeddings, model and experiment tracking, advanced hyperparameter optimization methods, convenient processing of online data (e.g., extracting relevant text and images from a website for building a training dataset), and optional hardware acceleration support.

Other helpful integrations with newer libraries might include: polars (already supported by scikit-learn), DuckDB (supported because DuckDB can convert to dataframes that scikit-learn can use), MLFlow for experiment tracking, Ray for distributed compute, Altair or hvPlot for interactive visualization

For packaging, I think that a native integration with skops could help avoid the use of pickle, which is discouraged in many settings due to its security risks.",Integrated experiment tracking,2,7,4,1,5,3,6,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","DuckDB, Polars","I really like the amazing documentation: an educational and approachable user guide, clear API reference, and abundant examples. I also really like composable pipelines; the ease of creating custom estimators or transformers thanks to duck-typing and instructive guides; and the abundant helpful utilities that simplify my workflow: downloading or creating example datasets, splitting data into train/val/test groups even with timeseries data, measuring and plotting model performance, and feature selection.","Keras, LightGBM, Transformers","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,1,Custom tool,,less than a minute,More than 5,Strongly agree,Yes,ryan.parker2@outlook.com
9/25/2024 16:12:38,I have read this statement in its entirety and affirm the stated conditions.,5,6,4,8,8,5,1,1,,,7,7,7,5,7,4,2,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Spark DataFrame",,"LightGBM, PyTorch, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",No,,,1,MLFlow,,less than 10 minutes,More than 5,Strongly agree,Yes,alexandre.miot@sgcib.com
9/26/2024 2:52:47,I have read this statement in its entirety and affirm the stated conditions.,4,4,4,6,6,4,2,4,,,6,4,2,7,5,3,1,,pandas,"I really like the consistent class structure of all model objects, it makes it much easier to use new models or swap them out in code.",PyTorch,,Yes,,No,,,1,,,less than a minute,1,Strongly agree,No,
9/26/2024 3:21:29,I have read this statement in its entirety and affirm the stated conditions.,7,6,5,6,5,3,2,4,,,6,4,3,4,5,3,4,"Confusion matrix, ROC curve, Feature importance, Residual plots, Learning curves","Dask DataFrame, pandas, Polars",Many utliity functions (e.g. concerning cross-validation) and the estimator API,"LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,3,,"Coiled, Other",less than a day,2,Strongly agree,No,
9/26/2024 8:40:18,I have read this statement in its entirety and affirm the stated conditions.,5,6,5,6,6,6,4,2,,,2,7,1,2,3,3,1,"Confusion matrix, Learning curves",pandas,,"PyTorch, Transformers, Other","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,,No,,,4,Other,Other,less than 10 minutes,2,Agree,No,
9/26/2024 9:13:11,I have read this statement in its entirety and affirm the stated conditions.,7,8,3,6,5,4,2,1,,,6,3,7,4,2,5,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,It's very easy to use since the documentation is very clear and easy to find.,"Keras, LightGBM, PyTorch, Transformers, XGBoost, Other","RandomForestClassifier or RandomForestRegressor, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,3,Custom tool,,"less than an hour, less than a day, more than a day",3,Strongly agree,No,
9/26/2024 9:40:29,I have read this statement in its entirety and affirm the stated conditions.,3,3,1,4,2,1,1,,"I don't see the necessity to add much. The main issue I have with scikit-learn it is ""too easy"" to use and thus so often misused by users without a minimum background in ML.","Provide serious ML documentation for the methods proposed, not just an enumeration of a large number of methods. Most of scikit-learn users need guidance to avoid pitfall. Those ""in the know"" would go to PyTorch or TensorFlow.",7,4,1,1,7,1,1,"Confusion matrix, ROC curve, Residual plots, Learning curves","cudf, pandas",Could be a great entry point for very light ML,"Keras, PyTorch, Transformers, Other","LogisticRegression, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Non-euclidean metrics",No,,,5,"Weight and biases, Custom tool",Other,"less than a day, more than a day",3,Strongly agree,No,
9/26/2024 9:43:06,I have read this statement in its entirety and affirm the stated conditions.,8,7,4,6,3,5,2,1,,Better Polars integration,6,7,2,5,4,3,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Polars",Consistent syntax.,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,4,Weight and biases,,more than a day,,Strongly agree,No,
9/26/2024 9:46:30,I have read this statement in its entirety and affirm the stated conditions.,8,4,3,7,5,6,2,1,,,7,6,3,2,5,4,1,"Confusion matrix, ROC curve, Feature importance","pandas, Polars",,"CatBoost, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,Sample weights,,4,"MLFlow, Weight and biases","Airflow, Kubeflow",less than an hour,More than 5,Strongly agree,No,
9/27/2024 0:01:51,I have read this statement in its entirety and affirm the stated conditions.,6,5,5,4,5,5,5,,,,6,6,6,6,5,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,,"CatBoost, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Uncertainty estimates for prediction, Feature importances",No,,,3,Weight and biases,,less than a day,1,Strongly agree,No,
9/27/2024 3:37:41,I have read this statement in its entirety and affirm the stated conditions.,8,8,4,8,8,4,1,,,,7,2,6,5,4,3,,"Confusion matrix, Feature importance, Residual plots","Dask DataFrame, pandas, Spark DataFrame",,"LightGBM, PyTorch, Transformers, XGBoost, Other","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Non-euclidean metrics",Yes,weights,nibba plz,,MLFlow,Airflow,"less than 10 minutes, less than an hour, less than a day, more than a day",More than 5,Strongly agree,No,
9/27/2024 5:22:43,I have read this statement in its entirety and affirm the stated conditions.,5,6,4,8,3,4,1,1,,"Missing and categorical data handling, even with an argument. ",7,7,6,4,5,2,,"Confusion matrix, ROC curve, Feature importance",pandas,The intuitive and consistent design,"LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances, Non-euclidean metrics",Yes,Missing and categorical handling confirmation ,"Model-specific handling, meaning that missing can be, for example, a category for classifiers",1,,,"less than a minute, less than 10 minutes",2,Strongly agree,Yes,Julian.Borisov@yahoo.com
9/27/2024 7:58:30,I have read this statement in its entirety and affirm the stated conditions.,7,7,7,8,7,8,7,7,"I really like the library and expecially it's documentation, I would like to see new features, and explanatins as educational materials. ",education materials,3,7,3,5,4,6,,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves",pandas,"It is clear how to use the methods, works fine","Keras, PyTorch","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Cost-sensitive learning, Metadata routing",No,,,5,"MLFlow, Weight and biases",,less than an hour,3,Strongly agree,No,
9/27/2024 8:32:46,I have read this statement in its entirety and affirm the stated conditions.,7,6,2,5,2,4,1,1,,,6,7,5,3,5,5,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other",pandas,,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,Feature importances,No,,,4,,,less than an hour,1,Strongly agree,,
9/27/2024 12:00:36,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,3,1,1,"Improving performance should be the highest priority, especially with the changes in Python 3.13 that allow the side stepping of the GIL. Parallelization would be highly desirable for many tasks in sklearn. ",,7,,7,7,4,4,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Polars, Spark DataFrame",The API is extremely easy to use and the documentation is phenomenal. ,"Keras, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",Yes,Hyperparameters such as learning rate and learning rate decay,Allows granularity over model training,5,Weight and biases,,less than a day,More than 5,Strongly agree,No,
9/27/2024 14:54:06,I have read this statement in its entirety and affirm the stated conditions.,7,7,3,8,8,8,3,,I think expansion of regression tools to better reflect those in statsmodels or R would be a great builder of the sklearn universe. additionally the example use case and educational documents are great,"more extensive and robust regression analysis tooling (types of regressions, weighting, plotting, summary stats, performance) ",7,3,7,5,5,4,4,"Confusion matrix, Feature importance, Residual plots","pandas, Polars",the pipeline functionality and easy of cross validation ,"XGBoost, Other","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,weighting of samples,using regression analysis with data that clearly has a underlying distribution change according to time due to idiosyncratic changes of the data being modelled should be better represented by a weighted regression give more credence to the most recent data,4,MLFlow,"Airflow, Argo, Custom tool",less than an hour,More than 5,Strongly agree,No,
9/27/2024 15:50:20,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,6,8,7,5,Please make it in hindi ,Bring some commercial topics ,7,7,7,7,7,7,5,"Confusion matrix, Reliability diagram, ROC curve, Feature importance, Residual plots, Learning curves","cudf, Dask DataFrame, pandas",,"Keras, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",,"Calibration of regressors, Cost-sensitive learning, Feature importances",,,,3,MLFlow,,"less than 10 minutes, less than an hour",2,,,
9/27/2024 17:12:55,I have read this statement in its entirety and affirm the stated conditions.,6,6,7,7,7,7,1,1,,,6,7,4,4,4,,4,"Confusion matrix, Feature importance, Other",pandas,,Keras,"RandomForestClassifier or RandomForestRegressor, Other",,Feature importances,,,,,,,less than 10 minutes,,Strongly agree,No,
9/27/2024 22:02:14,I have read this statement in its entirety and affirm the stated conditions.,8,8,1,8,4,4,1,,,,7,7,7,3,3,3,,"Confusion matrix, Feature importance, Residual plots",pandas,,"PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Feature importances",Yes,Weights,,3,MLFlow,,less than a minute,1,Strongly agree,No,
9/28/2024 3:06:50,I have read this statement in its entirety and affirm the stated conditions.,7,6,5,5,5,4,3,5,"I feel good about the current direction of sklearn. At work we use high volumes of data (pyspark), it would be nice if I can somehow use sklearn on that type of data. ","The ability to handle distributed, large-scale data easily (e.g. pyspark). ",6,6,5,6,6,6,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Other","pandas, Spark DataFrame","It's well-documented, reliable, and simple to use. I also like the large number of supported models. ","CatBoost, Keras, LightGBM, PyTorch, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,"Sample weights, but that's already possible. ",I think that is self-explanatory.,2,MLFlow,"Airflow, Custom tool","less than 10 seconds, less than a minute, less than 10 minutes, less than an hour",More than 5,Strongly agree,No,
9/28/2024 3:52:02,I have read this statement in its entirety and affirm the stated conditions.,7,5,8,8,8,7,2,1,"From packaging I understand ease of usability and re-usability, with make_blobs being a good example of helping generate random datasets quickly. Since DS/DA work involves a lot of experimentation work which is done in a stand alone python env, is it possible to introduce micro packages of commonly used scikit-learn modules, eg: MinMaxScaler, GridSearchCV, a seperate pip install for parametric models alone or linear models etc.

This way one does not have to pip install the whole scikit-learn ",The micro-packaging explained above.,5,7,7,7,5,7,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Spark DataFrame",East of use and performance with mainly ML related use-cases and quick experimentation and inference via plots.,"CatBoost, Keras, LightGBM, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, ColumnTransformer",No,"Cost-sensitive learning, Feature importances, Sample weights",,,,4,"MLFlow, Weight and biases",Airflow,"less than 10 minutes, less than an hour, less than a day",More than 5,Strongly agree,Yes,saldanhadeepakuconn@gmail.com
9/28/2024 9:23:46,I have read this statement in its entirety and affirm the stated conditions.,8,8,5,5,1,3,3,1,,,7,6,,4,7,5,3,"Confusion matrix, Reliability diagram, ROC curve, Residual plots",pandas,,"Keras, PyTorch","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Sample weights",,,,5,,,less than an hour,1,,,
9/28/2024 10:23:11,I have read this statement in its entirety and affirm the stated conditions.,6,5,4,8,8,8,,,,,7,7,7,7,,,,Other,pandas,,Keras,ColumnTransformer,No,"Calibration of probabilistic classifiers, Non-euclidean metrics",,,,4,Other,Custom tool,less than an hour,2,Strongly agree,No,
9/28/2024 15:07:15,I have read this statement in its entirety and affirm the stated conditions.,8,7,6,6,7,7,3,1,,"Improving performance of slow algorithms, Adding  some new package for automatic deployments.",3,7,2,4,6,5,1,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves","cudf, pandas, Polars","The unified Estimator APIs, the great docs, guides and tutorials.","PyTorch, Transformers, XGBoost","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",,,,4,MLFlow,Other,less than 10 minutes,More than 5,Strongly agree,Yes,esdairi.mohamed@gmail.com
9/29/2024 10:47:36,I have read this statement in its entirety and affirm the stated conditions.,4,3,3,3,2,2,3,1,,,4,3,3,2,3,3,1,"Confusion matrix, Feature importance, Learning curves","pandas, Polars",Ease of use and unified interface ,"CatBoost, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,3,MLFlow,Dagster,less than a day,2,Strongly agree,No,
9/30/2024 3:41:17,I have read this statement in its entirety and affirm the stated conditions.,8,3,3,7,8,3,5,1,I think improving performance should be a major point before adding new features and then retrofitting the documentation for the entire library.,Improved performance,7,5,7,3,5,4,,"ROC curve, Precision-Recall curve, Residual plots, Learning curves",pandas,It's simple and isn't extremely difficult to get started or to get a project off the ground.,"Keras, XGBoost","LogisticRegression, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances",No,,,5,"Neptune, MLFlow, Weight and biases","Argo, Kubeflow",less than 10 minutes,More than 5,Strongly agree,No,
9/30/2024 4:58:33,I have read this statement in its entirety and affirm the stated conditions.,7,5,5,8,8,6,4,3,Tools for training large datasets or tools to use new data stacks,Use polars for model training,6,6,7,5,2,3,,"ROC curve, Precision-Recall curve, Feature importance","DuckDB, pandas, Polars, Spark DataFrame",Permutation importance and other frameworks adapt to scikit learn such as xgboost,"LightGBM, PyTorch, Transformers, XGBoost","HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Feature importances",Yes,Graph knowledge information,Better inform model training ,4,MLFlow,"Airflow, Custom tool",less than an hour,2,Strongly agree,Yes,miguelmayhem92@gmail.com
9/30/2024 5:28:58,I have read this statement in its entirety and affirm the stated conditions.,7,6,5,8,6,6,1,7,"Most importantly of all, I think sklearn has a lot of power in how all other packages are used. New Features don't necessarily mean ""CoolNewRegressor"". Most important of all is shaping the APIs for things like dropping rows in pipelines and better probabilistic outputs as first class citizens. There are many solutions out there for both of these, and none are quiet satisfactory and certainly not standardised. Sklearn should try and standardise this. This is where I feel it can make the biggest improvements to the scientific computing/ML world. 

Docs and examples are really excellent. Maintaining this would be good. Packaging and reliability are both satisfactory.",API for dropping rows and better probabalistic/multi-quantile support in pipelines. ,7,4,6,5,2,3,1,"Feature importance, Residual plots, Learning curves, Other",pandas,Unified API. Fairly easy to extend. Super easy to use. Amazing docs and examples.,"LightGBM, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,"For ensemble models, things like callback for hyperparams, freature interactions, constraints.
E.g. Allowing no feature interactions in the first 100 tiers, then allowing 2nd order, the 3rd order. For hyperparams, reduce learning rates or tree depths w.r.t. iteration. ",Model training can be more bespoke. It can also allow testing of ideas/concepts for stand alone models.,3,MLFlow,"Airflow, Dagster, Kubeflow",less than a minute,More than 5,Strongly agree,Yes,Joshua.t.dunn@hotmail.co.uk
9/30/2024 6:44:48,I have read this statement in its entirety and affirm the stated conditions.,8,8,5,8,3,3,1,1,Performance and new features,Performance,4,7,4,7,7,7,4,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,Simple interfaces,"PyTorch, Transformers",Other,No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Non-euclidean metrics",No,,,5,Weight and biases,Other,more than a day,1,Strongly agree,No,
9/30/2024 7:19:14,I have read this statement in its entirety and affirm the stated conditions.,8,,,8,,,8,,"(1) Scikit-learn should see a lot more algorithms enter the space, or a Scikit-learn subsidiary should have an experimental space where new algorithms are fed in, following the Scikit-API ecosystem. Allowing Scikit-learn to be used more extensively in tabular supervised learning, as there have been numerous papers demonstrating the effectiveness of machine learning algorithms against deep learning neural net architectures. However, because Gradient Boosting appears to be the best option anyway anytime these days, we perhaps may have stopped looking at new ML algorithms. Scikit-learn should get a new skin; it has a community, tutorials, and every university in the world teaches Sklearn. However, Sklearn should not age in the same way that humans do; rather, the opposite. It should be updated every decade with a new skin, a set of new algorithms in whatever the ML workflow stage, and a sort of data flow of algorithms that are being continuously experimentally tested by the community and voted to be included in Sklearn, among other things.  As a result, performance is always continuously going up, and new features are constantly introduced for users, which is exactly what they want.

(2) Another new feature I am sort of dreaming of would be to allow all types of ML applications to mirror the steps of Scikit-Learn by building on top of Sklearn, modifying Sklearn estimators, complying with Sklearn's API, and so on. Hence, the feature would be to sort of creating a way to promote them out. We already like Sklearn, but it has the same scope as everything else in the world. It is unable to address highly specific issues. While doing so, researchers are increasingly creating such things, particularly in the last three years due to research open source trends. However, they are not promoted, and most people believe that Sklearn, Pytorch, and Keras are the only options. In broad terms, obviously.

As a result, by promoting those projects, Skrub can be a good candidate to show that if anyone wants to sort of modify Sklearn to their niche real-world scenario's tasks, Skrub might help you out in converting your diverse data into a tabular ML computable format blabla.

Example: Scikit-Longitudinal – https://github.com/simonprovost/scikit-longitudinal",Increasing the algorithms' space.,2,7,3,1,6,4,5,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Other",The ecosystem,"XGBoost, Other","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, Other",Yes,Feature importances,Yes,"Meta data such as perhaps in diverse ML applicatin (longitudinal data), temporal vector so that an estimator can be modified in leveraging such temporal vector.",Being modified to a specific use (niche usage that Sklearn does not have the scope for it),3,MLFlow,Kubeflow,less than 10 minutes,2,Strongly agree,Yes,s.g.provost@kent.ac.uk
9/30/2024 7:31:11,I have read this statement in its entirety and affirm the stated conditions.,6,8,5,7,4,2,3,1,"If the results are unreliable, I won't use the package. I think that improved performance is always good, but I don't think this is such a big problem today; having new features (at least compared to R) is something I do find could be meaningful. For example - the fact there isn't good principle curve implementation is a problem ",,2,7,3,6,5,4,1,"Feature importance, Residual plots",pandas,,"LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",No,,,1,,,less than 10 minutes,More than 5,Strongly agree,No,
9/30/2024 9:19:21,I have read this statement in its entirety and affirm the stated conditions.,7,7,7,7,8,8,4,5,,Better Ray integration,6,6,7,6,3,5,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve","Dask DataFrame, Modin, pandas, Spark DataFrame, Other",,"CatBoost, Jax, Keras, LightGBM, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",Yes,"Paths for files, metrics to visualize, model to save etc",Data to load from Disk rather than in-memory ( similar to Keras ),5,MLFlow,Airflow,"less than an hour, less than a day, more than a day",More than 5,Strongly agree,Yes,nilesh.patil@dream11.com
9/30/2024 16:48:04,I have read this statement in its entirety and affirm the stated conditions.,2,3,1,3,2,1,1,8,Extensibility is the single biggest thing I would like to have from SKL that it doesn't really do.,Responding to bugs and pull requests.,3,7,1,1,3,3,7,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","DuckDB, Other",Its unified (if awkward) interface for classifiers and other models.,Other,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,I've worked a lot in the past with learning directly from multi-relational data.,Learning directly from multi-relational DBs avoids the distortions inherent to transforming the data (via aggregations or joins) into a single-table (feature vector) form.,1,Custom tool,Custom tool,"less than 10 minutes, less than an hour, less than a day",1,Disagree,Yes,barnard@cs.wisc.edu
9/30/2024 17:23:51,I have read this statement in its entirety and affirm the stated conditions.,8,7,7,6,7,4,7,7,Very nice for every day,improve educational materials or provide more video guidence,7,7,6,6,5,7,,"Confusion matrix, ROC curve, Precision-Recall curve",pandas,help to build model more easier,"Keras, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,Cost-sensitive learning,No,,good,3,"MLFlow, Weight and biases, Custom tool",Other,"less than 10 seconds, less than an hour, less than a day",2,Strongly agree,Yes,danielnsanzabandi@gmail.com
10/1/2024 2:20:02,I have read this statement in its entirety and affirm the stated conditions.,5,6,,7,4,8,1,,,,6,7,5,2,4,3,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Polars, Spark DataFrame",Consistent modelling API and easy model evaluation ,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,3,,,less than an hour,More than 5,Strongly agree,No,
10/1/2024 4:32:57,I have read this statement in its entirety and affirm the stated conditions.,6,6,2,4,6,8,2,,,,7,1,7,6,5,4,,"Confusion matrix, Residual plots","Dask DataFrame, pandas",,,Other,Yes,Uncertainty estimates for prediction,No,,,3,,,less than an hour,,Strongly agree,No,
10/1/2024 11:40:29,I have read this statement in its entirety and affirm the stated conditions.,8,8,5,6,8,8,3,1,"Generally, I would like scikit-learn to keep up with the rapid improvement of machine learning science",,6,7,3,4,1,2,1,"Confusion matrix, ROC curve, Precision-Recall curve",pandas,User friendly and easy to use machine learning library with good performance,"Keras, XGBoost",Other,No,"Calibration of probabilistic classifiers, Calibration of regressors",No,,,4,,,less than an hour,2,Strongly agree,No,
10/1/2024 13:28:15,I have read this statement in its entirety and affirm the stated conditions.,5,3,1,7,7,5,1,1,,,7,6,3,2,3,1,1,"Confusion matrix, ROC curve, Precision-Recall curve","Dask DataFrame, pandas",,"LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Sample weights, Non-euclidean metrics",Yes,"Binary information such as whether an observation is censored. In particular, I would like to be able to fit models for censored regression.",I would be able to train e.g. a Tobit regressor.,1,MLFlow,,less than a minute,More than 5,Strongly agree,No,
10/2/2024 0:59:19,I have read this statement in its entirety and affirm the stated conditions.,8,8,5,6,5,8,4,,I already think its a great library for student like us who are learning ML with non CS background its fine till now,Graphing things might help me ig,4,7,2,7,5,5,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve",pandas,Its readability and simplicity,,Other,Yes,,No,,,3,,,less than a day,3,Strongly agree,No,
10/2/2024 3:37:53,I have read this statement in its entirety and affirm the stated conditions.,,,,,,,,,,,3,7,2,7,2,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","DuckDB, pandas",,"LightGBM, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,3,,Airflow,less than 10 minutes,4,Strongly agree,No,
10/2/2024 8:41:15,I have read this statement in its entirety and affirm the stated conditions.,1,1,1,8,5,4,1,1,"For my use cases, the performances and reliability of scikit learn are good, I would focus on adding more features (such as more support for multi task learning, more causal analysis methods, etc.) and documentation/educational material around that. It would be fantastic to have a single toolbox for all use cases on structured data ",Tools for performing causal analysis,4,7,5,3,2,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Spark DataFrame","Similar interface for a lot of estimators, and unified pipeline objects to orchestrate everything","Keras, LightGBM, PyTorch, Transformers, XGBoost, Other","RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,,,1,"MLFlow, Other",Other,"less than an hour, less than a day",More than 5,Strongly agree,Yes,satya.vsarma@gmail.com
10/2/2024 9:59:00,I have read this statement in its entirety and affirm the stated conditions.,7,7,5,3,5,5,1,1,,teach people about pipelines and their advantages as early as possible.,4,4,7,2,2,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","DuckDB, pandas, Polars",fit/predict was a game changer in the industry,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,2,"Neptune, Weight and biases, Custom tool, Other","Airflow, Argo, Dagster, Kubeflow, Custom tool, Other","less than 10 seconds, less than a minute, less than 10 minutes, less than an hour, less than a day, more than a day",More than 5,Strongly agree,No,
10/2/2024 10:52:35,I have read this statement in its entirety and affirm the stated conditions.,2,4,7,7,7,7,1,1,Knowing what already is in sklearn is the hardest part,Adding Lightgbm and/or neural net support,7,6,4,3,2,5,1,"Confusion matrix, ROC curve, Feature importance, Learning curves",pandas,One-stop shop for all kinds of different models,"LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,1,"MLFlow, Custom tool",Other,less than a minute,5,Strongly agree,No,
10/2/2024 11:10:47,I have read this statement in its entirety and affirm the stated conditions.,4,6,4,6,7,7,5,,,,6,6,5,5,4,6,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,the documentation,"Keras, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Feature importances, Sample weights, Non-euclidean metrics",No,,,2,MLFlow,,"less than 10 seconds, less than a minute",2,Agree,No,
10/2/2024 18:04:13,I have read this statement in its entirety and affirm the stated conditions.,4,6,6,4,8,8,4,1,Better documentation examples for those just getting started with a function are the most important improvement that could be made.,,3,7,6,2,6,6,1,"Confusion matrix, Feature importance",pandas,,"Keras, PyTorch",RandomForestClassifier or RandomForestRegressor,No,,No,,,1,,,less than 10 minutes,2,Neither agree nor disagree,No,nickodell@gmail.com
10/2/2024 21:27:42,I have read this statement in its entirety and affirm the stated conditions.,8,8,6,8,8,8,6,,Performance and Reliability is very important for my team. Documentation is bit messed up and unclear now.,Good examples and receipies in documentation,4,6,7,4,5,7,,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","cudf, pandas, Spark DataFrame",Less verbose syntax and sane defaults,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning",No,,,4,"MLFlow, Weight and biases","Airflow, Kubeflow",more than a day,5,Strongly agree,No,
10/3/2024 4:50:10,I have read this statement in its entirety and affirm the stated conditions.,5,8,5,8,5,4,2,1,,,5,7,5,6,6,4,1,"Confusion matrix, ROC curve, Precision-Recall curve","Dask DataFrame, pandas",Easy to implement estimators,"Keras, XGBoost, Other",Pipeline,Yes,"Cost-sensitive learning, Feature importances",No,,,3,,,less than a day,1,Agree,No,
10/3/2024 7:29:25,I have read this statement in its entirety and affirm the stated conditions.,5,8,5,6,7,7,5,,,,5,6,,2,4,3,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Polars",,"Jax, Keras, PyTorch",Pipeline,No,Feature importances,No,,,3,MLFlow,Other,,,Strongly agree,No,
10/3/2024 7:30:45,I have read this statement in its entirety and affirm the stated conditions.,8,7,4,4,5,3,2,7,Polars support and increased performance is number 1 for me,Polars support,7,5,7,5,6,5,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","DuckDB, Polars",,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","Pipeline, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Non-euclidean metrics",Yes,,,3,"MLFlow, Weight and biases","Argo, Custom tool",less than an hour,More than 5,Strongly agree,No,
10/3/2024 8:36:35,I have read this statement in its entirety and affirm the stated conditions.,8,2,8,8,8,8,2,,,"I'd like to see experimental namespaces for new algorithms, with lower requirements for being accepted, so that recent clustering algorithms have greater visibility.",1,1,1,5,1,7,,"Confusion matrix, ROC curve, Feature importance, Learning curves","pandas, Polars",The infrastructure around composing tools together.,"PyTorch, XGBoost",Pipeline,No,Non-euclidean metrics,No,,,5,Weight and biases,,less than 10 minutes,1,Strongly agree,Yes,ben@jourdan.me
10/3/2024 10:38:15,I have read this statement in its entirety and affirm the stated conditions.,6,7,7,5,6,5,5,4,sklearn is solid an reliable now; it should be kept that way,perhaps gpu support but I need this for DNN models which are well covered by other packages,6,6,5,4,4,4,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","Dask DataFrame, pandas",a fully integrated modeling and analysis pipeline,"Keras, PyTorch, Transformers",Pipeline,No,"Uncertainty estimates for prediction, Feature importances",Yes,"parameters to fine tune the loss I use (e.g., sample importance weights)",Make it more flexible,4,MLFlow,,more than a day,2,Strongly agree,No,
10/3/2024 13:40:22,I have read this statement in its entirety and affirm the stated conditions.,6,8,5,5,7,8,2,2,,,6,6,4,7,4,6,5,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Other",,"CatBoost, LightGBM, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,1,Custom tool,,"less than 10 minutes, less than an hour, less than a day",More than 5,Strongly agree,No,
10/3/2024 14:02:33,I have read this statement in its entirety and affirm the stated conditions.,7,3,3,5,2,2,1,1,GPU acceleration and performance for inference would be lovely,Ability to perform inference more quickly.,2,6,2,6,6,3,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","cudf, pandas","The documentation, the ease of use","CatBoost, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,5,MLFlow,Custom tool,less than a day,More than 5,Strongly agree,Yes,ganatradarpan@gmail.com
10/3/2024 14:59:54,I have read this statement in its entirety and affirm the stated conditions.,7,6,5,6,3,3,2,6,Other: further work on model and pipeline persistence would be good,pipelines being smarter about accepting / filtering the provided data columns they need,3,7,4,6,5,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Polars",pipelines,"LightGBM, XGBoost","LogisticRegression, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing",Yes,,,3,"MLFlow, Custom tool",Other,less than a day,More than 5,Agree,No,
10/4/2024 5:29:31,I have read this statement in its entirety and affirm the stated conditions.,8,1,7,8,4,7,7,1,"I would like to see improvements in performance, potentially through the use of GPUs or SMID instruction sets, as well as specific parallel processing strategies. Additionally, I'd suggest enhancing the hyperparameter tuning section.

Given the opportunity, I would also request the addition of a few more transformers, such as a Target Transform Classifier and a Train-Only Transformer. The Train-Only Transformer would be a distinct class of transformers that handle tasks like imbalanced data or data cleaning, executed solely during the model training phase. The ultimate goal is to integrate both transformers and estimators within the same pipeline instance. 

Plus a Categorical-Encoder for multi target Regression task .

Currently, most Scikit-learn models do not support incremental or online training. However, this feature is available in the River library. I would like to see similar capabilities incorporated into Scikit-learn.

For model explainability, we frequently rely on SHAP, and SHAP’s approach is excellent. Ideally, every model should come with SHAP explainability as a built-in feature Like CatBoost, XGBoost.

Regarding model saving, there are several standard formats, such as SKLEARN-ONNX. However, a problem I've encountered is that after exporting a model to a standard format like ONNX, SHAP and LIME explainability tools no longer function. I'm unsure what the solution to this issue is.","Model Shap Explainability should With Bosted Performance.
",6,7,1,,5,7,1,"Confusion matrix, ROC curve, Feature importance","pandas, Polars",Uniform API across all the ML task,"CatBoost, LightGBM, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Feature importances",No,,,5,MLFlow,Custom tool,less than 10 minutes,3,Strongly agree,Yes,r.das699@gmail.com
10/4/2024 7:11:06,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,5,,,7,7,6,6,7,6,4,"Confusion matrix, ROC curve, Precision-Recall curve","pandas, Polars",,"Keras, PyTorch","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,5,Custom tool,Custom tool,"less than 10 minutes, more than a day",1,Strongly agree,No,
10/4/2024 8:29:00,I have read this statement in its entirety and affirm the stated conditions.,6,6,6,7,7,8,1,1,,torch integration,5,7,6,7,5,5,4,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","pandas, Polars",,"PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,Cost-sensitive learning,,,,3,Weight and biases,,less than a day,,,,
10/4/2024 8:37:02,I have read this statement in its entirety and affirm the stated conditions.,4,1,4,8,1,6,7,3,"add more DL and real-world data tutorials and guides, not only mockup prebuilt datasets, sampling techniques (SMOTE etc)",more categorical features encoding and imbalanced learning options,2,5,4,6,5,7,,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves",pandas,consistent api,"CatBoost, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of probabilistic classifiers, Feature importances, Sample weights",No,,,4,,,less than a day,More than 5,Strongly agree,Yes,juanlux.rg@gmail.com
10/4/2024 9:59:11,I have read this statement in its entirety and affirm the stated conditions.,3,3,3,3,5,8,2,1,"I want to classify fragments from signals (i.e. classify the signal shape).
Each signal might consist of 200 points of time-based data (think e.g. audio or similar).  This is currently not possible with scikit-learn, because if I take each of the 200 points as feature, then the order of features is not important. 
In other words, I loose the shape of the signal. 
see also:
https://github.com/scikit-learn/scikit-learn/issues/11811#issuecomment-413047547

Put another way: What I want is TIME SERIES CLASSIFICATION, or detailed documentation/tutorial hints on how to do that (classify signal shapes, where a signal consists of 200 points)","How to classify signal shapes. Where a signal consists of say 200 points. aka Time Series Classification, or detailed documentation/tutorial hints on how to do that.",2,7,2,4,2,2,2,Other,Other,simple,,RandomForestClassifier or RandomForestRegressor,No,,No,,,1,,,less than an hour,2,Strongly agree,,
10/4/2024 10:08:19,I have read this statement in its entirety and affirm the stated conditions.,4,1,1,1,1,1,1,1,"Regarding speed: if I want speed, I will NOT USE PYTHON. For speed, I need a compiled language (e.g. C++ or rust) ! 

If you still want to stay with python, then PLEASE DO NOT WASTE TIME ON GPU ... . Because remember: you want speed... you need it compiled: C++, rust or similar.

So if you really want speed, then get people to write C++ code, rather than trying to fool yourself into believing that you can speed up feaking-slow python enough with GPU... You cannot:
https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/python3-gpp.html

Again: you want speed, you need a compiled language. No way to get around that (unless you like fooling youself with wishful thinking...)",compatible C++ training and classification code (for speed),,,,,,,,,,,,,,,,,,3,,,,,,,
10/4/2024 11:08:15,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,3,6,1,2,1,,Polars full compatibility ,7,7,6,5,6,6,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","DuckDB, pandas, Polars",ease of use and nice documentation,"CatBoost, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of regressors, Feature importances, Sample weights",No,,,3,MLFlow,,more than a day,3,Strongly agree,No,
10/4/2024 11:43:11,I have read this statement in its entirety and affirm the stated conditions.,8,1,1,8,3,8,1,1,,If ti would start to have GPU support,,,7,,7,7,,,pandas,,"CatBoost, PyTorch, XGBoost",RandomForestClassifier or RandomForestRegressor,No,,,,,,,,less than 10 minutes,1,Strongly agree,No,
10/4/2024 13:26:42,I have read this statement in its entirety and affirm the stated conditions.,6,4,2,2,1,1,1,,,Support for Polars dataframes,3,7,5,4,6,3,,"Precision-Recall curve, Feature importance, Learning curves","pandas, Polars, Spark DataFrame","The uniform and extensible API makes it easy to create objects that ""play well"" with SKLearn features.","XGBoost, Other","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Feature importances, Sample weights, Metadata routing",Yes,"It would be nice if XGBoost could be at the end of a pipeline where the pipeline somehow created a train/test split and the split data was able to populate the eval_set parameter, appropriately.",Single configurable pipeline means easier HPO/parameter tuning and vectorization over different train/test/validation splits. One pipeline to rule them all.,2,MLFlow,,less than a day,More than 5,Agree,No,
10/4/2024 16:17:00,I have read this statement in its entirety and affirm the stated conditions.,8,4,4,4,7,8,5,1,"Performance is critical e.g., big data; improvements on pipelines, compatibility with other pipeline libraries",support for more big data libraries,5,7,3,6,4,7,1,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","pandas, Polars",Easy access to internal variables and their docs,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,metadata related to X,This would help to reduce memory footprint of converting everything to shape of X (if it is a tensor/array),5,"MLFlow, Custom tool",Custom tool,"less than 10 minutes, less than a day, more than a day",More than 5,Strongly agree,No,Thank you.
10/4/2024 17:21:48,I have read this statement in its entirety and affirm the stated conditions.,5,8,2,8,6,3,1,1,"Sklearn is the most basic of all ML libraries, so it should be as reliable as possible. But it also doen't have all the methods of classical ML yet so adding them should be a priority. The documentation should also be kept in check, it is good now, but it should stay good. Also speeding something up is always a good thing. Everything else is not so important",New models.,4,3,2,5,7,6,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,,"CatBoost, PyTorch, Transformers","LogisticRegression, Pipeline, Other",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Sample weights, Non-euclidean metrics",No,,,3,,,,,Strongly agree,No,
10/5/2024 11:25:36,I have read this statement in its entirety and affirm the stated conditions.,5,8,6,2,7,4,1,,"Because scikit-learn often serves as a source of baseline methods, I would prioritise maintaining reliability for any future improvements.",Template or code generator for constructing pipelines,6,4,7,2,5,3,,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","pandas, Polars",Common API and conventions for a wide variety of methods,PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Uncertainty estimates for prediction, Feature importances, Metadata routing, Non-euclidean metrics",No,,,2,,,less than an hour,More than 5,Strongly agree,No,
10/6/2024 4:40:32,I have read this statement in its entirety and affirm the stated conditions.,7,8,,8,7,7,1,1,,,7,7,7,7,7,5,,"Confusion matrix, ROC curve, Feature importance, Residual plots, Learning curves",pandas,the unified interface combined with such a wide range of models and other functions,"PyTorch, Transformers, XGBoost, Other",Other,No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,Hyperparameters,Increase performance,2,MLFlow,,less than 10 minutes,,Strongly agree,,
10/6/2024 5:10:08,I have read this statement in its entirety and affirm the stated conditions.,1,8,1,2,8,5,1,8,"Definitely should prioritize closing issues, implementing bugfixes and improving reliability of existing features.","Bugfixes to multioutput functionality, which in my opinion is quite neglected",6,7,1,1,1,1,5,"Confusion matrix, Feature importance, Other",pandas,Ease of usage and API,"LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Cost-sensitive learning, Feature importances",No,,,1,,,less than 10 minutes,More than 5,Strongly agree,Yes,jakubadamczyk10@gmail.com
10/6/2024 8:57:58,I have read this statement in its entirety and affirm the stated conditions.,3,5,1,4,3,2,1,1,I would like to see structured output of model's metrics just like in other software.,structured output of model's metrics just like in other statistical software,6,6,6,6,5,5,1,"Confusion matrix, Reliability diagram, ROC curve, Residual plots, Learning curves","Dask DataFrame, pandas, Polars","intuitive API, performance","Jax, Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",No,,,1,"MLFlow, Weight and biases",Custom tool,"less than 10 minutes, more than a day",2,Strongly agree,No,
10/6/2024 11:58:28,I have read this statement in its entirety and affirm the stated conditions.,8,,,7,,,,,,,,7,7,,,,,,pandas,,XGBoost,"RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,3,,,less than a day,1,Strongly agree,No,
10/7/2024 6:07:37,I have read this statement in its entirety and affirm the stated conditions.,5,8,8,8,8,8,3,1,,,7,7,2,5,5,6,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Spark DataFrame",,"CatBoost, Jax, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Cost-sensitive learning, Sample weights",Yes,,,3,,"Argo, Kubeflow",less than a day,More than 5,Neither agree nor disagree,No,
10/7/2024 6:07:38,I have read this statement in its entirety and affirm the stated conditions.,6,6,6,7,8,8,6,6,,Model versioning or something like that? ,7,5,7,2,2,5,1,"Feature importance, Other",pandas,Amazing api design. ,"CatBoost, XGBoost",RandomForestClassifier or RandomForestRegressor,No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,2,"MLFlow, Custom tool",Custom tool,"less than 10 minutes, less than an hour",3,Strongly agree,No,
10/7/2024 6:25:10,I have read this statement in its entirety and affirm the stated conditions.,4,5,3,6,7,8,2,1,"It's an excellent package, but it would be great to make it even easier for beginners to use",,7,6,4,5,2,3,1,"Confusion matrix, Precision-Recall curve, Feature importance","pandas, Spark DataFrame",,XGBoost,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,2,,,less than a minute,3,Strongly agree,No,
10/7/2024 6:45:21,I have read this statement in its entirety and affirm the stated conditions.,8,8,5,8,4,1,1,1,"Performance is good, but it could be better, new features such as those contained in sklego and skrub would be great, as well as a pickle alternative for serialization.","A pickle alternative for serialization. ONNX is pretty good but poorly maintained for sklearn estimativa, so a specific one would be nice.",7,7,5,5,6,3,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots","Polars, Spark DataFrame",The consistency of the API is amazing and has set a hard to surpass standard.,"CatBoost, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",Yes,,,4,MLFlow,Airflow,"less than 10 minutes, less than an hour",More than 5,Strongly agree,Yes,walter@sperat.com.ar
10/7/2024 7:07:30,I have read this statement in its entirety and affirm the stated conditions.,8,6,1,6,2,2,1,,,,3,7,1,7,5,5,4,"Confusion matrix, Feature importance, Learning curves","pandas, Polars, Spark DataFrame",,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,4,,,less than 10 minutes,3,Agree,No,
10/7/2024 7:49:13,I have read this statement in its entirety and affirm the stated conditions.,5,5,6,7,7,6,1,5,"Scikit-learn is quite mature already. It has a wide range of proprocessors, estimators, and evaluation routines. It also provides a simple and effective interface through pipelines, and a versatile approach to inplement custom estimators. So I believe that what would benefit the project more is 1) the implementation of relevant estimators that are not straightforward to implement just extending base estimators, and that have no equivalent replacement (e.g., ordinal regression), 2) ""quality of life"" features, along the lines pf.the pipeline display feature, which is very useful, 3) further documentation of non-trivial use cases for existing features.",,7,6,3,4,7,5,4,"Confusion matrix, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,The pipeline API and the custom estimator interface.,Keras,"LogisticRegression, Pipeline, ColumnTransformer, Other",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,4,Custom tool,,less than an hour,1,Strongly agree,No,
10/7/2024 8:42:06,I have read this statement in its entirety and affirm the stated conditions.,3,3,2,3,3,2,2,1,Speed of execution ,NA,3,3,2,2,3,3,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Polars, Spark DataFrame",Ease of use,"CatBoost, Keras, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,NA,NA,4,MLFlow,"Airflow, Kubeflow","less than 10 minutes, less than an hour",3,Neither agree nor disagree,No,avishek84@yahoo.co.in
10/7/2024 9:05:52,I have read this statement in its entirety and affirm the stated conditions.,6,8,4,6,7,7,5,2,,,5,7,2,3,7,4,2,"Confusion matrix, ROC curve, Feature importance",pandas,The ability to create pipelines with lots of different elements and being able to change them easyly.,,"LogisticRegression, Pipeline, ColumnTransformer",Yes,"Feature importances, Sample weights, Non-euclidean metrics",No,,,3,,,less than a minute,1,Strongly agree,No,
10/7/2024 9:08:52,I have read this statement in its entirety and affirm the stated conditions.,5,4,4,8,4,8,1,1,Probabl and Vincent are on the right track - educating users of the cool and less popular features of scikit-learn,More content from Vincent. Making pipelines more popular. Pipelines are part of the data eng and also mlops process and making models with pipelines seems like an underrated and underused feature. ,7,6,5,7,5,4,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves","pandas, Spark DataFrame",Its pipeline api,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost",LogisticRegression,No,"Feature importances, Non-euclidean metrics",No,,,1,"MLFlow, Other","Airflow, Other",less than 10 minutes,1,Strongly agree,Yes,timee98@outlook.com
10/7/2024 9:45:03,I have read this statement in its entirety and affirm the stated conditions.,6,8,5,7,6,5,2,,,,7,7,2,3,6,4,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,Its simplicity and consistent API ,"CatBoost, LightGBM, PyTorch","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,4,MLFlow,Airflow,more than a day,More than 5,Strongly agree,Yes,agamemnon.krasoulis@gmail.com
10/7/2024 11:08:20,I have read this statement in its entirety and affirm the stated conditions.,6,8,5,7,8,6,1,,,,7,6,4,3,6,7,4,"ROC curve, Precision-Recall curve, Residual plots, Learning curves",Polars,Ease of use,"PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,4,Weight and biases,,more than a day,More than 5,Strongly agree,No,
10/7/2024 13:05:42,I have read this statement in its entirety and affirm the stated conditions.,3,3,3,2,3,3,1,,I love scikit learn and have learned from scikit learn team.,,3,3,3,3,3,3,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,Its optimized and efficient code.,"Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing, Non-euclidean metrics",No,,By guiding us through code within scikit learn,5,MLFlow,Other,less than a day,2,Strongly agree,Yes,saurabhmishra1806@gmail.com
10/7/2024 14:17:21,I have read this statement in its entirety and affirm the stated conditions.,8,4,4,7,5,4,2,6,,,6,5,7,4,2,3,1,"Confusion matrix, Reliability diagram, Feature importance, Learning curves","cudf, Modin, pandas, Polars, Spark DataFrame",Its pretty straightforward to use. Performance may be improve,"CatBoost, LightGBM, Transformers, XGBoost, Other","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",Yes,Separating exogen data for time series forecasting would be nice,Separating exog/endog data may improve how lags are created,2,"MLFlow, Weight and biases, Custom tool, Other","Airflow, Kubeflow, Custom tool, Other",less than a day,4,Strongly agree,Yes,camilo.camargo@guane.com.co
10/7/2024 14:20:21,I have read this statement in its entirety and affirm the stated conditions.,3,3,3,3,3,2,1,,,,2,3,2,3,3,2,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","Dask DataFrame, pandas, Polars",,XGBoost,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,,,4,,"Airflow, Kubeflow",less than a day,More than 5,Strongly agree,,
10/8/2024 1:48:00,I have read this statement in its entirety and affirm the stated conditions.,6,7,,7,8,8,3,,"I find it difficult to find examples—I typically have to google them to end up at the right place. Not all example notebooks are self-contained. It is difficult for me to send a reference to colleague, though I admit the sklearn site is for documentation, and not an ML book (I still use it as one, sometimes!).",Better serialization. For example: afaik the isotonic regression doesn’t serialize (but it can be done with workarounds).,4,6,5,5,6,7,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Residual plots, Learning curves","cudf, Dask DataFrame, pandas, Polars",I love the pipeline feature. I wish more people used it.,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Non-euclidean metrics",Yes,"yes, but only in the context of writing custom estimators. I believe often the issue was that I was writing out-of-scope code in the function definition. I think I mostly appreciate that the rails sklearn imposes forces me to write ML code “correctly.”",Easier caching of steps. Easier logging and callbacks.,3,"MLFlow, Custom tool",Custom tool,"less than 10 minutes, less than an hour",More than 5,Strongly agree,Yes,gleb.zh@gmail.com
10/8/2024 9:35:55,I have read this statement in its entirety and affirm the stated conditions.,7,7,5,4,7,3,2,2,Performance and Reliability of the existing functions is of main priority for my work,More / more versatile examples in the documentation,4,7,5,6,7,3,2,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves","pandas, Other",The continuity that all functions within a group or topic have a similar structure; The User Guides are almost always helpful; The possibility of using of smoothly using multiple classes/ functions within each other,"Keras, PyTorch, XGBoost",Other,Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,1,MLFlow,,less than 10 minutes,2,Strongly agree,No,
10/8/2024 10:59:32,I have read this statement in its entirety and affirm the stated conditions.,6,8,4,4,8,8,4,4,"Reliability has to be at the front, with technical documentation and ed materials right behind. Technical documentation helps me ensure that I'm using the objects correctly, and educational materials help me explain to my clients what exactly is going on under the hood.",Easier collection of learning curves - especially from ,4,3,2,7,6,5,1,"Confusion matrix, ROC curve, Feature importance, Learning curves",pandas,,"Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Cost-sensitive learning, Feature importances, Sample weights",No,,,4,,,less than an hour,3,Strongly agree,No,
10/8/2024 12:09:11,I have read this statement in its entirety and affirm the stated conditions.,8,7,5,6,4,2,3,1,,fully pandas support,3,2,7,6,4,5,1,"Confusion matrix, ROC curve, Feature importance, Residual plots","cudf, pandas",,"CatBoost, Keras, LightGBM, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,3,,,less than 10 minutes,2,Strongly agree,No,
10/8/2024 13:23:25,I have read this statement in its entirety and affirm the stated conditions.,6,7,5,5,6,5,2,1,"Performance and reliability of what's already there is clearly crucial and some things (e.g. kNN) perform badly (faiss based kNN beats it by miles speed wise). There's also more features that would be nice, some basic functionality is unnecessarily missing (e.g. restricted cubic splines).",Nothing stands out,7,6,6,4,2,3,6,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","cudf, pandas","Unified interface, ability to combine pipeline with model or combine models","CatBoost, LightGBM, PyTorch, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,initial guess per record (e.g. predict clinical outcome and there's an existing risk score),Incorporate existing knowledge and then model needs to figure out how records deviate from this,3,"Weight and biases, Custom tool",,"less than an hour, less than a day",3,Agree,No,
10/8/2024 17:05:02,I have read this statement in its entirety and affirm the stated conditions.,8,,,8,,,,,"using hardware acceleration aka parallel processing by default (i already search for scikit replacement  due poor, hard to use hyper parameter estimators, u have to code it for your self, there is a reason why dos is a past and poor hardware acceleration, everything relays on cpu, i wonder how u plan to utilize ARM cpu's and hardware acceleration) ",easy to use hyper parameter estimator wrapped in GUI not to loosing mind over it,5,5,7,5,5,5,7,Other,Other,nothing,Other,Other,No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Sample weights",No,,,5,,,less than a day,More than 5,Strongly agree,No,
10/8/2024 18:44:34,I have read this statement in its entirety and affirm the stated conditions.,5,8,6,8,7,7,2,2,,,5,7,4,7,5,7,1,"Confusion matrix, ROC curve, Feature importance, Residual plots","pandas, Polars",,"CatBoost, Transformers, XGBoost",,No,"Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,5,"MLFlow, Weight and biases",,less than a day,2,Strongly agree,,
10/8/2024 18:49:13,I have read this statement in its entirety and affirm the stated conditions.,7,6,4,8,5,6,2,,"It would be really nice if sklearn can incorporate some newer, but popular, methods, faster, like UMAP. 2) Having correct type stubs is very important, as it allows mypy to type check code for correctness",Having type stubs so that code using sklearn can be checked for correctness,6,7,4,2,5,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","pandas, Polars","1) simplicity, consistency, and stability of API, 2) staying focused on well-established models/techniques instead of trying to implement everything, and 3) decent runtime performance","PyTorch, Transformers",LogisticRegression,No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Sample weights, Non-euclidean metrics",No,,,2,,,"less than a minute, less than 10 minutes, less than an hour, less than a day, more than a day",5,Strongly agree,,
10/8/2024 21:29:30,I have read this statement in its entirety and affirm the stated conditions.,7,6,4,8,8,6,5,,"I think machine learning has developed a lot, and there are a lot of modern new features that are missing in scikit-learn. Moreover, instead of supporting older features, it might be useful to move outdated models/tools to another repository to reduce maintenance.",I am interested in extending scikit-learn usually. I think additional tools that allow me to extend the very well-optimized designs in Cython (or C++) of scikit-learn would be very helpful.,6,7,2,4,2,1,7,"Confusion matrix, ROC curve, Feature importance","pandas, Polars, Spark DataFrame","The RandomForest implementation, and the ease of using metrics and inspection tooling.",PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,"In causal inference, we commonly want to pass in treatments, instrumental variables, and additional variables that provide meta information about X and Y.",These additional information would be used in the model training procedure to optimize some alternative loss/cost function.,3,,,"less than an hour, less than a day",,Strongly agree,No,
10/9/2024 2:31:03,I have read this statement in its entirety and affirm the stated conditions.,6,6,4,3,7,8,3,1,"It's rock solid go to, but more details comparing methods would be great.","two long standing bug reports, nans in Hyperparameter tuning, and pandas with a range index as columns. But I've learned to live with them / use my patches. ",4,7,,6,5,3,,"Confusion matrix, ROC curve, Residual plots, Learning curves","pandas, Polars",,"CatBoost, Keras, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Non-euclidean metrics",No,,,3,"MLFlow, Custom tool",,less than 10 seconds,More than 5,Strongly agree,Yes,rum@bankingcircle.com
10/9/2024 3:41:29,I have read this statement in its entirety and affirm the stated conditions.,5,5,3,6,5,6,3,1,,,6,6,5,4,4,5,4,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,Building Pipelines,"Keras, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",No,,,4,Custom tool,Custom tool,"less than an hour, less than a day",1,Agree,No,
10/9/2024 5:22:59,I have read this statement in its entirety and affirm the stated conditions.,7,1,1,7,3,3,1,1,"Everything works super well, so I guess the only thing that can be improved is more models and better performance",Performance if I have to choose one thing,7,3,6,3,6,2,1,"Feature importance, Residual plots",Polars,Takes 1 sec to test a new model,Other,"RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of regressors, Feature importances, Sample weights",No,,,1,,Argo,"less than a minute, less than 10 minutes",More than 5,Strongly agree,No,
10/9/2024 5:44:31,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,5,4,7,5,,,,7,6,,6,5,4,,"Confusion matrix, ROC curve","DuckDB, pandas",,PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, ColumnTransformer",Yes,Cost-sensitive learning,,,,5,"MLFlow, Weight and biases",,less than an hour,More than 5,Strongly agree,Yes,dhruvadeepmalakar12345@gmail.com
10/9/2024 7:49:33,I have read this statement in its entirety and affirm the stated conditions.,8,4,3,6,5,7,2,1,"The User Guide is quite thorough, and the documentation is pretty extensive and full of examples. I feel like performance would be an interesting first priority since all the rest is so good already -- I've never had issues with packaging for instance.",Easier interoperability with DataFrame libraries,4,6,5,2,7,3,1,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves","cudf, Dask DataFrame, pandas, Polars",Pipelines,"Jax, Keras, PyTorch, Transformers","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Cost-sensitive learning, Feature importances",No,,,2,"DVC, MLFlow, Other",Airflow,less than 10 minutes,,Strongly agree,No,
10/9/2024 8:41:03,I have read this statement in its entirety and affirm the stated conditions.,7,8,1,6,6,3,1,1,"Of course the first thing I need as a researcher in medicine and data science, is reliability and knowing what exactly the functions are doing. and performance is my other struggle. I don't know how much improvement is possible, the more the better.
I found some features like IterativeImputer and SMOTE missing stuff I need. I added them on my github https://github.com/arashbagherian/My-public-scripts/tree/4582780bcbca181b1c75090cdf14be73730516f4/functional_iterative_imputer",New features,6,7,1,3,5,4,2,"ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,Modularity and ease of creating custom estimators and transformers. Also I was able to tweak the source code for some of my needs.,"Keras, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",Yes,Like the name or index of the categorical variables. I constantly need to make custom stuff to make that happen.,"For IterativeImputer use in mixed-type dataset, I want to estimate the categoricals with logisticregression and the numericals with linearregression and I have to build some weird workarounds. Also being able to pass arbitrary kwargs would make building custom functions much smoother.",3,Custom tool,Custom tool,less than a minute,1,Strongly agree,Yes,arashbagherian1380@gmail.com
10/9/2024 10:53:54,I have read this statement in its entirety and affirm the stated conditions.,2,2,2,7,2,2,2,2,Everything apart from implementing new features is already so good I wouldn't prioritize it.,Implementing more clustering algorithms that allow to use a precomputed distance.,2,7,2,4,6,7,5,"ROC curve, Feature importance",pandas,The documentation and that it's easy to use.,"Keras, XGBoost",RandomForestClassifier or RandomForestRegressor,No,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances, Non-euclidean metrics",No,,,1,,,less than an hour,More than 5,Strongly agree,No,
10/9/2024 11:42:48,I have read this statement in its entirety and affirm the stated conditions.,5,6,7,8,8,8,6,5,,,6,7,6,7,5,6,5,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Other",pandas,,"Keras, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Other",No,Feature importances,Yes,,,3,,,less than a minute,1,Agree,No,
10/9/2024 16:02:38,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,8,Bring Deep learning frameworks,multiple accuracy metric comparison during Gridsearch. Also bring Bayesian hyperparameter tuning,7,7,7,5,5,5,5,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,easy to implement and simplicity,"Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,5,Custom tool,Custom tool,less than 10 minutes,More than 5,Strongly agree,No,
10/10/2024 6:46:23,I have read this statement in its entirety and affirm the stated conditions.,6,7,3,7,5,6,1,,,Algorithms to deal with Categorical Data and Missing Values,7,7,5,5,7,7,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,,"Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Uncertainty estimates for prediction, Feature importances",No,,,1,DVC,,less than an hour,,Strongly agree,No,julien.siebert@iese.fraunhofer.de
10/10/2024 9:25:20,I have read this statement in its entirety and affirm the stated conditions.,4,6,4,4,8,8,2,1,,"More educational materials, including in-depth tutorials and use-cases for specific ML algorithms that help bridge ML theory and hands-on practice. ",6,7,2,4,7,7,1,"Confusion matrix, ROC curve, Feature importance",pandas,"Its accessible website and the User Guide section which almost reads like a textbook and (in most cases) takes its time to explain things clearly, thoroughly and ""calmly""","PyTorch, Transformers","LogisticRegression, Pipeline, ColumnTransformer",No,"Feature importances, Sample weights",No,,,4,DVC,,less than 10 minutes,,Strongly agree,No,
10/10/2024 12:54:27,I have read this statement in its entirety and affirm the stated conditions.,6,,,6,,,,,"Performance
I'm no expert, but it feels like there is some room for caching in various places that would speed things up. For instance if I use SelectKBest in a cross validation search, the results will be different for the 5 folds, but the same for the different model hyperparameters, but it seems to recalculate the k best for each fit. 

New Features
It's always exciting when new things are added to the library, such a cool suite of tools! Some things that could be cool would be hyperparameter sensitivity analysis for SML, or even time series though I know that would be a lot. Right now I have to use other libraries for these things and their documentation is always crap compared to sklearn.",,6,7,5,2,4,3,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,The documentation is extremely readable and everything is super easy to use,"Keras, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",,"Uncertainty estimates for prediction, Feature importances",No,,,3,,,"less than 10 minutes, less than an hour",,Strongly agree,No,
10/10/2024 13:12:58,I have read this statement in its entirety and affirm the stated conditions.,4,8,6,8,6,8,1,1,"I am a student and new to ML. I think your website is great, and benefit greatly from your educational and instructive articles. More educational material is a huge plus, as well as the continued addition of new features.",More educational materials.,6,7,3,5,4,7,3,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,Its flexibility. The ability to load a wide variety of models from the same library and implement them with the same or very similar interfaces.,PyTorch,"LogisticRegression, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction",No,,,2,,,less than 10 seconds,,Strongly agree,No,
10/10/2024 14:52:26,I have read this statement in its entirety and affirm the stated conditions.,2,3,4,5,6,7,4,1,SKlearn has been reliable for years. The docs are better than most but still could use some work. ,Clarity in error messages,7,6,1,5,4,3,2,"Confusion matrix, Other",pandas,fairly user,,LogisticRegression,No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,2,,,less than 10 seconds,2,Strongly agree,Yes,andrew@components.one
10/10/2024 15:17:23,I have read this statement in its entirety and affirm the stated conditions.,1,2,4,4,8,8,2,1,,clearer and simpler use case examples,7,7,7,7,1,4,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve",pandas,all in one package,"XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Cost-sensitive learning, Sample weights",No,,,3,MLFlow,,"less than a minute, less than 10 minutes, less than an hour, less than a day",4,Strongly agree,No,
10/10/2024 15:31:55,I have read this statement in its entirety and affirm the stated conditions.,6,5,7,8,8,8,2,2,,,5,7,3,7,3,7,2,"ROC curve, Residual plots, Learning curves, Other","cudf, pandas, Polars",,"Keras, PyTorch, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer, Other",No,"Calibration of probabilistic classifiers, Calibration of regressors",No,,,5,,,"less than a minute, less than 10 minutes",2,Strongly agree,Yes,rafael.suzuki2@b6tp.com
10/10/2024 16:37:10,I have read this statement in its entirety and affirm the stated conditions.,,,,,8,8,,,more examples would be helpful particularly for functions with many features. It is hard to know the features do,add type annotation,4,7,,,6,5,,"Confusion matrix, ROC curve, Feature importance, Learning curves","pandas, Spark DataFrame",,"Keras, PyTorch, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,Feature importances,,,,1,,,less than a day,1,Strongly agree,No,aedavids@ucsc.edu
10/10/2024 18:23:51,I have read this statement in its entirety and affirm the stated conditions.,8,4,3,5,2,3,2,,"I can't recall our team ever needing a missing feature, but we sometimes run into performance issues on larger datasets.",,6,5,2,2,3,3,,"Confusion matrix, ROC curve, Feature importance, Learning curves",pandas,Number of implemented models and helpers means we rarely have to use other libraries.,"Keras, Other","LogisticRegression, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights, Metadata routing",No,,,2,MLFlow,Other,less than 10 minutes,More than 5,Strongly agree,No,
10/11/2024 2:43:49,I have read this statement in its entirety and affirm the stated conditions.,2,2,2,3,3,3,1,1,,,3,3,3,2,1,1,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,,"CatBoost, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,3,MLFlow,Airflow,less than 10 minutes,2,Strongly agree,No,
10/11/2024 4:19:15,I have read this statement in its entirety and affirm the stated conditions.,8,4,4,6,7,6,2,,,,6,7,6,4,4,4,,"Confusion matrix, Feature importance, Learning curves",pandas,,"Keras, PyTorch, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",,,,5,"Weight and biases, Custom tool",Other,more than a day,2,Strongly agree,No,
10/11/2024 6:26:09,I have read this statement in its entirety and affirm the stated conditions.,8,8,2,8,4,3,8,7,it difficult to read ,,1,1,,,1,1,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,,"CatBoost, Keras, LightGBM, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances",,,,,,,,,,,
10/11/2024 8:09:45,I have read this statement in its entirety and affirm the stated conditions.,5,3,2,7,8,8,8,1,Improve the docs by adding visuals and make the website better,Improve the docs,7,7,1,2,2,4,1,"Confusion matrix, Reliability diagram, Precision-Recall curve, Residual plots, Learning curves",pandas,,PyTorch,"LogisticRegression, ColumnTransformer",No,"Calibration of regressors, Cost-sensitive learning, Feature importances, Sample weights, Metadata routing",Yes,,,4,Weight and biases,,"less than 10 seconds, less than an hour, more than a day",2,Strongly agree,No,
10/11/2024 9:13:56,I have read this statement in its entirety and affirm the stated conditions.,8,3,3,,2,6,1,1,"Performance - consider automatic support of categorical feature, estimators supporting GPU ",GPU support for estimators,7,7,1,1,7,5,1,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves",pandas,"wide range of estimators with consistent classes use, ensemble estimators","Keras, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Feature importances",No,,,4,Other,Other,"less than an hour, less than a day",3,Agree,No,
10/11/2024 9:24:41,I have read this statement in its entirety and affirm the stated conditions.,6,6,5,7,7,5,1,1,,,7,5,6,6,4,4,3,"Confusion matrix, Residual plots, Learning curves, Other",pandas,,"CatBoost, Keras","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",Yes,"Calibration of regressors, Uncertainty estimates for prediction",Yes,"search window for weights, how quickly to go to minimum (to avoid being stuck in local minimum),...",more control,2,,,less than a minute,More than 5,Strongly agree,,
10/11/2024 11:46:49,I have read this statement in its entirety and affirm the stated conditions.,8,7,3,6,5,4,2,,"Probably a large job, but my desire would be to focus on increasing performance through acceleration, and maybe GPU support (were applicable). For example, Intels intelex extension, but built directly under sklearn. I run gridsearches quite often as standard ML models like SVMs are still a very common benchmarking algorithm in my field. The data has a large dimensionality and often takes hours. For me it is important because my research attempts to advance simple methodologies for non-practioners with commodity hardware. ","A link to documentation embedded directly into the site for every/any particular package that highlights any changes or bugs fixed. This would be applicable were bugs were fixed in the most recent release, so it highlights to the user to check previous experiments to ensure it did not affect their outcomes.",5,7,3,4,6,2,1,"Confusion matrix, ROC curve, Precision-Recall curve, Learning curves",pandas,"Great documentation, learning materials, package design and ease of use.","Keras, PyTorch","RandomForestClassifier or RandomForestRegressor, Other",No,"Calibration of probabilistic classifiers, Cost-sensitive learning",No,,,5,,,"less than an hour, less than a day, more than a day",,Strongly agree,Yes,aaronflanagan044@hotmail.com
10/11/2024 11:49:59,I have read this statement in its entirety and affirm the stated conditions.,8,3,2,6,5,4,1,7,"Out of core learning (aka mini-batches or streaming) to handle large datasets that don't fit into memory is essential.  Additionally, parallelization.  These particularly apply to dimensionality reduction algorithms.","Being able to train with a streaming dataset (training with small batches of the dataset), particularly for dimensionalty reduction techniques, among those, nonlinear ones.",5,7,7,7,7,7,6,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves, Other",DuckDB,Ease of use and excellent user guides,"Jax, PyTorch, Other","RandomForestClassifier or RandomForestRegressor, Other",Yes,"Calibration of probabilistic classifiers, Sample weights, Non-euclidean metrics",No,N/A,N/A,4,Other,Other,less than a day,More than 5,Agree,No,
10/11/2024 13:39:01,I have read this statement in its entirety and affirm the stated conditions.,,,,,,,8,,"The formatting makes it so hard to read and is so ugly, especially in dark mode",,,,,,,,,,,,,,,,,,,,,,,,,,
10/11/2024 16:58:45,I have read this statement in its entirety and affirm the stated conditions.,1,1,1,8,8,1,1,1,,More algorithms,1,1,7,7,7,5,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Spark DataFrame",easy peasy lemon squeezy,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,,,5,MLFlow,"Airflow, Kubeflow","less than 10 seconds, less than a day, more than a day",More than 5,Strongly Disagree,Yes,christopherjenness@gmail.com
10/11/2024 17:17:18,I have read this statement in its entirety and affirm the stated conditions.,7,8,7,8,7,6,3,3,,,7,7,7,7,5,6,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","pandas, Spark DataFrame",,"LightGBM, Transformers, XGBoost",RandomForestClassifier or RandomForestRegressor,No,,,,,3,MLFlow,Other,"less than 10 minutes, less than a day",More than 5,Agree,No,
10/12/2024 5:53:26,I have read this statement in its entirety and affirm the stated conditions.,3,6,4,8,5,8,2,,"Many students use scikit-learn to learn ML, I would focus on helping these future library users",Improve deep learning capabilities ,3,7,4,5,3,1,,"Confusion matrix, ROC curve, Feature importance, Learning curves",pandas,Pipelines,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,3,Weight and biases,,less than 10 minutes,3,Strongly agree,Yes,jordi.nin@esade.edu
10/12/2024 8:36:23,I have read this statement in its entirety and affirm the stated conditions.,,,,8,8,8,8,,Good to add new features. Like AI.,AI,6,7,5,,,4,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance",pandas,It's simple but powerful.,,"RandomForestClassifier or RandomForestRegressor, Other",No,Feature importances,,,,,,,less than 10 minutes,2,Strongly agree,Yes,anooprockstar10@gmail.com
10/12/2024 10:12:27,I have read this statement in its entirety and affirm the stated conditions.,,,,8,8,8,,,,,5,7,5,6,7,7,,"Confusion matrix, Feature importance",pandas,,PyTorch,"RandomForestClassifier or RandomForestRegressor, Other",No,"Uncertainty estimates for prediction, Feature importances",Yes,random state,consistent randomness,2,,,less than 10 seconds,,Strongly agree,No,
10/13/2024 5:43:11,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,8,8,more about image processing and object detect learning,multidimensional  database ,7,7,7,7,7,7,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves",pandas,featuring,"Keras, PyTorch",LogisticRegression,No,Calibration of probabilistic classifiers,No,,databases are correct and big enough,3,Weight and biases,Airflow,less than an hour,1,Strongly agree,Yes,haroldjoergestorres@gmail.com
10/13/2024 11:29:44,I have read this statement in its entirety and affirm the stated conditions.,8,7,7,7,5,5,3,1,Performance and run-time speed,A reinforcement learning package,,,7,6,6,6,,"Confusion matrix, Reliability diagram, ROC curve, Feature importance, Learning curves","pandas, Polars",,"Jax, PyTorch, XGBoost","RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",,"Uncertainty estimates for prediction, Feature importances, Non-euclidean metrics",Yes,,,3,Other,Other,less than 10 minutes,2,Strongly agree,No,
10/13/2024 14:18:29,I have read this statement in its entirety and affirm the stated conditions.,4,4,4,8,7,7,2,5,"I would like to see more implementations of state of the the art techniques and more detailed technical documentation with comprehensive examples showcasing the various functionalities provided by scikit-learn, e.g. more end-to-end examples that go from feature pre-processing using Pipeline, Column-Transformer, etc to model training, to model performance assessment (e.g. rank-ordering and calibration).
It would also be good to have a standard functionality for model serialisation similar to sklearn-json (https://github.com/mlrequest/sklearn-json/tree/master)",Incorporate conformal prediction for binary classification problems,4,7,3,3,3,2,1,"Reliability diagram, ROC curve, Precision-Recall curve, Feature importance",pandas,,"CatBoost, LightGBM, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",No,NA,NA,4,MLFlow,"Airflow, Kubeflow",less than a day,More than 5,Agree,No,
10/13/2024 17:04:15,I have read this statement in its entirety and affirm the stated conditions.,5,5,4,7,6,6,3,3,I would like to see scikit learn add integrations to deep learning libraries. so not implement deep learning within it but add features such as converting a DNN to a pytorch model.,Model explainability,6,4,3,4,5,5,2,,pandas,the ability to do different experiments and have the same interface for all of them,PyTorch,Other,Yes,"Cost-sensitive learning, Feature importances",No,,,4,Custom tool,Custom tool,less than an hour,,Strongly agree,Yes,papalotis1@gmail.com
10/14/2024 1:51:44,I have read this statement in its entirety and affirm the stated conditions.,8,5,5,5,5,8,3,,Allow training and inference using 8-bit or even 4-bit floats,"Allow training and inference using 8-bit or even 4-bit floats, allow using computer's GPU(s)",7,6,7,5,6,6,4,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots",pandas,,"CatBoost, Keras, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Non-euclidean metrics",Yes,8-bit quantization,Speed-up,5,Custom tool,Other,,2,Strongly agree,No,
10/14/2024 2:12:03,I have read this statement in its entirety and affirm the stated conditions.,3,6,8,8,5,5,1,1,,,2,2,2,7,2,2,,ROC curve,pandas,,"Keras, Transformers, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,,No,,,1,MLFlow,,"less than 10 seconds, less than a minute, less than 10 minutes",More than 5,Strongly agree,No,
10/14/2024 2:48:20,I have read this statement in its entirety and affirm the stated conditions.,4,8,4,8,6,6,1,4,"Reliability and Features are the most important for model deployment and case-specific implementations. Additional features for evaluation, hyperparameter optimization, explainability etc. are also very important for me as a user.","Customization and/or specific usecase support would be the most important, such as custom loss functions, models-with-rules and ensembe models are the most important additions for me.",5,3,5,3,1,5,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Other","pandas, Spark DataFrame","Features working on all models, such as evaluation metrics, grid searches, pipelines","XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, Other",Yes,"Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",Yes,"Bayesian priors, RL-like rewards/regrets and values for applying rules.","Rules, regrets/rewards and priors would steer the model towards the real solution (assuming correct use). ",3,"MLFlow, Custom tool",,less than an hour,2,Strongly agree,,
10/14/2024 2:54:10,I have read this statement in its entirety and affirm the stated conditions.,7,8,5,7,7,5,1,5,,,7,7,5,5,5,5,5,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Learning curves",pandas,,"Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",Yes,,,5,,,less than a minute,More than 5,Strongly agree,Yes,ZHL411@UCSD.EDU
10/14/2024 9:35:49,I have read this statement in its entirety and affirm the stated conditions.,7,7,6,6,7,5,4,1,,I need to think about it,6,6,3,3,5,4,3,"Confusion matrix, ROC curve, Residual plots, Learning curves","pandas, Polars",the clean structure,"Jax, Keras, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline",Yes,"Uncertainty estimates for prediction, Feature importances",No,,,2,MLFlow,,"less than 10 seconds, less than a minute",More than 5,Strongly agree,No,
10/14/2024 9:50:11,I have read this statement in its entirety and affirm the stated conditions.,6,4,3,7,4,4,2,,,,7,7,7,4,4,1,,"Confusion matrix, Residual plots, Learning curves",pandas,,PyTorch,"RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances",No,,,2,Weight and biases,,less than an hour,4,Agree,No,
10/14/2024 10:01:05,I have read this statement in its entirety and affirm the stated conditions.,1,1,8,1,8,7,4,8,"Make it easier to install, use, and understand. New features are a lower priority than accessing the current ones.",do a lot of soul searching over forcing people to change third party deps from using sklearn to your new name. was that really the best use of the internet's time?,,,,,,,,,,,,,,,,,,,,,,,,,
10/14/2024 11:23:29,I have read this statement in its entirety and affirm the stated conditions.,7,6,6,8,5,5,2,1,,,5,6,6,4,4,4,4,"Confusion matrix, Feature importance, Residual plots",pandas,API,"PyTorch, Transformers, XGBoost",Pipeline,Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,5,Weight and biases,"Custom tool, Other",less than 10 minutes,1,Agree,No,
10/14/2024 15:08:34,I have read this statement in its entirety and affirm the stated conditions.,7,7,6,8,5,4,1,1,,,6,7,4,3,5,2,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","pandas, Polars, Spark DataFrame",,"CatBoost, LightGBM, XGBoost","Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",,,,2,"DVC, MLFlow",Airflow,"less than 10 minutes, less than an hour",More than 5,Strongly agree,No,
10/14/2024 15:14:52,I have read this statement in its entirety and affirm the stated conditions.,6,6,4,6,5,7,1,1,,a new GridSearchCV that also takes different estimators/classifiers as input and also gives me the best classifier,6,7,1,6,6,4,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Learning curves",pandas,simplicity and convenience,PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of probabilistic classifiers, Uncertainty estimates for prediction, Feature importances",No,,,3,,,less than 10 minutes,1,Strongly agree,Yes,stamakro@gmail.com
10/14/2024 21:46:08,I have read this statement in its entirety and affirm the stated conditions.,7,7,6,7,6,7,6,,,,7,7,7,6,5,6,,"Confusion matrix, ROC curve, Feature importance",pandas,,Transformers,"RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,"Calibration of regressors, Sample weights",No,,,5,MLFlow,Custom tool,more than a day,2,Strongly agree,No,
10/15/2024 2:09:29,I have read this statement in its entirety and affirm the stated conditions.,6,8,7,8,8,8,8,5,"If it can do better.
","New tool for learning.
",6,6,7,7,5,7,4,"Reliability diagram, Precision-Recall curve, Feature importance, Residual plots, Learning curves","Dask DataFrame, DuckDB, Modin, pandas, Polars, Spark DataFrame, Other",I need to learn firsthand.,"CatBoost, LightGBM, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Metadata routing, Non-euclidean metrics",No,,,5,"Neptune, MLFlow, Weight and biases, Custom tool, Other","Airflow, Argo, Coiled, Kubeflow, Metaflow (outerbounds), Other",more than a day,More than 5,Neither agree nor disagree,Yes,Talagadew@gmail.com
10/15/2024 3:32:06,I have read this statement in its entirety and affirm the stated conditions.,4,5,5,6,5,8,7,7,,,7,6,6,7,6,6,,"Confusion matrix, ROC curve, Precision-Recall curve, Residual plots","DuckDB, pandas, Other",,PyTorch,"LogisticRegression, RandomForestClassifier or RandomForestRegressor",,,,,,2,,,,,,,
10/15/2024 3:40:12,I have read this statement in its entirety and affirm the stated conditions.,8,6,4,4,3,5,1,,"The performance is key because I work with large amount of data, and the reliability is another key aspect of my job because I'm a scientist.",Try to integrate ML models from another libraries (e.g. XGBoost) inside the pipelines.,3,7,3,7,5,4,,"Confusion matrix, Feature importance",pandas,Simplicity,"Keras, XGBoost","RandomForestClassifier or RandomForestRegressor, Pipeline",No,"Calibration of probabilistic classifiers, Calibration of regressors, Cost-sensitive learning, Feature importances, Sample weights",Yes,Weights,,4,Weight and biases,,"less than an hour, less than a day",1,Strongly agree,No,
10/15/2024 10:43:51,I have read this statement in its entirety and affirm the stated conditions.,4,4,4,4,6,5,1,1,I think that Scikit-learn is pretty much complete. What I would need would be silhouette-based clustering.,,3,5,1,4,6,7,1,"Confusion matrix, Learning curves",,The simplicity,"PyTorch, Transformers",,No,"Calibration of probabilistic classifiers, Feature importances",No,,,2,,,"less than 10 seconds, less than a day",More than 5,Strongly agree,No,
10/15/2024 16:22:08,I have read this statement in its entirety and affirm the stated conditions.,8,6,2,4,3,2,1,1,Would like to see sk beating intel and rapids. Scikit should be the one-stop for classic ml algos.,"Better control over parallelism. Unify all the threads, processes, mkl, blas, openblas, etc parameters and allow precise control of threads and processes and inter-operability with other libs like joblib.",7,7,7,2,1,1,1,"Confusion matrix, Learning curves","cudf, pandas","Common api for train, fit, predict, etc. Plus decent documentation of libs and algos.","Jax, Keras, LightGBM","LogisticRegression, RandomForestClassifier or RandomForestRegressor",Yes,"Calibration of probabilistic classifiers, Feature importances, Sample weights",Yes,Weights to model exponential averages,Better fit for timeseries,5,Custom tool,Custom tool,less than a day,More than 5,Agree,No,
10/16/2024 4:12:21,I have read this statement in its entirety and affirm the stated conditions.,7,4,6,7,3,6,7,5,It needs to have more performance when using the library for a larger dataset with more educational materials,packaging,7,6,6,,,,,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance","Dask DataFrame, DuckDB, pandas",,"Keras, PyTorch, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,,,3,"MLFlow, Weight and biases","Airflow, Kubeflow, Metaflow (outerbounds)",more than a day,4,Strongly agree,No,
10/16/2024 8:28:34,I have read this statement in its entirety and affirm the stated conditions.,8,6,3,8,6,6,1,1,"- The website is good, why would you change it?!?
- Similaringly, I think the package is already quite nicely structured and whilst I believe that one can always find an argument for refactoring something, I don't think this is necessary for sklearn
- Right now the main limitation of sklearn for my use cases is the inability to work with data that does not fit into memory as a whole.","- The possibility not to load all data into memory at once (for models where this makes sense/that have an intuitive stochastic variant), since I often struggle with OOM due to the usage of too large datasets ",5,7,2,4,6,6,1,"Confusion matrix, Precision-Recall curve, Feature importance, Learning curves, Other","cudf, pandas","- The standardized Interface
- The good user guide (which I often use as a reference when I want to remember some advantages/disadvantages/working principle of a standard ML algorithm or would like to point novice ML practitioners to some algorithm)
- The diversity of algorithms/solvable tasks (for most ML tasks, sklearn has something for you to get started (or sometimes already be satisfied))","LightGBM, PyTorch, Other","LogisticRegression, Pipeline, Other",Yes,"Calibration of probabilistic classifiers, Feature importances, Sample weights, Metadata routing",Yes,"Sample weights, but that's often already implemented.",,2,Custom tool,,"less than 10 seconds, less than a day, more than a day",,Strongly agree,,
10/16/2024 9:05:19,I have read this statement in its entirety and affirm the stated conditions.,8,6,7,8,7,4,1,4,"new features, supporting new algorithms",incorporating more advanced classification algorithms,5,7,4,6,2,4,3,"Confusion matrix, Precision-Recall curve",pandas,fast experimentaion,"Keras, PyTorch","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Non-euclidean metrics",No,,,2,Custom tool,Custom tool,less than a day,More than 5,Strongly agree,No,kujtim.rahmani@gmail.com
10/16/2024 11:10:16,I have read this statement in its entirety and affirm the stated conditions.,4,8,2,4,6,7,1,1,if it wasn't reliable i wouldn't use it,,6,7,2,5,3,4,1,"Confusion matrix, ROC curve, Feature importance, Other","pandas, Polars","easy to use, fast MVP","Keras, LightGBM, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor",No,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,"number of cores to use, parameters like tree depth, etc..","faster, replicable, overall better",2,MLFlow,Airflow,less than an hour,3,Strongly agree,Yes,mo7tarif1@hotmail.com
10/16/2024 13:19:07,I have read this statement in its entirety and affirm the stated conditions.,8,5,5,6,5,8,5,,,,7,3,7,5,6,2,1,"Confusion matrix, Learning curves",pandas,,"PyTorch, Transformers",,No,"Calibration of regressors, Uncertainty estimates for prediction",No,,,5,Weight and biases,,"less than an hour, less than a day",2,Strongly agree,No,
10/16/2024 16:48:45,I have read this statement in its entirety and affirm the stated conditions.,7,3,3,5,5,4,1,1,Model training is the bottleneck that could use improvement (where possible). It slows development turn-around time and can sometimes require a large amount of resources.,ability to build and leverage larger neural network models (like Keras),7,7,4,3,5,6,4,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots","pandas, Spark DataFrame",model classes tend to follow a very formulaic structure and it's easy for me to define and use my own class with the BaseEstimator and TransformerMixin objects.,"Keras, Transformers, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",No,n/a,n/a,4,MLFlow,"Airflow, Other",less than an hour,More than 5,Strongly agree,No,
10/16/2024 18:15:11,I have read this statement in its entirety and affirm the stated conditions.,,,,,,,,,,,,,,,,,,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves, Other","cudf, Dask DataFrame, pandas, Spark DataFrame",Documentation,"Keras, PyTorch, Transformers, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, Other",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",No,,,4,"MLFlow, Weight and biases, Other","Airflow, Dagster, Kubeflow","less than a day, more than a day",More than 5,Strongly agree,No,
10/17/2024 4:16:15,I have read this statement in its entirety and affirm the stated conditions.,6,5,5,8,6,4,3,1,,,7,3,6,3,2,2,1,"Confusion matrix, Feature importance, Residual plots",pandas,,PyTorch,"RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances",Yes,future forecasting,,3,MLFlow,Airflow,less than an hour,More than 5,Strongly agree,No,
10/17/2024 6:20:01,I have read this statement in its entirety and affirm the stated conditions.,3,3,1,8,4,4,1,,,,7,5,2,4,5,1,,Residual plots,,,,Other,No,"Calibration of regressors, Uncertainty estimates for prediction, Sample weights",Yes,Uncertainty on Y.,,3,,,less than a minute,2,Strongly agree,No,
10/17/2024 6:52:11,I have read this statement in its entirety and affirm the stated conditions.,8,4,4,5,3,3,2,2,,"Incorporate higher-performing dataframe libraries, such as Polars.",7,7,7,4,4,2,1,"Confusion matrix, Reliability diagram, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","cudf, pandas, Polars","Really well designed. Hides a lot of complexities behind simple-to-use interfaces. Advanced ML methods often work out of the box without extensive parameter tuning. A brilliant library that is well designed and maintained, for which we will be forever thankful.","CatBoost, Keras, LightGBM, PyTorch, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",,,,4,MLFlow,"Airflow, Other",less than a day,2,Strongly agree,No,
10/17/2024 11:59:08,I have read this statement in its entirety and affirm the stated conditions.,8,8,8,8,8,8,1,3,,,7,7,7,5,4,5,1,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","cudf, pandas, Other",,"CatBoost, LightGBM, XGBoost, Other","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier",No,Feature importances,No,,,5,,,,1,Neither agree nor disagree,No,
10/18/2024 13:52:19,I have read this statement in its entirety and affirm the stated conditions.,4,5,4,6,4,5,3,4,,,5,5,5,5,5,5,4,"ROC curve, Feature importance, Residual plots",pandas,,PyTorch,RandomForestClassifier or RandomForestRegressor,No,"Calibration of regressors, Uncertainty estimates for prediction, Feature importances, Sample weights",,,,3,,,less than 10 minutes,3,Strongly agree,Yes,bharath_hn@outlook.com
10/21/2024 3:04:43,I have read this statement in its entirety and affirm the stated conditions.,5,7,2,5,8,8,2,1,,,7,7,7,7,5,7,1,"Confusion matrix, Residual plots, Learning curves, Other",pandas,,Other,"LogisticRegression, Pipeline, Other",No,Uncertainty estimates for prediction,,,,1,,,less than 10 minutes,1,Strongly agree,No,
10/23/2024 5:24:46,I have read this statement in its entirety and affirm the stated conditions.,8,8,4,6,6,6,1,4,,Parallelize StandardScaler and SimpleImputer across the columns. I have 10k columns and those simple operations are slower than SGD...,7,7,2,6,5,6,4,"Confusion matrix, ROC curve, Precision-Recall curve, Feature importance, Residual plots, Learning curves","Dask DataFrame, DuckDB, pandas, Polars",Educational Documentation,"Keras, LightGBM","LogisticRegression, RandomForestClassifier or RandomForestRegressor, HistGradientBoostingRegressor or HistGradientBoostingClassifier, Pipeline, ColumnTransformer",Yes,Feature importances,Yes,Column Names,Interpret Feature Importance per Column,1,,,"less than 10 minutes, less than an hour",2,Neither agree nor disagree,Yes,jakob+sklearn@kasbauer.eu
10/26/2024 6:18:59,I have read this statement in its entirety and affirm the stated conditions.,7,5,3,7,5,3,1,,"While documentation and education materials are important, they are already excellent and setting standards for others. The performance on the other hand is fairly average, esp. for small batches, and many existing features are really incomplete.","Partial re-design and expansion of the feature transformation pipeline. The existing transformers do not follow the same consistent convention, and do not cover many practical situations, e.g. non-scalar transformations, separately pre-trained transformers, instruments for building pipelines from meta-descriptions.",7,7,3,3,5,7,,"Reliability diagram, ROC curve, Precision-Recall curve, Feature importance",pandas,Maturity and the range of available tools.,"CatBoost, LightGBM, XGBoost","LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline, ColumnTransformer",Yes,"Calibration of probabilistic classifiers, Calibration of regressors, Uncertainty estimates for prediction, Cost-sensitive learning, Feature importances, Sample weights",Yes,"hyperparameters, settings ","It would allow one to consider more flexible models. Also, it would be helpful to e.g. be able to switch off some unnecessary built-in checks to improve the evaluation performance of a live model.",2,"DVC, MLFlow",Airflow,less than 10 minutes,More than 5,Strongly agree,Yes,aleksey.pichugin@gmail.com
10/28/2024 18:52:13,I have read this statement in its entirety and affirm the stated conditions.,5,7,5,6,7,7,3,3,,,5,7,3,7,5,3,,"ROC curve, Precision-Recall curve, Learning curves","Dask DataFrame, pandas, Polars",,Keras,"LogisticRegression, RandomForestClassifier or RandomForestRegressor, Pipeline",Yes,"Cost-sensitive learning, Feature importances",,,,1,"DVC, MLFlow",Other,"less than a minute, less than an hour",4,Strongly agree,No,